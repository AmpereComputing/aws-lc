// This file is generated from a similarly-named Perl script in the BoringSSL
// source tree. Do not edit by hand.

#if defined(__has_feature)
#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
#define OPENSSL_NO_ASM
#endif
#endif

#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM)
#if defined(BORINGSSL_PREFIX)
#include <boringssl_prefix_symbols_asm.h>
#endif
.text	
.globl	gcm_init_avx512
.hidden gcm_init_avx512
.hidden	gcm_init_avx512
.type	gcm_init_avx512,@function
.align	32
gcm_init_avx512:
.cfi_startproc	
.byte	243,15,30,250
	vmovdqu64	(%rsi),%xmm16
	vpalignr	$8,%xmm16,%xmm16,%xmm16

	vmovdqa64	%xmm16,%xmm2
	vpsllq	$1,%xmm16,%xmm16
	vpsrlq	$63,%xmm2,%xmm2
	vmovdqa	%xmm2,%xmm1
	vpslldq	$8,%xmm2,%xmm2
	vpsrldq	$8,%xmm1,%xmm1
	vporq	%xmm2,%xmm16,%xmm16

	vpshufd	$36,%xmm1,%xmm2
	vpcmpeqd	TWOONE(%rip),%xmm2,%xmm2
	vpand	POLY(%rip),%xmm2,%xmm2
	vpxorq	%xmm2,%xmm16,%xmm16

	vmovdqu64	%xmm16,240(%rdi)
	vshufi32x4	$0x00,%ymm16,%ymm16,%ymm4
	vmovdqa	%ymm4,%ymm3

	vpclmulqdq	$0x11,%ymm4,%ymm3,%ymm0
	vpclmulqdq	$0x00,%ymm4,%ymm3,%ymm1
	vpclmulqdq	$0x01,%ymm4,%ymm3,%ymm2
	vpclmulqdq	$0x10,%ymm4,%ymm3,%ymm3
	vpxorq	%ymm2,%ymm3,%ymm3

	vpsrldq	$8,%ymm3,%ymm2
	vpslldq	$8,%ymm3,%ymm3
	vpxorq	%ymm2,%ymm0,%ymm0
	vpxorq	%ymm1,%ymm3,%ymm3



	vmovdqu64	POLY2(%rip),%ymm2

	vpclmulqdq	$0x01,%ymm3,%ymm2,%ymm1
	vpslldq	$8,%ymm1,%ymm1
	vpxorq	%ymm1,%ymm3,%ymm3



	vpclmulqdq	$0x00,%ymm3,%ymm2,%ymm1
	vpsrldq	$4,%ymm1,%ymm1
	vpclmulqdq	$0x10,%ymm3,%ymm2,%ymm3
	vpslldq	$4,%ymm3,%ymm3

	vpternlogq	$0x96,%ymm1,%ymm0,%ymm3

	vmovdqu64	%xmm3,224(%rdi)
	vinserti64x2	$1,%xmm16,%ymm3,%ymm4
	vmovdqa64	%ymm4,%ymm5

	vpclmulqdq	$0x11,%ymm3,%ymm4,%ymm0
	vpclmulqdq	$0x00,%ymm3,%ymm4,%ymm1
	vpclmulqdq	$0x01,%ymm3,%ymm4,%ymm2
	vpclmulqdq	$0x10,%ymm3,%ymm4,%ymm4
	vpxorq	%ymm2,%ymm4,%ymm4

	vpsrldq	$8,%ymm4,%ymm2
	vpslldq	$8,%ymm4,%ymm4
	vpxorq	%ymm2,%ymm0,%ymm0
	vpxorq	%ymm1,%ymm4,%ymm4



	vmovdqu64	POLY2(%rip),%ymm2

	vpclmulqdq	$0x01,%ymm4,%ymm2,%ymm1
	vpslldq	$8,%ymm1,%ymm1
	vpxorq	%ymm1,%ymm4,%ymm4



	vpclmulqdq	$0x00,%ymm4,%ymm2,%ymm1
	vpsrldq	$4,%ymm1,%ymm1
	vpclmulqdq	$0x10,%ymm4,%ymm2,%ymm4
	vpslldq	$4,%ymm4,%ymm4

	vpternlogq	$0x96,%ymm1,%ymm0,%ymm4

	vmovdqu64	%ymm4,192(%rdi)

	vinserti64x4	$1,%ymm5,%zmm4,%zmm4


	vshufi64x2	$0x00,%zmm4,%zmm4,%zmm3
	vmovdqa64	%zmm4,%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm0
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm1
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm2
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm2,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm2
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm2,%zmm0,%zmm0
	vpxorq	%zmm1,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm2

	vpclmulqdq	$0x01,%zmm4,%zmm2,%zmm1
	vpslldq	$8,%zmm1,%zmm1
	vpxorq	%zmm1,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm2,%zmm1
	vpsrldq	$4,%zmm1,%zmm1
	vpclmulqdq	$0x10,%zmm4,%zmm2,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm1,%zmm0,%zmm4

	vmovdqu64	%zmm4,128(%rdi)
	vshufi64x2	$0x00,%zmm4,%zmm4,%zmm3

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm0
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm1
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm2
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm2,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm2
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm2,%zmm0,%zmm0
	vpxorq	%zmm1,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm2

	vpclmulqdq	$0x01,%zmm5,%zmm2,%zmm1
	vpslldq	$8,%zmm1,%zmm1
	vpxorq	%zmm1,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm2,%zmm1
	vpsrldq	$4,%zmm1,%zmm1
	vpclmulqdq	$0x10,%zmm5,%zmm2,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm1,%zmm0,%zmm5

	vmovdqu64	%zmm5,64(%rdi)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm0
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm1
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm2
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm2,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm2
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm2,%zmm0,%zmm0
	vpxorq	%zmm1,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm2

	vpclmulqdq	$0x01,%zmm4,%zmm2,%zmm1
	vpslldq	$8,%zmm1,%zmm1
	vpxorq	%zmm1,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm2,%zmm1
	vpsrldq	$4,%zmm1,%zmm1
	vpclmulqdq	$0x10,%zmm4,%zmm2,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm1,%zmm0,%zmm4

	vmovdqu64	%zmm4,0(%rdi)
	vzeroupper
.Lexit_init:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	gcm_init_avx512, .-gcm_init_avx512
.globl	gcm_gmult_avx512
.hidden gcm_gmult_avx512
.hidden	gcm_gmult_avx512
.type	gcm_gmult_avx512,@function
.align	32
gcm_gmult_avx512:
.cfi_startproc	
.byte	243,15,30,250
	vmovdqu64	(%rdi),%xmm1


	vpshufb	SHUF_MASK(%rip),%xmm1,%xmm1
	vmovdqu64	240(%rsi),%xmm2

	vpclmulqdq	$0x11,%xmm2,%xmm1,%xmm3
	vpclmulqdq	$0x00,%xmm2,%xmm1,%xmm4
	vpclmulqdq	$0x01,%xmm2,%xmm1,%xmm5
	vpclmulqdq	$0x10,%xmm2,%xmm1,%xmm1
	vpxorq	%xmm5,%xmm1,%xmm1

	vpsrldq	$8,%xmm1,%xmm5
	vpslldq	$8,%xmm1,%xmm1
	vpxorq	%xmm5,%xmm3,%xmm3
	vpxorq	%xmm4,%xmm1,%xmm1



	vmovdqu64	POLY2(%rip),%xmm5

	vpclmulqdq	$0x01,%xmm1,%xmm5,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm1,%xmm1



	vpclmulqdq	$0x00,%xmm1,%xmm5,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm5,%xmm1
	vpslldq	$4,%xmm1,%xmm1

	vpternlogq	$0x96,%xmm4,%xmm3,%xmm1

	vpshufb	SHUF_MASK(%rip),%xmm1,%xmm1
	vmovdqu64	%xmm1,(%rdi)
	vzeroupper
.Lexit_gmult:
	.byte	0xf3,0xc3
.cfi_endproc	
.size	gcm_gmult_avx512, .-gcm_gmult_avx512
.globl	gcm_ghash_avx512
.hidden gcm_ghash_avx512
.hidden	gcm_ghash_avx512
.type	gcm_ghash_avx512,@function
.align	32
gcm_ghash_avx512:
.cfi_startproc	
.Lghash_seh_begin:
.byte	243,15,30,250
	pushq	%rbx
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbx,-16
.Lghash_seh_push_rbx:
	pushq	%rbp
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbp,-24
.Lghash_seh_push_rbp:
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-32
.Lghash_seh_push_r12:
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-40
.Lghash_seh_push_r13:
	pushq	%r14
.cfi_adjust_cfa_offset	8
.cfi_offset	%r14,-48
.Lghash_seh_push_r14:
	pushq	%r15
.cfi_adjust_cfa_offset	8
.cfi_offset	%r15,-56
.Lghash_seh_push_r15:










	leaq	0(%rsp),%rbp
.cfi_def_cfa_register	%rbp
.Lghash_seh_setfp:

.Lghash_seh_prolog_end:
	subq	$820,%rsp
	andq	$(-64),%rsp
	vmovdqu64	(%rdi),%xmm14
	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14
	movq	%rdx,%r10
	movq	%rcx,%r11
	orq	%r11,%r11
	jz	.L_CALC_AAD_done_bweagazuDvjAhkg

	xorq	%rbx,%rbx
	vmovdqa64	SHUF_MASK(%rip),%zmm16

.L_get_AAD_loop48x16_bweagazuDvjAhkg:
	cmpq	$768,%r11
	jl	.L_exit_AAD_loop48x16_bweagazuDvjAhkg
	vmovdqu64	0(%r10),%zmm11
	vmovdqu64	64(%r10),%zmm3
	vmovdqu64	128(%r10),%zmm4
	vmovdqu64	192(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	testq	%rbx,%rbx
	jnz	.L_skip_hkeys_precomputation_ldDhoiaDbxpndyl

	vmovdqu64	192(%rsi),%zmm1
	vmovdqu64	%zmm1,704(%rsp)

	vmovdqu64	128(%rsi),%zmm9
	vmovdqu64	%zmm9,640(%rsp)


	vshufi64x2	$0x00,%zmm9,%zmm9,%zmm9

	vmovdqu64	64(%rsi),%zmm10
	vmovdqu64	%zmm10,576(%rsp)

	vmovdqu64	0(%rsi),%zmm12
	vmovdqu64	%zmm12,512(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,448(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,384(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,320(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,256(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,192(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,128(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,64(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,0(%rsp)
.L_skip_hkeys_precomputation_ldDhoiaDbxpndyl:
	movq	$1,%rbx
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	0(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	64(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpxorq	%zmm17,%zmm10,%zmm7
	vpxorq	%zmm13,%zmm1,%zmm6
	vpxorq	%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	128(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	192(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	256(%r10),%zmm11
	vmovdqu64	320(%r10),%zmm3
	vmovdqu64	384(%r10),%zmm4
	vmovdqu64	448(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vmovdqu64	256(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	320(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	384(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	448(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	512(%r10),%zmm11
	vmovdqu64	576(%r10),%zmm3
	vmovdqu64	640(%r10),%zmm4
	vmovdqu64	704(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vmovdqu64	512(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	576(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	640(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	704(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7

	vpsrldq	$8,%zmm7,%zmm1
	vpslldq	$8,%zmm7,%zmm9
	vpxorq	%zmm1,%zmm6,%zmm6
	vpxorq	%zmm9,%zmm8,%zmm8
	vextracti64x4	$1,%zmm6,%ymm1
	vpxorq	%ymm1,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm1
	vpxorq	%xmm1,%xmm6,%xmm6
	vextracti64x4	$1,%zmm8,%ymm9
	vpxorq	%ymm9,%ymm8,%ymm8
	vextracti32x4	$1,%ymm8,%xmm9
	vpxorq	%xmm9,%xmm8,%xmm8
	vmovdqa64	POLY2(%rip),%xmm10


	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
	vpslldq	$8,%xmm1,%xmm1
	vpxorq	%xmm1,%xmm8,%xmm1


	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
	vpsrldq	$4,%xmm9,%xmm9
	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm6,%xmm9,%xmm14

	subq	$768,%r11
	je	.L_CALC_AAD_done_bweagazuDvjAhkg

	addq	$768,%r10
	jmp	.L_get_AAD_loop48x16_bweagazuDvjAhkg

.L_exit_AAD_loop48x16_bweagazuDvjAhkg:

	cmpq	$512,%r11
	jl	.L_less_than_32x16_bweagazuDvjAhkg

	vmovdqu64	0(%r10),%zmm11
	vmovdqu64	64(%r10),%zmm3
	vmovdqu64	128(%r10),%zmm4
	vmovdqu64	192(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	testq	%rbx,%rbx
	jnz	.L_skip_hkeys_precomputation_CDqmjmEDwdkEdGC

	vmovdqu64	192(%rsi),%zmm1
	vmovdqu64	%zmm1,704(%rsp)

	vmovdqu64	128(%rsi),%zmm9
	vmovdqu64	%zmm9,640(%rsp)


	vshufi64x2	$0x00,%zmm9,%zmm9,%zmm9

	vmovdqu64	64(%rsi),%zmm10
	vmovdqu64	%zmm10,576(%rsp)

	vmovdqu64	0(%rsi),%zmm12
	vmovdqu64	%zmm12,512(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,448(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,384(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,320(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,256(%rsp)
.L_skip_hkeys_precomputation_CDqmjmEDwdkEdGC:
	movq	$1,%rbx
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	256(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	320(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpxorq	%zmm17,%zmm10,%zmm7
	vpxorq	%zmm13,%zmm1,%zmm6
	vpxorq	%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	384(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	448(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	256(%r10),%zmm11
	vmovdqu64	320(%r10),%zmm3
	vmovdqu64	384(%r10),%zmm4
	vmovdqu64	448(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vmovdqu64	512(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	576(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	640(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	704(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7

	vpsrldq	$8,%zmm7,%zmm1
	vpslldq	$8,%zmm7,%zmm9
	vpxorq	%zmm1,%zmm6,%zmm6
	vpxorq	%zmm9,%zmm8,%zmm8
	vextracti64x4	$1,%zmm6,%ymm1
	vpxorq	%ymm1,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm1
	vpxorq	%xmm1,%xmm6,%xmm6
	vextracti64x4	$1,%zmm8,%ymm9
	vpxorq	%ymm9,%ymm8,%ymm8
	vextracti32x4	$1,%ymm8,%xmm9
	vpxorq	%xmm9,%xmm8,%xmm8
	vmovdqa64	POLY2(%rip),%xmm10


	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
	vpslldq	$8,%xmm1,%xmm1
	vpxorq	%xmm1,%xmm8,%xmm1


	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
	vpsrldq	$4,%xmm9,%xmm9
	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm6,%xmm9,%xmm14

	subq	$512,%r11
	je	.L_CALC_AAD_done_bweagazuDvjAhkg

	addq	$512,%r10
	jmp	.L_less_than_16x16_bweagazuDvjAhkg

.L_less_than_32x16_bweagazuDvjAhkg:
	cmpq	$256,%r11
	jl	.L_less_than_16x16_bweagazuDvjAhkg

	vmovdqu64	0(%r10),%zmm11
	vmovdqu64	64(%r10),%zmm3
	vmovdqu64	128(%r10),%zmm4
	vmovdqu64	192(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	0(%rsi),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	64(%rsi),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpxorq	%zmm17,%zmm10,%zmm7
	vpxorq	%zmm13,%zmm1,%zmm6
	vpxorq	%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	128(%rsi),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	192(%rsi),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7

	vpsrldq	$8,%zmm7,%zmm1
	vpslldq	$8,%zmm7,%zmm9
	vpxorq	%zmm1,%zmm6,%zmm6
	vpxorq	%zmm9,%zmm8,%zmm8
	vextracti64x4	$1,%zmm6,%ymm1
	vpxorq	%ymm1,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm1
	vpxorq	%xmm1,%xmm6,%xmm6
	vextracti64x4	$1,%zmm8,%ymm9
	vpxorq	%ymm9,%ymm8,%ymm8
	vextracti32x4	$1,%ymm8,%xmm9
	vpxorq	%xmm9,%xmm8,%xmm8
	vmovdqa64	POLY2(%rip),%xmm10


	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
	vpslldq	$8,%xmm1,%xmm1
	vpxorq	%xmm1,%xmm8,%xmm1


	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
	vpsrldq	$4,%xmm9,%xmm9
	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm6,%xmm9,%xmm14

	subq	$256,%r11
	je	.L_CALC_AAD_done_bweagazuDvjAhkg

	addq	$256,%r10

.L_less_than_16x16_bweagazuDvjAhkg:

	leaq	byte64_len_to_mask_table(%rip),%r12
	leaq	(%r12,%r11,8),%r12


	addl	$15,%r11d
	shrl	$4,%r11d
	cmpl	$2,%r11d
	jb	.L_AAD_blocks_1_bweagazuDvjAhkg
	je	.L_AAD_blocks_2_bweagazuDvjAhkg
	cmpl	$4,%r11d
	jb	.L_AAD_blocks_3_bweagazuDvjAhkg
	je	.L_AAD_blocks_4_bweagazuDvjAhkg
	cmpl	$6,%r11d
	jb	.L_AAD_blocks_5_bweagazuDvjAhkg
	je	.L_AAD_blocks_6_bweagazuDvjAhkg
	cmpl	$8,%r11d
	jb	.L_AAD_blocks_7_bweagazuDvjAhkg
	je	.L_AAD_blocks_8_bweagazuDvjAhkg
	cmpl	$10,%r11d
	jb	.L_AAD_blocks_9_bweagazuDvjAhkg
	je	.L_AAD_blocks_10_bweagazuDvjAhkg
	cmpl	$12,%r11d
	jb	.L_AAD_blocks_11_bweagazuDvjAhkg
	je	.L_AAD_blocks_12_bweagazuDvjAhkg
	cmpl	$14,%r11d
	jb	.L_AAD_blocks_13_bweagazuDvjAhkg
	je	.L_AAD_blocks_14_bweagazuDvjAhkg
	cmpl	$15,%r11d
	je	.L_AAD_blocks_15_bweagazuDvjAhkg
.L_AAD_blocks_16_bweagazuDvjAhkg:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%zmm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	0(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	64(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	128(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm9,%zmm11,%zmm1
	vpternlogq	$0x96,%zmm10,%zmm3,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm12,%zmm11,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm3,%zmm8
	vmovdqu64	192(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm5,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm5,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm5,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm5,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_bweagazuDvjAhkg
.L_AAD_blocks_15_bweagazuDvjAhkg:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%zmm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	16(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	80(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	144(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
	vmovdqu64	208(%rsi),%ymm15
	vinserti64x2	$2,240(%rsi),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm5,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm5,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm5,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm5,%zmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_bweagazuDvjAhkg
.L_AAD_blocks_14_bweagazuDvjAhkg:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%ymm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%ymm16,%ymm5,%ymm5
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	32(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	96(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	160(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
	vmovdqu64	224(%rsi),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm5,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm5,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm5,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm5,%ymm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_bweagazuDvjAhkg
.L_AAD_blocks_13_bweagazuDvjAhkg:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%xmm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%xmm16,%xmm5,%xmm5
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	48(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	112(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	176(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
	vmovdqu64	240(%rsi),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm5,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm5,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm5,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm5,%xmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_bweagazuDvjAhkg
.L_AAD_blocks_12_bweagazuDvjAhkg:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	64(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	128(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	192(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_bweagazuDvjAhkg
.L_AAD_blocks_11_bweagazuDvjAhkg:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	80(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	144(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13
	vmovdqu64	208(%rsi),%ymm15
	vinserti64x2	$2,240(%rsi),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_bweagazuDvjAhkg
.L_AAD_blocks_10_bweagazuDvjAhkg:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%ymm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%ymm16,%ymm4,%ymm4
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	96(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	160(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13
	vmovdqu64	224(%rsi),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm4,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm4,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm4,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm4,%ymm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_bweagazuDvjAhkg
.L_AAD_blocks_9_bweagazuDvjAhkg:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%xmm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%xmm16,%xmm4,%xmm4
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	112(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	176(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13
	vmovdqu64	240(%rsi),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm4,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm4,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm4,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm4,%xmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_bweagazuDvjAhkg
.L_AAD_blocks_8_bweagazuDvjAhkg:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	128(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	192(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_bweagazuDvjAhkg
.L_AAD_blocks_7_bweagazuDvjAhkg:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	144(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
	vmovdqu64	208(%rsi),%ymm15
	vinserti64x2	$2,240(%rsi),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_bweagazuDvjAhkg
.L_AAD_blocks_6_bweagazuDvjAhkg:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%ymm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%ymm16,%ymm3,%ymm3
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	160(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
	vmovdqu64	224(%rsi),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm3,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm3,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm3,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm3,%ymm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_bweagazuDvjAhkg
.L_AAD_blocks_5_bweagazuDvjAhkg:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%xmm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%xmm16,%xmm3,%xmm3
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	176(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
	vmovdqu64	240(%rsi),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm3,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm3,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm3,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm3,%xmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_bweagazuDvjAhkg
.L_AAD_blocks_4_bweagazuDvjAhkg:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	192(%rsi),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_bweagazuDvjAhkg
.L_AAD_blocks_3_bweagazuDvjAhkg:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	208(%rsi),%ymm15
	vinserti64x2	$2,240(%rsi),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_bweagazuDvjAhkg
.L_AAD_blocks_2_bweagazuDvjAhkg:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%ymm11{%k1}{z}
	vpshufb	%ymm16,%ymm11,%ymm11
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	224(%rsi),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm11,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm11,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm11,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm11,%ymm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

	jmp	.L_CALC_AAD_done_bweagazuDvjAhkg
.L_AAD_blocks_1_bweagazuDvjAhkg:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%xmm11{%k1}{z}
	vpshufb	%xmm16,%xmm11,%xmm11
	vpxorq	%zmm14,%zmm11,%zmm11
	vmovdqu64	240(%rsi),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm11,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm11,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm11,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm11,%xmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm14

.L_CALC_AAD_done_bweagazuDvjAhkg:
	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14
	vmovdqu64	%xmm14,(%rdi)
	cmpq	$256,%rcx
	jbe	.Lskip_hkeys_cleanup_Cffhnocumnkylxz
	vpxor	%xmm0,%xmm0,%xmm0
	vmovdqa64	%zmm0,0(%rsp)
	vmovdqa64	%zmm0,64(%rsp)
	vmovdqa64	%zmm0,128(%rsp)
	vmovdqa64	%zmm0,192(%rsp)
	vmovdqa64	%zmm0,256(%rsp)
	vmovdqa64	%zmm0,320(%rsp)
	vmovdqa64	%zmm0,384(%rsp)
	vmovdqa64	%zmm0,448(%rsp)
	vmovdqa64	%zmm0,512(%rsp)
	vmovdqa64	%zmm0,576(%rsp)
	vmovdqa64	%zmm0,640(%rsp)
	vmovdqa64	%zmm0,704(%rsp)
.Lskip_hkeys_cleanup_Cffhnocumnkylxz:
	vzeroupper
	leaq	(%rbp),%rsp
.cfi_def_cfa_register	%rsp
	popq	%r15
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r15
	popq	%r14
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r14
	popq	%r13
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r13
	popq	%r12
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r12
	popq	%rbp
.cfi_adjust_cfa_offset	-8
.cfi_restore	%rbp
	popq	%rbx
.cfi_adjust_cfa_offset	-8
.cfi_restore	%rbx
.Lexit_ghash:
	.byte	0xf3,0xc3
.Lghash_seh_end:
.cfi_endproc	
.size	gcm_ghash_avx512, .-gcm_ghash_avx512
.globl	gcm_setiv_avx512
.hidden gcm_setiv_avx512
.hidden	gcm_setiv_avx512
.type	gcm_setiv_avx512,@function
.align	32
gcm_setiv_avx512:
.cfi_startproc	
.Lsetiv_seh_begin:
.byte	243,15,30,250
	pushq	%rbx
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbx,-16
.Lsetiv_seh_push_rbx:
	pushq	%rbp
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbp,-24
.Lsetiv_seh_push_rbp:
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-32
.Lsetiv_seh_push_r12:
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-40
.Lsetiv_seh_push_r13:
	pushq	%r14
.cfi_adjust_cfa_offset	8
.cfi_offset	%r14,-48
.Lsetiv_seh_push_r14:
	pushq	%r15
.cfi_adjust_cfa_offset	8
.cfi_offset	%r15,-56
.Lsetiv_seh_push_r15:










	leaq	0(%rsp),%rbp
.cfi_def_cfa_register	%rbp
.Lsetiv_seh_setfp:

.Lsetiv_seh_prolog_end:
	subq	$820,%rsp
	andq	$(-64),%rsp
	cmpq	$12,%rcx
	je	iv_len_12_init_IV
	vpxor	%xmm2,%xmm2,%xmm2
	leaq	96(%rsi),%r13
	movq	%rdx,%r10
	movq	%rcx,%r11
	orq	%r11,%r11
	jz	.L_CALC_AAD_done_nrtDGszAnhavlGj

	xorq	%rbx,%rbx
	vmovdqa64	SHUF_MASK(%rip),%zmm16

.L_get_AAD_loop48x16_nrtDGszAnhavlGj:
	cmpq	$768,%r11
	jl	.L_exit_AAD_loop48x16_nrtDGszAnhavlGj
	vmovdqu64	0(%r10),%zmm11
	vmovdqu64	64(%r10),%zmm3
	vmovdqu64	128(%r10),%zmm4
	vmovdqu64	192(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	testq	%rbx,%rbx
	jnz	.L_skip_hkeys_precomputation_bfDsCnlCujrfwFc

	vmovdqu64	192(%r13),%zmm1
	vmovdqu64	%zmm1,704(%rsp)

	vmovdqu64	128(%r13),%zmm9
	vmovdqu64	%zmm9,640(%rsp)


	vshufi64x2	$0x00,%zmm9,%zmm9,%zmm9

	vmovdqu64	64(%r13),%zmm10
	vmovdqu64	%zmm10,576(%rsp)

	vmovdqu64	0(%r13),%zmm12
	vmovdqu64	%zmm12,512(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,448(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,384(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,320(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,256(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,192(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,128(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,64(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,0(%rsp)
.L_skip_hkeys_precomputation_bfDsCnlCujrfwFc:
	movq	$1,%rbx
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	0(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	64(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpxorq	%zmm17,%zmm10,%zmm7
	vpxorq	%zmm13,%zmm1,%zmm6
	vpxorq	%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	128(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	192(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	256(%r10),%zmm11
	vmovdqu64	320(%r10),%zmm3
	vmovdqu64	384(%r10),%zmm4
	vmovdqu64	448(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vmovdqu64	256(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	320(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	384(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	448(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	512(%r10),%zmm11
	vmovdqu64	576(%r10),%zmm3
	vmovdqu64	640(%r10),%zmm4
	vmovdqu64	704(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vmovdqu64	512(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	576(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	640(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	704(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7

	vpsrldq	$8,%zmm7,%zmm1
	vpslldq	$8,%zmm7,%zmm9
	vpxorq	%zmm1,%zmm6,%zmm6
	vpxorq	%zmm9,%zmm8,%zmm8
	vextracti64x4	$1,%zmm6,%ymm1
	vpxorq	%ymm1,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm1
	vpxorq	%xmm1,%xmm6,%xmm6
	vextracti64x4	$1,%zmm8,%ymm9
	vpxorq	%ymm9,%ymm8,%ymm8
	vextracti32x4	$1,%ymm8,%xmm9
	vpxorq	%xmm9,%xmm8,%xmm8
	vmovdqa64	POLY2(%rip),%xmm10


	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
	vpslldq	$8,%xmm1,%xmm1
	vpxorq	%xmm1,%xmm8,%xmm1


	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
	vpsrldq	$4,%xmm9,%xmm9
	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm6,%xmm9,%xmm2

	subq	$768,%r11
	je	.L_CALC_AAD_done_nrtDGszAnhavlGj

	addq	$768,%r10
	jmp	.L_get_AAD_loop48x16_nrtDGszAnhavlGj

.L_exit_AAD_loop48x16_nrtDGszAnhavlGj:

	cmpq	$512,%r11
	jl	.L_less_than_32x16_nrtDGszAnhavlGj

	vmovdqu64	0(%r10),%zmm11
	vmovdqu64	64(%r10),%zmm3
	vmovdqu64	128(%r10),%zmm4
	vmovdqu64	192(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	testq	%rbx,%rbx
	jnz	.L_skip_hkeys_precomputation_gtbgGzjftcbaBnc

	vmovdqu64	192(%r13),%zmm1
	vmovdqu64	%zmm1,704(%rsp)

	vmovdqu64	128(%r13),%zmm9
	vmovdqu64	%zmm9,640(%rsp)


	vshufi64x2	$0x00,%zmm9,%zmm9,%zmm9

	vmovdqu64	64(%r13),%zmm10
	vmovdqu64	%zmm10,576(%rsp)

	vmovdqu64	0(%r13),%zmm12
	vmovdqu64	%zmm12,512(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,448(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,384(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm10,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm10,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm10,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm10,%zmm10

	vpsrldq	$8,%zmm10,%zmm17
	vpslldq	$8,%zmm10,%zmm10
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm10,%zmm10



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm10,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm10,%zmm10



	vpclmulqdq	$0x00,%zmm10,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm10,%zmm17,%zmm10
	vpslldq	$4,%zmm10,%zmm10

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm10

	vmovdqu64	%zmm10,320(%rsp)

	vpclmulqdq	$0x11,%zmm9,%zmm12,%zmm13
	vpclmulqdq	$0x00,%zmm9,%zmm12,%zmm15
	vpclmulqdq	$0x01,%zmm9,%zmm12,%zmm17
	vpclmulqdq	$0x10,%zmm9,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm12,%zmm12

	vpsrldq	$8,%zmm12,%zmm17
	vpslldq	$8,%zmm12,%zmm12
	vpxorq	%zmm17,%zmm13,%zmm13
	vpxorq	%zmm15,%zmm12,%zmm12



	vmovdqu64	POLY2(%rip),%zmm17

	vpclmulqdq	$0x01,%zmm12,%zmm17,%zmm15
	vpslldq	$8,%zmm15,%zmm15
	vpxorq	%zmm15,%zmm12,%zmm12



	vpclmulqdq	$0x00,%zmm12,%zmm17,%zmm15
	vpsrldq	$4,%zmm15,%zmm15
	vpclmulqdq	$0x10,%zmm12,%zmm17,%zmm12
	vpslldq	$4,%zmm12,%zmm12

	vpternlogq	$0x96,%zmm15,%zmm13,%zmm12

	vmovdqu64	%zmm12,256(%rsp)
.L_skip_hkeys_precomputation_gtbgGzjftcbaBnc:
	movq	$1,%rbx
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	256(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	320(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpxorq	%zmm17,%zmm10,%zmm7
	vpxorq	%zmm13,%zmm1,%zmm6
	vpxorq	%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	384(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	448(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	256(%r10),%zmm11
	vmovdqu64	320(%r10),%zmm3
	vmovdqu64	384(%r10),%zmm4
	vmovdqu64	448(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vmovdqu64	512(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	576(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	640(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	704(%rsp),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7

	vpsrldq	$8,%zmm7,%zmm1
	vpslldq	$8,%zmm7,%zmm9
	vpxorq	%zmm1,%zmm6,%zmm6
	vpxorq	%zmm9,%zmm8,%zmm8
	vextracti64x4	$1,%zmm6,%ymm1
	vpxorq	%ymm1,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm1
	vpxorq	%xmm1,%xmm6,%xmm6
	vextracti64x4	$1,%zmm8,%ymm9
	vpxorq	%ymm9,%ymm8,%ymm8
	vextracti32x4	$1,%ymm8,%xmm9
	vpxorq	%xmm9,%xmm8,%xmm8
	vmovdqa64	POLY2(%rip),%xmm10


	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
	vpslldq	$8,%xmm1,%xmm1
	vpxorq	%xmm1,%xmm8,%xmm1


	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
	vpsrldq	$4,%xmm9,%xmm9
	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm6,%xmm9,%xmm2

	subq	$512,%r11
	je	.L_CALC_AAD_done_nrtDGszAnhavlGj

	addq	$512,%r10
	jmp	.L_less_than_16x16_nrtDGszAnhavlGj

.L_less_than_32x16_nrtDGszAnhavlGj:
	cmpq	$256,%r11
	jl	.L_less_than_16x16_nrtDGszAnhavlGj

	vmovdqu64	0(%r10),%zmm11
	vmovdqu64	64(%r10),%zmm3
	vmovdqu64	128(%r10),%zmm4
	vmovdqu64	192(%r10),%zmm5
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	0(%r13),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm11,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm11,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm11,%zmm12
	vmovdqu64	64(%r13),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm3,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm3,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm3,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm3,%zmm18
	vpxorq	%zmm17,%zmm10,%zmm7
	vpxorq	%zmm13,%zmm1,%zmm6
	vpxorq	%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7
	vmovdqu64	128(%r13),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm19,%zmm4,%zmm9
	vpclmulqdq	$0x01,%zmm19,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm19,%zmm4,%zmm12
	vmovdqu64	192(%r13),%zmm19
	vpclmulqdq	$0x11,%zmm19,%zmm5,%zmm13
	vpclmulqdq	$0x00,%zmm19,%zmm5,%zmm15
	vpclmulqdq	$0x01,%zmm19,%zmm5,%zmm17
	vpclmulqdq	$0x10,%zmm19,%zmm5,%zmm18

	vpternlogq	$0x96,%zmm17,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm1,%zmm6
	vpternlogq	$0x96,%zmm15,%zmm9,%zmm8
	vpternlogq	$0x96,%zmm18,%zmm12,%zmm7

	vpsrldq	$8,%zmm7,%zmm1
	vpslldq	$8,%zmm7,%zmm9
	vpxorq	%zmm1,%zmm6,%zmm6
	vpxorq	%zmm9,%zmm8,%zmm8
	vextracti64x4	$1,%zmm6,%ymm1
	vpxorq	%ymm1,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm1
	vpxorq	%xmm1,%xmm6,%xmm6
	vextracti64x4	$1,%zmm8,%ymm9
	vpxorq	%ymm9,%ymm8,%ymm8
	vextracti32x4	$1,%ymm8,%xmm9
	vpxorq	%xmm9,%xmm8,%xmm8
	vmovdqa64	POLY2(%rip),%xmm10


	vpclmulqdq	$0x01,%xmm8,%xmm10,%xmm1
	vpslldq	$8,%xmm1,%xmm1
	vpxorq	%xmm1,%xmm8,%xmm1


	vpclmulqdq	$0x00,%xmm1,%xmm10,%xmm9
	vpsrldq	$4,%xmm9,%xmm9
	vpclmulqdq	$0x10,%xmm1,%xmm10,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm6,%xmm9,%xmm2

	subq	$256,%r11
	je	.L_CALC_AAD_done_nrtDGszAnhavlGj

	addq	$256,%r10

.L_less_than_16x16_nrtDGszAnhavlGj:

	leaq	byte64_len_to_mask_table(%rip),%r12
	leaq	(%r12,%r11,8),%r12


	addl	$15,%r11d
	shrl	$4,%r11d
	cmpl	$2,%r11d
	jb	.L_AAD_blocks_1_nrtDGszAnhavlGj
	je	.L_AAD_blocks_2_nrtDGszAnhavlGj
	cmpl	$4,%r11d
	jb	.L_AAD_blocks_3_nrtDGszAnhavlGj
	je	.L_AAD_blocks_4_nrtDGszAnhavlGj
	cmpl	$6,%r11d
	jb	.L_AAD_blocks_5_nrtDGszAnhavlGj
	je	.L_AAD_blocks_6_nrtDGszAnhavlGj
	cmpl	$8,%r11d
	jb	.L_AAD_blocks_7_nrtDGszAnhavlGj
	je	.L_AAD_blocks_8_nrtDGszAnhavlGj
	cmpl	$10,%r11d
	jb	.L_AAD_blocks_9_nrtDGszAnhavlGj
	je	.L_AAD_blocks_10_nrtDGszAnhavlGj
	cmpl	$12,%r11d
	jb	.L_AAD_blocks_11_nrtDGszAnhavlGj
	je	.L_AAD_blocks_12_nrtDGszAnhavlGj
	cmpl	$14,%r11d
	jb	.L_AAD_blocks_13_nrtDGszAnhavlGj
	je	.L_AAD_blocks_14_nrtDGszAnhavlGj
	cmpl	$15,%r11d
	je	.L_AAD_blocks_15_nrtDGszAnhavlGj
.L_AAD_blocks_16_nrtDGszAnhavlGj:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%zmm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	0(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	64(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	128(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm9,%zmm11,%zmm1
	vpternlogq	$0x96,%zmm10,%zmm3,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm12,%zmm11,%zmm7
	vpternlogq	$0x96,%zmm13,%zmm3,%zmm8
	vmovdqu64	192(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm5,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm5,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm5,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm5,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_nrtDGszAnhavlGj
.L_AAD_blocks_15_nrtDGszAnhavlGj:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%zmm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%zmm16,%zmm5,%zmm5
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	16(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	80(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	144(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
	vmovdqu64	208(%r13),%ymm15
	vinserti64x2	$2,240(%r13),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm5,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm5,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm5,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm5,%zmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_nrtDGszAnhavlGj
.L_AAD_blocks_14_nrtDGszAnhavlGj:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%ymm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%ymm16,%ymm5,%ymm5
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	32(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	96(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	160(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
	vmovdqu64	224(%r13),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm5,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm5,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm5,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm5,%ymm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_nrtDGszAnhavlGj
.L_AAD_blocks_13_nrtDGszAnhavlGj:
	subq	$1536,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4
	vmovdqu8	192(%r10),%xmm5{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpshufb	%xmm16,%xmm5,%xmm5
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	48(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	112(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	176(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13
	vmovdqu64	240(%r13),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm5,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm5,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm5,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm5,%xmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_nrtDGszAnhavlGj
.L_AAD_blocks_12_nrtDGszAnhavlGj:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	64(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	128(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vmovdqu64	192(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm1,%zmm11,%zmm9
	vpternlogq	$0x96,%zmm6,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm11
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm3
	vpternlogq	$0x96,%zmm7,%zmm11,%zmm12
	vpternlogq	$0x96,%zmm8,%zmm3,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_nrtDGszAnhavlGj
.L_AAD_blocks_11_nrtDGszAnhavlGj:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%zmm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%zmm16,%zmm4,%zmm4
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	80(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	144(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13
	vmovdqu64	208(%r13),%ymm15
	vinserti64x2	$2,240(%r13),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm4,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm4,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm4,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm4,%zmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_nrtDGszAnhavlGj
.L_AAD_blocks_10_nrtDGszAnhavlGj:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%ymm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%ymm16,%ymm4,%ymm4
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	96(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	160(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13
	vmovdqu64	224(%r13),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm4,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm4,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm4,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm4,%ymm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_nrtDGszAnhavlGj
.L_AAD_blocks_9_nrtDGszAnhavlGj:
	subq	$1024,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3
	vmovdqu8	128(%r10),%xmm4{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpshufb	%xmm16,%xmm4,%xmm4
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	112(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	176(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13
	vmovdqu64	240(%r13),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm4,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm4,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm4,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm4,%xmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_nrtDGszAnhavlGj
.L_AAD_blocks_8_nrtDGszAnhavlGj:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	128(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vmovdqu64	192(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm13
	vpxorq	%zmm9,%zmm1,%zmm9
	vpxorq	%zmm10,%zmm6,%zmm10
	vpxorq	%zmm12,%zmm7,%zmm12
	vpxorq	%zmm13,%zmm8,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_nrtDGszAnhavlGj
.L_AAD_blocks_7_nrtDGszAnhavlGj:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%zmm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%zmm16,%zmm3,%zmm3
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	144(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
	vmovdqu64	208(%r13),%ymm15
	vinserti64x2	$2,240(%r13),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm3,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm3,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm3,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm3,%zmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_nrtDGszAnhavlGj
.L_AAD_blocks_6_nrtDGszAnhavlGj:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%ymm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%ymm16,%ymm3,%ymm3
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	160(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
	vmovdqu64	224(%r13),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm3,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm3,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm3,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm3,%ymm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_nrtDGszAnhavlGj
.L_AAD_blocks_5_nrtDGszAnhavlGj:
	subq	$512,%r12
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11
	vmovdqu8	64(%r10),%xmm3{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpshufb	%xmm16,%xmm3,%xmm3
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	176(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13
	vmovdqu64	240(%r13),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm3,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm3,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm3,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm3,%xmm6

	vpxorq	%zmm12,%zmm7,%zmm7
	vpxorq	%zmm13,%zmm8,%zmm8
	vpxorq	%zmm9,%zmm1,%zmm1
	vpxorq	%zmm10,%zmm6,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_nrtDGszAnhavlGj
.L_AAD_blocks_4_nrtDGszAnhavlGj:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	192(%r13),%zmm15
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm9
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm10
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm12
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm13

	vpxorq	%zmm13,%zmm12,%zmm12
	vpsrldq	$8,%zmm12,%zmm7
	vpslldq	$8,%zmm12,%zmm8
	vpxorq	%zmm7,%zmm9,%zmm1
	vpxorq	%zmm8,%zmm10,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_nrtDGszAnhavlGj
.L_AAD_blocks_3_nrtDGszAnhavlGj:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%zmm11{%k1}{z}
	vpshufb	%zmm16,%zmm11,%zmm11
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	208(%r13),%ymm15
	vinserti64x2	$2,240(%r13),%zmm15,%zmm15
	vpclmulqdq	$0x01,%zmm15,%zmm11,%zmm7
	vpclmulqdq	$0x10,%zmm15,%zmm11,%zmm8
	vpclmulqdq	$0x11,%zmm15,%zmm11,%zmm1
	vpclmulqdq	$0x00,%zmm15,%zmm11,%zmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_nrtDGszAnhavlGj
.L_AAD_blocks_2_nrtDGszAnhavlGj:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%ymm11{%k1}{z}
	vpshufb	%ymm16,%ymm11,%ymm11
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	224(%r13),%ymm15
	vpclmulqdq	$0x01,%ymm15,%ymm11,%ymm7
	vpclmulqdq	$0x10,%ymm15,%ymm11,%ymm8
	vpclmulqdq	$0x11,%ymm15,%ymm11,%ymm1
	vpclmulqdq	$0x00,%ymm15,%ymm11,%ymm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

	jmp	.L_CALC_AAD_done_nrtDGszAnhavlGj
.L_AAD_blocks_1_nrtDGszAnhavlGj:
	kmovq	(%r12),%k1
	vmovdqu8	0(%r10),%xmm11{%k1}{z}
	vpshufb	%xmm16,%xmm11,%xmm11
	vpxorq	%zmm2,%zmm11,%zmm11
	vmovdqu64	240(%r13),%xmm15
	vpclmulqdq	$0x01,%xmm15,%xmm11,%xmm7
	vpclmulqdq	$0x10,%xmm15,%xmm11,%xmm8
	vpclmulqdq	$0x11,%xmm15,%xmm11,%xmm1
	vpclmulqdq	$0x00,%xmm15,%xmm11,%xmm6

	vpxorq	%zmm8,%zmm7,%zmm7
	vpsrldq	$8,%zmm7,%zmm12
	vpslldq	$8,%zmm7,%zmm13
	vpxorq	%zmm12,%zmm1,%zmm1
	vpxorq	%zmm13,%zmm6,%zmm6
	vextracti64x4	$1,%zmm1,%ymm12
	vpxorq	%ymm12,%ymm1,%ymm1
	vextracti32x4	$1,%ymm1,%xmm12
	vpxorq	%xmm12,%xmm1,%xmm1
	vextracti64x4	$1,%zmm6,%ymm13
	vpxorq	%ymm13,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm13
	vpxorq	%xmm13,%xmm6,%xmm6
	vmovdqa64	POLY2(%rip),%xmm15


	vpclmulqdq	$0x01,%xmm6,%xmm15,%xmm7
	vpslldq	$8,%xmm7,%xmm7
	vpxorq	%xmm7,%xmm6,%xmm7


	vpclmulqdq	$0x00,%xmm7,%xmm15,%xmm8
	vpsrldq	$4,%xmm8,%xmm8
	vpclmulqdq	$0x10,%xmm7,%xmm15,%xmm2
	vpslldq	$4,%xmm2,%xmm2
	vpternlogq	$0x96,%xmm1,%xmm8,%xmm2

.L_CALC_AAD_done_nrtDGszAnhavlGj:
	movq	%rcx,%r10
	shlq	$3,%r10
	vmovq	%r10,%xmm3


	vpxorq	%xmm2,%xmm3,%xmm2

	vmovdqu64	240(%r13),%xmm1

	vpclmulqdq	$0x11,%xmm1,%xmm2,%xmm11
	vpclmulqdq	$0x00,%xmm1,%xmm2,%xmm3
	vpclmulqdq	$0x01,%xmm1,%xmm2,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm2,%xmm2
	vpxorq	%xmm4,%xmm2,%xmm2

	vpsrldq	$8,%xmm2,%xmm4
	vpslldq	$8,%xmm2,%xmm2
	vpxorq	%xmm4,%xmm11,%xmm11
	vpxorq	%xmm3,%xmm2,%xmm2



	vmovdqu64	POLY2(%rip),%xmm4

	vpclmulqdq	$0x01,%xmm2,%xmm4,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm2,%xmm2



	vpclmulqdq	$0x00,%xmm2,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm2,%xmm4,%xmm2
	vpslldq	$4,%xmm2,%xmm2

	vpternlogq	$0x96,%xmm3,%xmm11,%xmm2

	vpshufb	SHUF_MASK(%rip),%xmm2,%xmm2
	jmp	skip_iv_len_12_init_IV
iv_len_12_init_IV:

	vmovdqu8	ONEf(%rip),%xmm2
	movq	%rdx,%r11
	movl	$0x0000000000000fff,%r10d
	kmovq	%r10,%k1
	vmovdqu8	(%r11),%xmm2{%k1}
skip_iv_len_12_init_IV:
	vmovdqu	%xmm2,%xmm1


	movl	240(%rdi),%r10d
	cmpl	$9,%r10d
	je	.Laes_128_mzAddqDyAyssmFf
	cmpl	$11,%r10d
	je	.Laes_192_mzAddqDyAyssmFf
	cmpl	$13,%r10d
	je	.Laes_256_mzAddqDyAyssmFf
	jmp	.Lexit_aes_mzAddqDyAyssmFf
.align	32
.Laes_128_mzAddqDyAyssmFf:
	vpxorq	0(%rdi),%xmm1,%xmm1

	vaesenc	16(%rdi),%xmm1,%xmm1

	vaesenc	32(%rdi),%xmm1,%xmm1

	vaesenc	48(%rdi),%xmm1,%xmm1

	vaesenc	64(%rdi),%xmm1,%xmm1

	vaesenc	80(%rdi),%xmm1,%xmm1

	vaesenc	96(%rdi),%xmm1,%xmm1

	vaesenc	112(%rdi),%xmm1,%xmm1

	vaesenc	128(%rdi),%xmm1,%xmm1

	vaesenc	144(%rdi),%xmm1,%xmm1

	vaesenclast	160(%rdi),%xmm1,%xmm1
	jmp	.Lexit_aes_mzAddqDyAyssmFf
.align	32
.Laes_192_mzAddqDyAyssmFf:
	vpxorq	0(%rdi),%xmm1,%xmm1

	vaesenc	16(%rdi),%xmm1,%xmm1

	vaesenc	32(%rdi),%xmm1,%xmm1

	vaesenc	48(%rdi),%xmm1,%xmm1

	vaesenc	64(%rdi),%xmm1,%xmm1

	vaesenc	80(%rdi),%xmm1,%xmm1

	vaesenc	96(%rdi),%xmm1,%xmm1

	vaesenc	112(%rdi),%xmm1,%xmm1

	vaesenc	128(%rdi),%xmm1,%xmm1

	vaesenc	144(%rdi),%xmm1,%xmm1

	vaesenc	160(%rdi),%xmm1,%xmm1

	vaesenc	176(%rdi),%xmm1,%xmm1

	vaesenclast	192(%rdi),%xmm1,%xmm1
	jmp	.Lexit_aes_mzAddqDyAyssmFf
.align	32
.Laes_256_mzAddqDyAyssmFf:
	vpxorq	0(%rdi),%xmm1,%xmm1

	vaesenc	16(%rdi),%xmm1,%xmm1

	vaesenc	32(%rdi),%xmm1,%xmm1

	vaesenc	48(%rdi),%xmm1,%xmm1

	vaesenc	64(%rdi),%xmm1,%xmm1

	vaesenc	80(%rdi),%xmm1,%xmm1

	vaesenc	96(%rdi),%xmm1,%xmm1

	vaesenc	112(%rdi),%xmm1,%xmm1

	vaesenc	128(%rdi),%xmm1,%xmm1

	vaesenc	144(%rdi),%xmm1,%xmm1

	vaesenc	160(%rdi),%xmm1,%xmm1

	vaesenc	176(%rdi),%xmm1,%xmm1

	vaesenc	192(%rdi),%xmm1,%xmm1

	vaesenc	208(%rdi),%xmm1,%xmm1

	vaesenclast	224(%rdi),%xmm1,%xmm1
	jmp	.Lexit_aes_mzAddqDyAyssmFf
.Lexit_aes_mzAddqDyAyssmFf:

	vmovdqu	%xmm1,32(%rsi)


	vpshufb	SHUF_MASK(%rip),%xmm2,%xmm2
	vmovdqu	%xmm2,0(%rsi)
.Lexit_setiv:
	cmpq	$256,%rcx
	jbe	.Lskip_hkeys_cleanup_uwrscicCgbqueno
	vpxor	%xmm0,%xmm0,%xmm0
	vmovdqa64	%zmm0,0(%rsp)
	vmovdqa64	%zmm0,64(%rsp)
	vmovdqa64	%zmm0,128(%rsp)
	vmovdqa64	%zmm0,192(%rsp)
	vmovdqa64	%zmm0,256(%rsp)
	vmovdqa64	%zmm0,320(%rsp)
	vmovdqa64	%zmm0,384(%rsp)
	vmovdqa64	%zmm0,448(%rsp)
	vmovdqa64	%zmm0,512(%rsp)
	vmovdqa64	%zmm0,576(%rsp)
	vmovdqa64	%zmm0,640(%rsp)
	vmovdqa64	%zmm0,704(%rsp)
.Lskip_hkeys_cleanup_uwrscicCgbqueno:
	vzeroupper
	leaq	(%rbp),%rsp
.cfi_def_cfa_register	%rsp
	popq	%r15
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r15
	popq	%r14
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r14
	popq	%r13
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r13
	popq	%r12
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r12
	popq	%rbp
.cfi_adjust_cfa_offset	-8
.cfi_restore	%rbp
	popq	%rbx
.cfi_adjust_cfa_offset	-8
.cfi_restore	%rbx
	.byte	0xf3,0xc3
.Lsetiv_seh_end:
.cfi_endproc	
.size	gcm_setiv_avx512, .-gcm_setiv_avx512
.globl	aes_gcm_encrypt_avx512
.hidden aes_gcm_encrypt_avx512
.hidden	aes_gcm_encrypt_avx512
.type	aes_gcm_encrypt_avx512,@function
.align	32
aes_gcm_encrypt_avx512:
.cfi_startproc	
.Lencrypt_seh_begin:
#ifdef BORINGSSL_DISPATCH_TEST
.extern	BORINGSSL_function_hit
.hidden BORINGSSL_function_hit
	movb	$1,BORINGSSL_function_hit+6(%rip)
#endif
.byte	243,15,30,250
	pushq	%rbx
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbx,-16
.Lencrypt_seh_push_rbx:
	pushq	%rbp
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbp,-24
.Lencrypt_seh_push_rbp:
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-32
.Lencrypt_seh_push_r12:
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-40
.Lencrypt_seh_push_r13:
	pushq	%r14
.cfi_adjust_cfa_offset	8
.cfi_offset	%r14,-48
.Lencrypt_seh_push_r14:
	pushq	%r15
.cfi_adjust_cfa_offset	8
.cfi_offset	%r15,-56
.Lencrypt_seh_push_r15:










	leaq	0(%rsp),%rbp
.cfi_def_cfa_register	%rbp
.Lencrypt_seh_setfp:

.Lencrypt_seh_prolog_end:
	subq	$1588,%rsp
	andq	$(-64),%rsp


	movl	240(%rdi),%eax
	cmpl	$9,%eax
	je	.Laes_gcm_encrypt_128_avx512
	cmpl	$11,%eax
	je	.Laes_gcm_encrypt_192_avx512
	cmpl	$13,%eax
	je	.Laes_gcm_encrypt_256_avx512
	xorl	%eax,%eax
	jmp	.Lexit_gcm_encrypt
.align	32
.Laes_gcm_encrypt_128_avx512:
	orq	%r8,%r8
	je	.L_enc_dec_abort_bshwBszxskuCafj
	xorq	%r14,%r14
	vmovdqu64	64(%rsi),%xmm14
	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14

	movl	(%rdx),%eax
	orq	%rax,%rax
	je	.L_partial_block_done_uCmmiftcDelbtyA
	movl	$16,%r10d
	leaq	byte_len_to_mask_table(%rip),%r12
	cmpq	%r10,%r8
	cmovcq	%r8,%r10
	kmovw	(%r12,%r10,2),%k1
	vmovdqu8	(%rcx),%xmm0{%k1}{z}

	vmovdqu64	16(%rsi),%xmm3

	leaq	96(%rsi),%r10
	vmovdqu64	240(%r10),%xmm4



	leaq	SHIFT_MASK(%rip),%r12
	addq	%rax,%r12
	vmovdqu64	(%r12),%xmm5
	vpshufb	%xmm5,%xmm3,%xmm3
	vpxorq	%xmm0,%xmm3,%xmm3


	leaq	(%r8,%rax,1),%r13
	subq	$16,%r13
	jge	.L_no_extra_mask_uCmmiftcDelbtyA
	subq	%r13,%r12
.L_no_extra_mask_uCmmiftcDelbtyA:



	vmovdqu64	16(%r12),%xmm0
	vpand	%xmm0,%xmm3,%xmm3
	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
	vpshufb	%xmm5,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm14,%xmm14
	cmpq	$0,%r13
	jl	.L_partial_incomplete_uCmmiftcDelbtyA

	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm14,%xmm14

	vpsrldq	$8,%xmm14,%xmm11
	vpslldq	$8,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm7,%xmm7
	vpxorq	%xmm10,%xmm14,%xmm14



	vmovdqu64	POLY2(%rip),%xmm11

	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
	vpslldq	$8,%xmm10,%xmm10
	vpxorq	%xmm10,%xmm14,%xmm14



	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
	vpsrldq	$4,%xmm10,%xmm10
	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
	vpslldq	$4,%xmm14,%xmm14

	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14

	movl	$0,(%rdx)

	movq	%rax,%r12
	movq	$16,%rax
	subq	%r12,%rax
	jmp	.L_enc_dec_done_uCmmiftcDelbtyA

.L_partial_incomplete_uCmmiftcDelbtyA:
	addl	%r8d,(%rdx)
	movq	%r8,%rax

.L_enc_dec_done_uCmmiftcDelbtyA:


	leaq	byte_len_to_mask_table(%rip),%r12
	kmovw	(%r12,%rax,2),%k1

	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
	vpshufb	%xmm5,%xmm3,%xmm3
	movq	%r9,%r12
	vmovdqu8	%xmm3,(%r12){%k1}
.L_partial_block_done_uCmmiftcDelbtyA:
	vmovdqu64	0(%rsi),%xmm2
	subq	%rax,%r8
	je	.L_enc_dec_done_bshwBszxskuCafj
	cmpq	$256,%r8
	jbe	.L_message_below_equal_16_blocks_bshwBszxskuCafj

	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
	vmovdqa64	ddq_addbe_1234(%rip),%zmm28






	vmovd	%xmm2,%r15d
	andl	$255,%r15d

	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpshufb	%zmm29,%zmm2,%zmm2



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_hAymnAeerahwBll
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_hAymnAeerahwBll
.L_next_16_overflow_hAymnAeerahwBll:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_hAymnAeerahwBll:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	0(%rcx,%rax,1),%zmm0
	vmovdqu8	64(%rcx,%rax,1),%zmm3
	vmovdqu8	128(%rcx,%rax,1),%zmm4
	vmovdqu8	192(%rcx,%rax,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,0(%r10,%rax,1)
	vmovdqu8	%zmm10,64(%r10,%rax,1)
	vmovdqu8	%zmm11,128(%r10,%rax,1)
	vmovdqu8	%zmm12,192(%r10,%rax,1)

	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
	vmovdqa64	%zmm7,768(%rsp)
	vmovdqa64	%zmm10,832(%rsp)
	vmovdqa64	%zmm11,896(%rsp)
	vmovdqa64	%zmm12,960(%rsp)
	leaq	96(%rsi),%r12
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_ztrCdstpjClcabx

	vmovdqu64	192(%r12),%zmm0
	vmovdqu64	%zmm0,704(%rsp)

	vmovdqu64	128(%r12),%zmm3
	vmovdqu64	%zmm3,640(%rsp)


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	64(%r12),%zmm4
	vmovdqu64	%zmm4,576(%rsp)

	vmovdqu64	0(%r12),%zmm5
	vmovdqu64	%zmm5,512(%rsp)
.L_skip_hkeys_precomputation_ztrCdstpjClcabx:
	cmpq	$512,%r8
	jb	.L_message_below_32_blocks_bshwBszxskuCafj



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_AndxvpGcGtnFGlo
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_AndxvpGcGtnFGlo
.L_next_16_overflow_AndxvpGcGtnFGlo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_AndxvpGcGtnFGlo:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	256(%rcx,%rax,1),%zmm0
	vmovdqu8	320(%rcx,%rax,1),%zmm3
	vmovdqu8	384(%rcx,%rax,1),%zmm4
	vmovdqu8	448(%rcx,%rax,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,256(%r10,%rax,1)
	vmovdqu8	%zmm10,320(%r10,%rax,1)
	vmovdqu8	%zmm11,384(%r10,%rax,1)
	vmovdqu8	%zmm12,448(%r10,%rax,1)

	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
	vmovdqa64	%zmm7,1024(%rsp)
	vmovdqa64	%zmm10,1088(%rsp)
	vmovdqa64	%zmm11,1152(%rsp)
	vmovdqa64	%zmm12,1216(%rsp)
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_hjtyFvomrcgnpwE
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,192(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,128(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,64(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,0(%rsp)
.L_skip_hkeys_precomputation_hjtyFvomrcgnpwE:
	movq	$1,%r14
	addq	$512,%rax
	subq	$512,%r8

	cmpq	$768,%r8
	jb	.L_no_more_big_nblocks_bshwBszxskuCafj
.L_encrypt_big_nblocks_bshwBszxskuCafj:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_odkehfEbtnErkgi
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_odkehfEbtnErkgi
.L_16_blocks_overflow_odkehfEbtnErkgi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_odkehfEbtnErkgi:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_EwDdAtEysyycCpr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_EwDdAtEysyycCpr
.L_16_blocks_overflow_EwDdAtEysyycCpr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_EwDdAtEysyycCpr:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%rax,1),%zmm17
	vmovdqu8	320(%rcx,%rax,1),%zmm19
	vmovdqu8	384(%rcx,%rax,1),%zmm20
	vmovdqu8	448(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%rax,1)
	vmovdqu8	%zmm3,320(%r10,%rax,1)
	vmovdqu8	%zmm4,384(%r10,%rax,1)
	vmovdqu8	%zmm5,448(%r10,%rax,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_oeiElGGbswnxCAv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_oeiElGGbswnxCAv
.L_16_blocks_overflow_oeiElGGbswnxCAv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_oeiElGGbswnxCAv:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	512(%rcx,%rax,1),%zmm17
	vmovdqu8	576(%rcx,%rax,1),%zmm19
	vmovdqu8	640(%rcx,%rax,1),%zmm20
	vmovdqu8	704(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30


	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10

	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
	vpxorq	%zmm24,%zmm6,%zmm6
	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
	vpxorq	%zmm25,%zmm7,%zmm7
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vextracti64x4	$1,%zmm6,%ymm12
	vpxorq	%ymm12,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm12
	vpxorq	%xmm12,%xmm6,%xmm6
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,512(%r10,%rax,1)
	vmovdqu8	%zmm3,576(%r10,%rax,1)
	vmovdqu8	%zmm4,640(%r10,%rax,1)
	vmovdqu8	%zmm5,704(%r10,%rax,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1024(%rsp)
	vmovdqa64	%zmm3,1088(%rsp)
	vmovdqa64	%zmm4,1152(%rsp)
	vmovdqa64	%zmm5,1216(%rsp)
	vmovdqa64	%zmm6,%zmm14

	addq	$768,%rax
	subq	$768,%r8
	cmpq	$768,%r8
	jae	.L_encrypt_big_nblocks_bshwBszxskuCafj

.L_no_more_big_nblocks_bshwBszxskuCafj:

	cmpq	$512,%r8
	jae	.L_encrypt_32_blocks_bshwBszxskuCafj

	cmpq	$256,%r8
	jae	.L_encrypt_16_blocks_bshwBszxskuCafj
.L_encrypt_0_blocks_ghash_32_bshwBszxskuCafj:
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$256,%ebx
	subl	%r10d,%ebx
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	addl	$256,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_mEckjlsvyCnyowu

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_mEckjlsvyCnyowu
	jb	.L_last_num_blocks_is_7_1_mEckjlsvyCnyowu


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_mEckjlsvyCnyowu
	jb	.L_last_num_blocks_is_11_9_mEckjlsvyCnyowu


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_mEckjlsvyCnyowu
	ja	.L_last_num_blocks_is_16_mEckjlsvyCnyowu
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_mEckjlsvyCnyowu
	jmp	.L_last_num_blocks_is_13_mEckjlsvyCnyowu

.L_last_num_blocks_is_11_9_mEckjlsvyCnyowu:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_mEckjlsvyCnyowu
	ja	.L_last_num_blocks_is_11_mEckjlsvyCnyowu
	jmp	.L_last_num_blocks_is_9_mEckjlsvyCnyowu

.L_last_num_blocks_is_7_1_mEckjlsvyCnyowu:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_mEckjlsvyCnyowu
	jb	.L_last_num_blocks_is_3_1_mEckjlsvyCnyowu

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_mEckjlsvyCnyowu
	je	.L_last_num_blocks_is_6_mEckjlsvyCnyowu
	jmp	.L_last_num_blocks_is_5_mEckjlsvyCnyowu

.L_last_num_blocks_is_3_1_mEckjlsvyCnyowu:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_mEckjlsvyCnyowu
	je	.L_last_num_blocks_is_2_mEckjlsvyCnyowu
.L_last_num_blocks_is_1_mEckjlsvyCnyowu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_vFdFGEvccjrqhCm
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_vFdFGEvccjrqhCm

.L_16_blocks_overflow_vFdFGEvccjrqhCm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_vFdFGEvccjrqhCm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_eplqdBzppedihCb





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_eplqdBzppedihCb
.L_small_initial_partial_block_eplqdBzppedihCb:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_eplqdBzppedihCb
.L_small_initial_compute_done_eplqdBzppedihCb:
.L_after_reduction_eplqdBzppedihCb:
	jmp	.L_last_blocks_done_mEckjlsvyCnyowu
.L_last_num_blocks_is_2_mEckjlsvyCnyowu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_cfaExAdfekqnzsx
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_cfaExAdfekqnzsx

.L_16_blocks_overflow_cfaExAdfekqnzsx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_cfaExAdfekqnzsx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_djktBnwsdgwayny





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_djktBnwsdgwayny
.L_small_initial_partial_block_djktBnwsdgwayny:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_djktBnwsdgwayny:

	orq	%r8,%r8
	je	.L_after_reduction_djktBnwsdgwayny
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_djktBnwsdgwayny:
	jmp	.L_last_blocks_done_mEckjlsvyCnyowu
.L_last_num_blocks_is_3_mEckjlsvyCnyowu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_kgGeatmgBBdncku
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_kgGeatmgBBdncku

.L_16_blocks_overflow_kgGeatmgBBdncku:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_kgGeatmgBBdncku:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_lfrfEsFFAlDCzdE





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_lfrfEsFFAlDCzdE
.L_small_initial_partial_block_lfrfEsFFAlDCzdE:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lfrfEsFFAlDCzdE:

	orq	%r8,%r8
	je	.L_after_reduction_lfrfEsFFAlDCzdE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_lfrfEsFFAlDCzdE:
	jmp	.L_last_blocks_done_mEckjlsvyCnyowu
.L_last_num_blocks_is_4_mEckjlsvyCnyowu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_jhjBdhhmetgDvaB
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_jhjBdhhmetgDvaB

.L_16_blocks_overflow_jhjBdhhmetgDvaB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_jhjBdhhmetgDvaB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qFpcweknyzaydfA





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qFpcweknyzaydfA
.L_small_initial_partial_block_qFpcweknyzaydfA:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qFpcweknyzaydfA:

	orq	%r8,%r8
	je	.L_after_reduction_qFpcweknyzaydfA
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_qFpcweknyzaydfA:
	jmp	.L_last_blocks_done_mEckjlsvyCnyowu
.L_last_num_blocks_is_5_mEckjlsvyCnyowu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_FjsvhmvgrpxqFee
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_FjsvhmvgrpxqFee

.L_16_blocks_overflow_FjsvhmvgrpxqFee:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_FjsvhmvgrpxqFee:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vDsAjumDhbsmeie





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vDsAjumDhbsmeie
.L_small_initial_partial_block_vDsAjumDhbsmeie:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_vDsAjumDhbsmeie:

	orq	%r8,%r8
	je	.L_after_reduction_vDsAjumDhbsmeie
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_vDsAjumDhbsmeie:
	jmp	.L_last_blocks_done_mEckjlsvyCnyowu
.L_last_num_blocks_is_6_mEckjlsvyCnyowu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_blvlvtdwviAxjhA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_blvlvtdwviAxjhA

.L_16_blocks_overflow_blvlvtdwviAxjhA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_blvlvtdwviAxjhA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_Gvmgjvgvxzzyucz





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_Gvmgjvgvxzzyucz
.L_small_initial_partial_block_Gvmgjvgvxzzyucz:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_Gvmgjvgvxzzyucz:

	orq	%r8,%r8
	je	.L_after_reduction_Gvmgjvgvxzzyucz
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_Gvmgjvgvxzzyucz:
	jmp	.L_last_blocks_done_mEckjlsvyCnyowu
.L_last_num_blocks_is_7_mEckjlsvyCnyowu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_kFhDDEvFBzftktj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_kFhDDEvFBzftktj

.L_16_blocks_overflow_kFhDDEvFBzftktj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_kFhDDEvFBzftktj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gBonjzClkskFFkp





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gBonjzClkskFFkp
.L_small_initial_partial_block_gBonjzClkskFFkp:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gBonjzClkskFFkp:

	orq	%r8,%r8
	je	.L_after_reduction_gBonjzClkskFFkp
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_gBonjzClkskFFkp:
	jmp	.L_last_blocks_done_mEckjlsvyCnyowu
.L_last_num_blocks_is_8_mEckjlsvyCnyowu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_BvlyoBrvGEmFrgx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_BvlyoBrvGEmFrgx

.L_16_blocks_overflow_BvlyoBrvGEmFrgx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_BvlyoBrvGEmFrgx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qbwwencbptaEtap





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qbwwencbptaEtap
.L_small_initial_partial_block_qbwwencbptaEtap:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qbwwencbptaEtap:

	orq	%r8,%r8
	je	.L_after_reduction_qbwwencbptaEtap
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_qbwwencbptaEtap:
	jmp	.L_last_blocks_done_mEckjlsvyCnyowu
.L_last_num_blocks_is_9_mEckjlsvyCnyowu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_cpyzmwvfltdhhij
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_cpyzmwvfltdhhij

.L_16_blocks_overflow_cpyzmwvfltdhhij:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_cpyzmwvfltdhhij:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xikraonDdhpCbEk





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xikraonDdhpCbEk
.L_small_initial_partial_block_xikraonDdhpCbEk:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xikraonDdhpCbEk:

	orq	%r8,%r8
	je	.L_after_reduction_xikraonDdhpCbEk
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xikraonDdhpCbEk:
	jmp	.L_last_blocks_done_mEckjlsvyCnyowu
.L_last_num_blocks_is_10_mEckjlsvyCnyowu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_nitekiddEwxdyGv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_nitekiddEwxdyGv

.L_16_blocks_overflow_nitekiddEwxdyGv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_nitekiddEwxdyGv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nmybbGrdmbqbvkl





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nmybbGrdmbqbvkl
.L_small_initial_partial_block_nmybbGrdmbqbvkl:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nmybbGrdmbqbvkl:

	orq	%r8,%r8
	je	.L_after_reduction_nmybbGrdmbqbvkl
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_nmybbGrdmbqbvkl:
	jmp	.L_last_blocks_done_mEckjlsvyCnyowu
.L_last_num_blocks_is_11_mEckjlsvyCnyowu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_lkkgpwpDGabsktn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_lkkgpwpDGabsktn

.L_16_blocks_overflow_lkkgpwpDGabsktn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_lkkgpwpDGabsktn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_FEgzdqFuCqdoaxb





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_FEgzdqFuCqdoaxb
.L_small_initial_partial_block_FEgzdqFuCqdoaxb:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_FEgzdqFuCqdoaxb:

	orq	%r8,%r8
	je	.L_after_reduction_FEgzdqFuCqdoaxb
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_FEgzdqFuCqdoaxb:
	jmp	.L_last_blocks_done_mEckjlsvyCnyowu
.L_last_num_blocks_is_12_mEckjlsvyCnyowu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_AosknntnGvdzbdo
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_AosknntnGvdzbdo

.L_16_blocks_overflow_AosknntnGvdzbdo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_AosknntnGvdzbdo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xnAzeppxtbwfCGB





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xnAzeppxtbwfCGB
.L_small_initial_partial_block_xnAzeppxtbwfCGB:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xnAzeppxtbwfCGB:

	orq	%r8,%r8
	je	.L_after_reduction_xnAzeppxtbwfCGB
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xnAzeppxtbwfCGB:
	jmp	.L_last_blocks_done_mEckjlsvyCnyowu
.L_last_num_blocks_is_13_mEckjlsvyCnyowu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_BbGmdeafsbjAiem
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_BbGmdeafsbjAiem

.L_16_blocks_overflow_BbGmdeafsbjAiem:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_BbGmdeafsbjAiem:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jugAmvEfgBscivm





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jugAmvEfgBscivm
.L_small_initial_partial_block_jugAmvEfgBscivm:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jugAmvEfgBscivm:

	orq	%r8,%r8
	je	.L_after_reduction_jugAmvEfgBscivm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jugAmvEfgBscivm:
	jmp	.L_last_blocks_done_mEckjlsvyCnyowu
.L_last_num_blocks_is_14_mEckjlsvyCnyowu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_lFBCtwcwnCvjhAk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_lFBCtwcwnCvjhAk

.L_16_blocks_overflow_lFBCtwcwnCvjhAk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_lFBCtwcwnCvjhAk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xcrcmhxFyBhzCfe





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xcrcmhxFyBhzCfe
.L_small_initial_partial_block_xcrcmhxFyBhzCfe:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xcrcmhxFyBhzCfe:

	orq	%r8,%r8
	je	.L_after_reduction_xcrcmhxFyBhzCfe
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xcrcmhxFyBhzCfe:
	jmp	.L_last_blocks_done_mEckjlsvyCnyowu
.L_last_num_blocks_is_15_mEckjlsvyCnyowu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_ncFzjoumjucEBcf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_ncFzjoumjucEBcf

.L_16_blocks_overflow_ncFzjoumjucEBcf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_ncFzjoumjucEBcf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ggpoxGjwdstgnyf





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ggpoxGjwdstgnyf
.L_small_initial_partial_block_ggpoxGjwdstgnyf:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ggpoxGjwdstgnyf:

	orq	%r8,%r8
	je	.L_after_reduction_ggpoxGjwdstgnyf
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ggpoxGjwdstgnyf:
	jmp	.L_last_blocks_done_mEckjlsvyCnyowu
.L_last_num_blocks_is_16_mEckjlsvyCnyowu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_DxosrcupBjczjvh
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_DxosrcupBjczjvh

.L_16_blocks_overflow_DxosrcupBjczjvh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_DxosrcupBjczjvh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_FkAGAxCziCEFunB:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_FkAGAxCziCEFunB:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_FkAGAxCziCEFunB:
	jmp	.L_last_blocks_done_mEckjlsvyCnyowu
.L_last_num_blocks_is_0_mEckjlsvyCnyowu:
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_mEckjlsvyCnyowu:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_bshwBszxskuCafj
.L_encrypt_32_blocks_bshwBszxskuCafj:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_BzqerEcjFiipdDC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_BzqerEcjFiipdDC
.L_16_blocks_overflow_BzqerEcjFiipdDC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_BzqerEcjFiipdDC:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_eGqkjsyprrlxmvf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_eGqkjsyprrlxmvf
.L_16_blocks_overflow_eGqkjsyprrlxmvf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_eGqkjsyprrlxmvf:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%rax,1),%zmm17
	vmovdqu8	320(%rcx,%rax,1),%zmm19
	vmovdqu8	384(%rcx,%rax,1),%zmm20
	vmovdqu8	448(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%rax,1)
	vmovdqu8	%zmm3,320(%r10,%rax,1)
	vmovdqu8	%zmm4,384(%r10,%rax,1)
	vmovdqu8	%zmm5,448(%r10,%rax,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

	subq	$512,%r8
	addq	$512,%rax
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_dpacqtbmugclAuu

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_dpacqtbmugclAuu
	jb	.L_last_num_blocks_is_7_1_dpacqtbmugclAuu


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_dpacqtbmugclAuu
	jb	.L_last_num_blocks_is_11_9_dpacqtbmugclAuu


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_dpacqtbmugclAuu
	ja	.L_last_num_blocks_is_16_dpacqtbmugclAuu
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_dpacqtbmugclAuu
	jmp	.L_last_num_blocks_is_13_dpacqtbmugclAuu

.L_last_num_blocks_is_11_9_dpacqtbmugclAuu:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_dpacqtbmugclAuu
	ja	.L_last_num_blocks_is_11_dpacqtbmugclAuu
	jmp	.L_last_num_blocks_is_9_dpacqtbmugclAuu

.L_last_num_blocks_is_7_1_dpacqtbmugclAuu:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_dpacqtbmugclAuu
	jb	.L_last_num_blocks_is_3_1_dpacqtbmugclAuu

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_dpacqtbmugclAuu
	je	.L_last_num_blocks_is_6_dpacqtbmugclAuu
	jmp	.L_last_num_blocks_is_5_dpacqtbmugclAuu

.L_last_num_blocks_is_3_1_dpacqtbmugclAuu:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_dpacqtbmugclAuu
	je	.L_last_num_blocks_is_2_dpacqtbmugclAuu
.L_last_num_blocks_is_1_dpacqtbmugclAuu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_xaFiwbEssyoEgpi
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_xaFiwbEssyoEgpi

.L_16_blocks_overflow_xaFiwbEssyoEgpi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_xaFiwbEssyoEgpi:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_tCCaukAsqrdyyaA





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_tCCaukAsqrdyyaA
.L_small_initial_partial_block_tCCaukAsqrdyyaA:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_tCCaukAsqrdyyaA
.L_small_initial_compute_done_tCCaukAsqrdyyaA:
.L_after_reduction_tCCaukAsqrdyyaA:
	jmp	.L_last_blocks_done_dpacqtbmugclAuu
.L_last_num_blocks_is_2_dpacqtbmugclAuu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_BvFmEyfopArceuu
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_BvFmEyfopArceuu

.L_16_blocks_overflow_BvFmEyfopArceuu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_BvFmEyfopArceuu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DkmxiDropFrgmra





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DkmxiDropFrgmra
.L_small_initial_partial_block_DkmxiDropFrgmra:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DkmxiDropFrgmra:

	orq	%r8,%r8
	je	.L_after_reduction_DkmxiDropFrgmra
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DkmxiDropFrgmra:
	jmp	.L_last_blocks_done_dpacqtbmugclAuu
.L_last_num_blocks_is_3_dpacqtbmugclAuu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_CenEueEisEAFhfE
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_CenEueEisEAFhfE

.L_16_blocks_overflow_CenEueEisEAFhfE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_CenEueEisEAFhfE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dxbavFesFcBrBDb





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dxbavFesFcBrBDb
.L_small_initial_partial_block_dxbavFesFcBrBDb:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dxbavFesFcBrBDb:

	orq	%r8,%r8
	je	.L_after_reduction_dxbavFesFcBrBDb
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dxbavFesFcBrBDb:
	jmp	.L_last_blocks_done_dpacqtbmugclAuu
.L_last_num_blocks_is_4_dpacqtbmugclAuu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_gDDxBssqmBFuguA
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_gDDxBssqmBFuguA

.L_16_blocks_overflow_gDDxBssqmBFuguA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_gDDxBssqmBFuguA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_omDwcrrEjAngaFj





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_omDwcrrEjAngaFj
.L_small_initial_partial_block_omDwcrrEjAngaFj:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_omDwcrrEjAngaFj:

	orq	%r8,%r8
	je	.L_after_reduction_omDwcrrEjAngaFj
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_omDwcrrEjAngaFj:
	jmp	.L_last_blocks_done_dpacqtbmugclAuu
.L_last_num_blocks_is_5_dpacqtbmugclAuu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_qsuezCzmEdGgFhj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_qsuezCzmEdGgFhj

.L_16_blocks_overflow_qsuezCzmEdGgFhj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_qsuezCzmEdGgFhj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GEnvhbDjAExvDoi





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GEnvhbDjAExvDoi
.L_small_initial_partial_block_GEnvhbDjAExvDoi:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GEnvhbDjAExvDoi:

	orq	%r8,%r8
	je	.L_after_reduction_GEnvhbDjAExvDoi
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_GEnvhbDjAExvDoi:
	jmp	.L_last_blocks_done_dpacqtbmugclAuu
.L_last_num_blocks_is_6_dpacqtbmugclAuu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_kgaeFfyesgfyfgh
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_kgaeFfyesgfyfgh

.L_16_blocks_overflow_kgaeFfyesgfyfgh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_kgaeFfyesgfyfgh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zgqyvflFrEpDcGc





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zgqyvflFrEpDcGc
.L_small_initial_partial_block_zgqyvflFrEpDcGc:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zgqyvflFrEpDcGc:

	orq	%r8,%r8
	je	.L_after_reduction_zgqyvflFrEpDcGc
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_zgqyvflFrEpDcGc:
	jmp	.L_last_blocks_done_dpacqtbmugclAuu
.L_last_num_blocks_is_7_dpacqtbmugclAuu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_fdiwnBqlrvjuCfC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_fdiwnBqlrvjuCfC

.L_16_blocks_overflow_fdiwnBqlrvjuCfC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_fdiwnBqlrvjuCfC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_aszEjiDbDnrnDaA





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_aszEjiDbDnrnDaA
.L_small_initial_partial_block_aszEjiDbDnrnDaA:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_aszEjiDbDnrnDaA:

	orq	%r8,%r8
	je	.L_after_reduction_aszEjiDbDnrnDaA
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_aszEjiDbDnrnDaA:
	jmp	.L_last_blocks_done_dpacqtbmugclAuu
.L_last_num_blocks_is_8_dpacqtbmugclAuu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_yioBnejxnickvEc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_yioBnejxnickvEc

.L_16_blocks_overflow_yioBnejxnickvEc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_yioBnejxnickvEc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cvnjnFtaoDbCFDm





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cvnjnFtaoDbCFDm
.L_small_initial_partial_block_cvnjnFtaoDbCFDm:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cvnjnFtaoDbCFDm:

	orq	%r8,%r8
	je	.L_after_reduction_cvnjnFtaoDbCFDm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cvnjnFtaoDbCFDm:
	jmp	.L_last_blocks_done_dpacqtbmugclAuu
.L_last_num_blocks_is_9_dpacqtbmugclAuu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_jkfGnsyGCDhkein
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_jkfGnsyGCDhkein

.L_16_blocks_overflow_jkfGnsyGCDhkein:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_jkfGnsyGCDhkein:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_hojqhqmibaCqawt





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_hojqhqmibaCqawt
.L_small_initial_partial_block_hojqhqmibaCqawt:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_hojqhqmibaCqawt:

	orq	%r8,%r8
	je	.L_after_reduction_hojqhqmibaCqawt
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_hojqhqmibaCqawt:
	jmp	.L_last_blocks_done_dpacqtbmugclAuu
.L_last_num_blocks_is_10_dpacqtbmugclAuu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_tobGidAsDhDzgqv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_tobGidAsDhDzgqv

.L_16_blocks_overflow_tobGidAsDhDzgqv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_tobGidAsDhDzgqv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_hlBFbGrillAGGyA





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_hlBFbGrillAGGyA
.L_small_initial_partial_block_hlBFbGrillAGGyA:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_hlBFbGrillAGGyA:

	orq	%r8,%r8
	je	.L_after_reduction_hlBFbGrillAGGyA
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_hlBFbGrillAGGyA:
	jmp	.L_last_blocks_done_dpacqtbmugclAuu
.L_last_num_blocks_is_11_dpacqtbmugclAuu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_EtrelgrmAquttdq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_EtrelgrmAquttdq

.L_16_blocks_overflow_EtrelgrmAquttdq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_EtrelgrmAquttdq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_lfifepoyvAfGqdi





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_lfifepoyvAfGqdi
.L_small_initial_partial_block_lfifepoyvAfGqdi:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lfifepoyvAfGqdi:

	orq	%r8,%r8
	je	.L_after_reduction_lfifepoyvAfGqdi
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_lfifepoyvAfGqdi:
	jmp	.L_last_blocks_done_dpacqtbmugclAuu
.L_last_num_blocks_is_12_dpacqtbmugclAuu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_lyfjaqoaCzboejy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_lyfjaqoaCzboejy

.L_16_blocks_overflow_lyfjaqoaCzboejy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_lyfjaqoaCzboejy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_olDEkwmvexFFFth





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_olDEkwmvexFFFth
.L_small_initial_partial_block_olDEkwmvexFFFth:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_olDEkwmvexFFFth:

	orq	%r8,%r8
	je	.L_after_reduction_olDEkwmvexFFFth
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_olDEkwmvexFFFth:
	jmp	.L_last_blocks_done_dpacqtbmugclAuu
.L_last_num_blocks_is_13_dpacqtbmugclAuu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_rgfekwkGzhthFfu
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_rgfekwkGzhthFfu

.L_16_blocks_overflow_rgfekwkGzhthFfu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_rgfekwkGzhthFfu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nxewlacaDFbqtcv





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nxewlacaDFbqtcv
.L_small_initial_partial_block_nxewlacaDFbqtcv:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nxewlacaDFbqtcv:

	orq	%r8,%r8
	je	.L_after_reduction_nxewlacaDFbqtcv
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_nxewlacaDFbqtcv:
	jmp	.L_last_blocks_done_dpacqtbmugclAuu
.L_last_num_blocks_is_14_dpacqtbmugclAuu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_tiGhatyBEvpzDbe
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_tiGhatyBEvpzDbe

.L_16_blocks_overflow_tiGhatyBEvpzDbe:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_tiGhatyBEvpzDbe:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_esasBbgezbtGnpy





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_esasBbgezbtGnpy
.L_small_initial_partial_block_esasBbgezbtGnpy:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_esasBbgezbtGnpy:

	orq	%r8,%r8
	je	.L_after_reduction_esasBbgezbtGnpy
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_esasBbgezbtGnpy:
	jmp	.L_last_blocks_done_dpacqtbmugclAuu
.L_last_num_blocks_is_15_dpacqtbmugclAuu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_osmxzFatmeEAhsg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_osmxzFatmeEAhsg

.L_16_blocks_overflow_osmxzFatmeEAhsg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_osmxzFatmeEAhsg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_wkxotkAFipfGwwz





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_wkxotkAFipfGwwz
.L_small_initial_partial_block_wkxotkAFipfGwwz:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_wkxotkAFipfGwwz:

	orq	%r8,%r8
	je	.L_after_reduction_wkxotkAFipfGwwz
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_wkxotkAFipfGwwz:
	jmp	.L_last_blocks_done_dpacqtbmugclAuu
.L_last_num_blocks_is_16_dpacqtbmugclAuu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_cgmkfcGnxycjgfs
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_cgmkfcGnxycjgfs

.L_16_blocks_overflow_cgmkfcGnxycjgfs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_cgmkfcGnxycjgfs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_zapeeDwdhlmixbp:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zapeeDwdhlmixbp:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_zapeeDwdhlmixbp:
	jmp	.L_last_blocks_done_dpacqtbmugclAuu
.L_last_num_blocks_is_0_dpacqtbmugclAuu:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_dpacqtbmugclAuu:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_bshwBszxskuCafj
.L_encrypt_16_blocks_bshwBszxskuCafj:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_dnDbnGoxhFrhAnl
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_dnDbnGoxhFrhAnl
.L_16_blocks_overflow_dnDbnGoxhFrhAnl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_dnDbnGoxhFrhAnl:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	256(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	320(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	384(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	448(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	subq	$256,%r8
	addq	$256,%rax
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_prnAortsnrtxtls

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_prnAortsnrtxtls
	jb	.L_last_num_blocks_is_7_1_prnAortsnrtxtls


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_prnAortsnrtxtls
	jb	.L_last_num_blocks_is_11_9_prnAortsnrtxtls


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_prnAortsnrtxtls
	ja	.L_last_num_blocks_is_16_prnAortsnrtxtls
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_prnAortsnrtxtls
	jmp	.L_last_num_blocks_is_13_prnAortsnrtxtls

.L_last_num_blocks_is_11_9_prnAortsnrtxtls:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_prnAortsnrtxtls
	ja	.L_last_num_blocks_is_11_prnAortsnrtxtls
	jmp	.L_last_num_blocks_is_9_prnAortsnrtxtls

.L_last_num_blocks_is_7_1_prnAortsnrtxtls:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_prnAortsnrtxtls
	jb	.L_last_num_blocks_is_3_1_prnAortsnrtxtls

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_prnAortsnrtxtls
	je	.L_last_num_blocks_is_6_prnAortsnrtxtls
	jmp	.L_last_num_blocks_is_5_prnAortsnrtxtls

.L_last_num_blocks_is_3_1_prnAortsnrtxtls:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_prnAortsnrtxtls
	je	.L_last_num_blocks_is_2_prnAortsnrtxtls
.L_last_num_blocks_is_1_prnAortsnrtxtls:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_gAbtAaonnFgGcBl
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_gAbtAaonnFgGcBl

.L_16_blocks_overflow_gAbtAaonnFgGcBl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_gAbtAaonnFgGcBl:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%xmm31,%xmm0,%xmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_tEwErdfeemlBjim





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_tEwErdfeemlBjim
.L_small_initial_partial_block_tEwErdfeemlBjim:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)











	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_tEwErdfeemlBjim
.L_small_initial_compute_done_tEwErdfeemlBjim:
.L_after_reduction_tEwErdfeemlBjim:
	jmp	.L_last_blocks_done_prnAortsnrtxtls
.L_last_num_blocks_is_2_prnAortsnrtxtls:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_rFllGjFzGucphpz
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_rFllGjFzGucphpz

.L_16_blocks_overflow_rFllGjFzGucphpz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_rFllGjFzGucphpz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%ymm31,%ymm0,%ymm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nxzetBvGwbBiprB





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nxzetBvGwbBiprB
.L_small_initial_partial_block_nxzetBvGwbBiprB:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nxzetBvGwbBiprB:

	orq	%r8,%r8
	je	.L_after_reduction_nxzetBvGwbBiprB
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_nxzetBvGwbBiprB:
	jmp	.L_last_blocks_done_prnAortsnrtxtls
.L_last_num_blocks_is_3_prnAortsnrtxtls:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_ohyqkpfCDDeBagn
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_ohyqkpfCDDeBagn

.L_16_blocks_overflow_ohyqkpfCDDeBagn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_ohyqkpfCDDeBagn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nhxaEtFtsptuqrb





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nhxaEtFtsptuqrb
.L_small_initial_partial_block_nhxaEtFtsptuqrb:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nhxaEtFtsptuqrb:

	orq	%r8,%r8
	je	.L_after_reduction_nhxaEtFtsptuqrb
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_nhxaEtFtsptuqrb:
	jmp	.L_last_blocks_done_prnAortsnrtxtls
.L_last_num_blocks_is_4_prnAortsnrtxtls:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_vxccdygaGcmzCkf
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_vxccdygaGcmzCkf

.L_16_blocks_overflow_vxccdygaGcmzCkf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_vxccdygaGcmzCkf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_luvgyevdhjBlbFC





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_luvgyevdhjBlbFC
.L_small_initial_partial_block_luvgyevdhjBlbFC:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_luvgyevdhjBlbFC:

	orq	%r8,%r8
	je	.L_after_reduction_luvgyevdhjBlbFC
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_luvgyevdhjBlbFC:
	jmp	.L_last_blocks_done_prnAortsnrtxtls
.L_last_num_blocks_is_5_prnAortsnrtxtls:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_xchtpobtwyibqeb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_xchtpobtwyibqeb

.L_16_blocks_overflow_xchtpobtwyibqeb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_xchtpobtwyibqeb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_uedlGasDdcyqjgC





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_uedlGasDdcyqjgC
.L_small_initial_partial_block_uedlGasDdcyqjgC:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_uedlGasDdcyqjgC:

	orq	%r8,%r8
	je	.L_after_reduction_uedlGasDdcyqjgC
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_uedlGasDdcyqjgC:
	jmp	.L_last_blocks_done_prnAortsnrtxtls
.L_last_num_blocks_is_6_prnAortsnrtxtls:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_DgFlGebkCaztGqm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_DgFlGebkCaztGqm

.L_16_blocks_overflow_DgFlGebkCaztGqm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_DgFlGebkCaztGqm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zuanpoywnzjdixm





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zuanpoywnzjdixm
.L_small_initial_partial_block_zuanpoywnzjdixm:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zuanpoywnzjdixm:

	orq	%r8,%r8
	je	.L_after_reduction_zuanpoywnzjdixm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_zuanpoywnzjdixm:
	jmp	.L_last_blocks_done_prnAortsnrtxtls
.L_last_num_blocks_is_7_prnAortsnrtxtls:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_pCfnybyrjnvBiue
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_pCfnybyrjnvBiue

.L_16_blocks_overflow_pCfnybyrjnvBiue:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_pCfnybyrjnvBiue:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ifvrijtjgroouhj





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ifvrijtjgroouhj
.L_small_initial_partial_block_ifvrijtjgroouhj:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ifvrijtjgroouhj:

	orq	%r8,%r8
	je	.L_after_reduction_ifvrijtjgroouhj
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ifvrijtjgroouhj:
	jmp	.L_last_blocks_done_prnAortsnrtxtls
.L_last_num_blocks_is_8_prnAortsnrtxtls:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_DoFaAyqgeeattsE
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_DoFaAyqgeeattsE

.L_16_blocks_overflow_DoFaAyqgeeattsE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_DoFaAyqgeeattsE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DtwxecivpjDqwAA





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DtwxecivpjDqwAA
.L_small_initial_partial_block_DtwxecivpjDqwAA:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DtwxecivpjDqwAA:

	orq	%r8,%r8
	je	.L_after_reduction_DtwxecivpjDqwAA
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DtwxecivpjDqwAA:
	jmp	.L_last_blocks_done_prnAortsnrtxtls
.L_last_num_blocks_is_9_prnAortsnrtxtls:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_fuxEhBxykqnhFru
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_fuxEhBxykqnhFru

.L_16_blocks_overflow_fuxEhBxykqnhFru:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_fuxEhBxykqnhFru:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_kgxhnlAxrmevsDy





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_kgxhnlAxrmevsDy
.L_small_initial_partial_block_kgxhnlAxrmevsDy:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_kgxhnlAxrmevsDy:

	orq	%r8,%r8
	je	.L_after_reduction_kgxhnlAxrmevsDy
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_kgxhnlAxrmevsDy:
	jmp	.L_last_blocks_done_prnAortsnrtxtls
.L_last_num_blocks_is_10_prnAortsnrtxtls:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_qmrvarxyyzsaCvv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_qmrvarxyyzsaCvv

.L_16_blocks_overflow_qmrvarxyyzsaCvv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_qmrvarxyyzsaCvv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qoAoepkxmkevCpa





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qoAoepkxmkevCpa
.L_small_initial_partial_block_qoAoepkxmkevCpa:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qoAoepkxmkevCpa:

	orq	%r8,%r8
	je	.L_after_reduction_qoAoepkxmkevCpa
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_qoAoepkxmkevCpa:
	jmp	.L_last_blocks_done_prnAortsnrtxtls
.L_last_num_blocks_is_11_prnAortsnrtxtls:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_BsleyGDflvpcvgb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_BsleyGDflvpcvgb

.L_16_blocks_overflow_BsleyGDflvpcvgb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_BsleyGDflvpcvgb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_uBoAxkzrnkddtFh





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_uBoAxkzrnkddtFh
.L_small_initial_partial_block_uBoAxkzrnkddtFh:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_uBoAxkzrnkddtFh:

	orq	%r8,%r8
	je	.L_after_reduction_uBoAxkzrnkddtFh
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_uBoAxkzrnkddtFh:
	jmp	.L_last_blocks_done_prnAortsnrtxtls
.L_last_num_blocks_is_12_prnAortsnrtxtls:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_ewhdGkeuaCdlEjy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_ewhdGkeuaCdlEjy

.L_16_blocks_overflow_ewhdGkeuaCdlEjy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_ewhdGkeuaCdlEjy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DeqyFoucwiecixl





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DeqyFoucwiecixl
.L_small_initial_partial_block_DeqyFoucwiecixl:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DeqyFoucwiecixl:

	orq	%r8,%r8
	je	.L_after_reduction_DeqyFoucwiecixl
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DeqyFoucwiecixl:
	jmp	.L_last_blocks_done_prnAortsnrtxtls
.L_last_num_blocks_is_13_prnAortsnrtxtls:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_EeCElEuxqxqvfiB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_EeCElEuxqxqvfiB

.L_16_blocks_overflow_EeCElEuxqxqvfiB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_EeCElEuxqxqvfiB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jqxjvoFFrhyFugk





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jqxjvoFFrhyFugk
.L_small_initial_partial_block_jqxjvoFFrhyFugk:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jqxjvoFFrhyFugk:

	orq	%r8,%r8
	je	.L_after_reduction_jqxjvoFFrhyFugk
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jqxjvoFFrhyFugk:
	jmp	.L_last_blocks_done_prnAortsnrtxtls
.L_last_num_blocks_is_14_prnAortsnrtxtls:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_fdBtDoGxExbxdky
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_fdBtDoGxExbxdky

.L_16_blocks_overflow_fdBtDoGxExbxdky:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_fdBtDoGxExbxdky:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DzzAhhjxdrkhcom





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DzzAhhjxdrkhcom
.L_small_initial_partial_block_DzzAhhjxdrkhcom:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DzzAhhjxdrkhcom:

	orq	%r8,%r8
	je	.L_after_reduction_DzzAhhjxdrkhcom
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DzzAhhjxdrkhcom:
	jmp	.L_last_blocks_done_prnAortsnrtxtls
.L_last_num_blocks_is_15_prnAortsnrtxtls:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_kGbxartBdyDFCaa
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_kGbxartBdyDFCaa

.L_16_blocks_overflow_kGbxartBdyDFCaa:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_kGbxartBdyDFCaa:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_byCGGtpnlBsofkE





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_byCGGtpnlBsofkE
.L_small_initial_partial_block_byCGGtpnlBsofkE:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_byCGGtpnlBsofkE:

	orq	%r8,%r8
	je	.L_after_reduction_byCGGtpnlBsofkE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_byCGGtpnlBsofkE:
	jmp	.L_last_blocks_done_prnAortsnrtxtls
.L_last_num_blocks_is_16_prnAortsnrtxtls:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_dxnEjCDgewlnacr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_dxnEjCDgewlnacr

.L_16_blocks_overflow_dxnEjCDgewlnacr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_dxnEjCDgewlnacr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_kEEEpCfuraqpFth:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_kEEEpCfuraqpFth:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_kEEEpCfuraqpFth:
	jmp	.L_last_blocks_done_prnAortsnrtxtls
.L_last_num_blocks_is_0_prnAortsnrtxtls:
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_prnAortsnrtxtls:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_bshwBszxskuCafj

.L_message_below_32_blocks_bshwBszxskuCafj:


	subq	$256,%r8
	addq	$256,%rax
	movl	%r8d,%r10d
	leaq	96(%rsi),%r12
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_trrADvjwEDjAxrl
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)
.L_skip_hkeys_precomputation_trrADvjwEDjAxrl:
	movq	$1,%r14
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_gmgpscyiomkdFCx

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_gmgpscyiomkdFCx
	jb	.L_last_num_blocks_is_7_1_gmgpscyiomkdFCx


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_gmgpscyiomkdFCx
	jb	.L_last_num_blocks_is_11_9_gmgpscyiomkdFCx


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_gmgpscyiomkdFCx
	ja	.L_last_num_blocks_is_16_gmgpscyiomkdFCx
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_gmgpscyiomkdFCx
	jmp	.L_last_num_blocks_is_13_gmgpscyiomkdFCx

.L_last_num_blocks_is_11_9_gmgpscyiomkdFCx:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_gmgpscyiomkdFCx
	ja	.L_last_num_blocks_is_11_gmgpscyiomkdFCx
	jmp	.L_last_num_blocks_is_9_gmgpscyiomkdFCx

.L_last_num_blocks_is_7_1_gmgpscyiomkdFCx:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_gmgpscyiomkdFCx
	jb	.L_last_num_blocks_is_3_1_gmgpscyiomkdFCx

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_gmgpscyiomkdFCx
	je	.L_last_num_blocks_is_6_gmgpscyiomkdFCx
	jmp	.L_last_num_blocks_is_5_gmgpscyiomkdFCx

.L_last_num_blocks_is_3_1_gmgpscyiomkdFCx:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_gmgpscyiomkdFCx
	je	.L_last_num_blocks_is_2_gmgpscyiomkdFCx
.L_last_num_blocks_is_1_gmgpscyiomkdFCx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_BofrBsivDsebyBf
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_BofrBsivDsebyBf

.L_16_blocks_overflow_BofrBsivDsebyBf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_BofrBsivDsebyBf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_tGgEojjqeueynuy





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_tGgEojjqeueynuy
.L_small_initial_partial_block_tGgEojjqeueynuy:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_tGgEojjqeueynuy
.L_small_initial_compute_done_tGgEojjqeueynuy:
.L_after_reduction_tGgEojjqeueynuy:
	jmp	.L_last_blocks_done_gmgpscyiomkdFCx
.L_last_num_blocks_is_2_gmgpscyiomkdFCx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_rcGemnvAmbilxxn
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_rcGemnvAmbilxxn

.L_16_blocks_overflow_rcGemnvAmbilxxn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_rcGemnvAmbilxxn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_FufiaEsptwCvnbG





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_FufiaEsptwCvnbG
.L_small_initial_partial_block_FufiaEsptwCvnbG:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_FufiaEsptwCvnbG:

	orq	%r8,%r8
	je	.L_after_reduction_FufiaEsptwCvnbG
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_FufiaEsptwCvnbG:
	jmp	.L_last_blocks_done_gmgpscyiomkdFCx
.L_last_num_blocks_is_3_gmgpscyiomkdFCx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_AoaFcjkjwrzkEwa
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_AoaFcjkjwrzkEwa

.L_16_blocks_overflow_AoaFcjkjwrzkEwa:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_AoaFcjkjwrzkEwa:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qmcxErqsjdmetCx





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qmcxErqsjdmetCx
.L_small_initial_partial_block_qmcxErqsjdmetCx:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qmcxErqsjdmetCx:

	orq	%r8,%r8
	je	.L_after_reduction_qmcxErqsjdmetCx
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_qmcxErqsjdmetCx:
	jmp	.L_last_blocks_done_gmgpscyiomkdFCx
.L_last_num_blocks_is_4_gmgpscyiomkdFCx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_AFfrjobgmaftGrx
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_AFfrjobgmaftGrx

.L_16_blocks_overflow_AFfrjobgmaftGrx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_AFfrjobgmaftGrx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qtuCjDDqqjbBAnF





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qtuCjDDqqjbBAnF
.L_small_initial_partial_block_qtuCjDDqqjbBAnF:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qtuCjDDqqjbBAnF:

	orq	%r8,%r8
	je	.L_after_reduction_qtuCjDDqqjbBAnF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_qtuCjDDqqjbBAnF:
	jmp	.L_last_blocks_done_gmgpscyiomkdFCx
.L_last_num_blocks_is_5_gmgpscyiomkdFCx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_zpzkciiBvDaoDbA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_zpzkciiBvDaoDbA

.L_16_blocks_overflow_zpzkciiBvDaoDbA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_zpzkciiBvDaoDbA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DsEEyxqiqlnEnlv





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DsEEyxqiqlnEnlv
.L_small_initial_partial_block_DsEEyxqiqlnEnlv:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DsEEyxqiqlnEnlv:

	orq	%r8,%r8
	je	.L_after_reduction_DsEEyxqiqlnEnlv
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DsEEyxqiqlnEnlv:
	jmp	.L_last_blocks_done_gmgpscyiomkdFCx
.L_last_num_blocks_is_6_gmgpscyiomkdFCx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_tobgujBFvvacvCz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_tobgujBFvvacvCz

.L_16_blocks_overflow_tobgujBFvvacvCz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_tobgujBFvvacvCz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_fkludECGnathFdt





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_fkludECGnathFdt
.L_small_initial_partial_block_fkludECGnathFdt:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_fkludECGnathFdt:

	orq	%r8,%r8
	je	.L_after_reduction_fkludECGnathFdt
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_fkludECGnathFdt:
	jmp	.L_last_blocks_done_gmgpscyiomkdFCx
.L_last_num_blocks_is_7_gmgpscyiomkdFCx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_pcpCFhgckfkeeyr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_pcpCFhgckfkeeyr

.L_16_blocks_overflow_pcpCFhgckfkeeyr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_pcpCFhgckfkeeyr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GjhkGavcvGylkrl





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GjhkGavcvGylkrl
.L_small_initial_partial_block_GjhkGavcvGylkrl:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GjhkGavcvGylkrl:

	orq	%r8,%r8
	je	.L_after_reduction_GjhkGavcvGylkrl
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_GjhkGavcvGylkrl:
	jmp	.L_last_blocks_done_gmgpscyiomkdFCx
.L_last_num_blocks_is_8_gmgpscyiomkdFCx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_hDyostdafkhCypi
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_hDyostdafkhCypi

.L_16_blocks_overflow_hDyostdafkhCypi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_hDyostdafkhCypi:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_igArBhdBCyjqEah





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_igArBhdBCyjqEah
.L_small_initial_partial_block_igArBhdBCyjqEah:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_igArBhdBCyjqEah:

	orq	%r8,%r8
	je	.L_after_reduction_igArBhdBCyjqEah
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_igArBhdBCyjqEah:
	jmp	.L_last_blocks_done_gmgpscyiomkdFCx
.L_last_num_blocks_is_9_gmgpscyiomkdFCx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_ADBkjfxaqpgDwwc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_ADBkjfxaqpgDwwc

.L_16_blocks_overflow_ADBkjfxaqpgDwwc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_ADBkjfxaqpgDwwc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xbgocDGhkCtsstx





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xbgocDGhkCtsstx
.L_small_initial_partial_block_xbgocDGhkCtsstx:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xbgocDGhkCtsstx:

	orq	%r8,%r8
	je	.L_after_reduction_xbgocDGhkCtsstx
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xbgocDGhkCtsstx:
	jmp	.L_last_blocks_done_gmgpscyiomkdFCx
.L_last_num_blocks_is_10_gmgpscyiomkdFCx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_tBdxcFgloGrlEAB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_tBdxcFgloGrlEAB

.L_16_blocks_overflow_tBdxcFgloGrlEAB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_tBdxcFgloGrlEAB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ggouvbyquglxcpn





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ggouvbyquglxcpn
.L_small_initial_partial_block_ggouvbyquglxcpn:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ggouvbyquglxcpn:

	orq	%r8,%r8
	je	.L_after_reduction_ggouvbyquglxcpn
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ggouvbyquglxcpn:
	jmp	.L_last_blocks_done_gmgpscyiomkdFCx
.L_last_num_blocks_is_11_gmgpscyiomkdFCx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_drGwfeeppkejrsE
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_drGwfeeppkejrsE

.L_16_blocks_overflow_drGwfeeppkejrsE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_drGwfeeppkejrsE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_iljmbwhiBhfEvma





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_iljmbwhiBhfEvma
.L_small_initial_partial_block_iljmbwhiBhfEvma:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_iljmbwhiBhfEvma:

	orq	%r8,%r8
	je	.L_after_reduction_iljmbwhiBhfEvma
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_iljmbwhiBhfEvma:
	jmp	.L_last_blocks_done_gmgpscyiomkdFCx
.L_last_num_blocks_is_12_gmgpscyiomkdFCx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_uxiuBAEdmvDftmn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_uxiuBAEdmvDftmn

.L_16_blocks_overflow_uxiuBAEdmvDftmn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_uxiuBAEdmvDftmn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_owxbrfAcssopoyA





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_owxbrfAcssopoyA
.L_small_initial_partial_block_owxbrfAcssopoyA:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_owxbrfAcssopoyA:

	orq	%r8,%r8
	je	.L_after_reduction_owxbrfAcssopoyA
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_owxbrfAcssopoyA:
	jmp	.L_last_blocks_done_gmgpscyiomkdFCx
.L_last_num_blocks_is_13_gmgpscyiomkdFCx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_kEAyCcpwahraddz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_kEAyCcpwahraddz

.L_16_blocks_overflow_kEAyCcpwahraddz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_kEAyCcpwahraddz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EtdChAozouoqfri





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EtdChAozouoqfri
.L_small_initial_partial_block_EtdChAozouoqfri:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_EtdChAozouoqfri:

	orq	%r8,%r8
	je	.L_after_reduction_EtdChAozouoqfri
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_EtdChAozouoqfri:
	jmp	.L_last_blocks_done_gmgpscyiomkdFCx
.L_last_num_blocks_is_14_gmgpscyiomkdFCx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_wBfAguilirsGwaj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_wBfAguilirsGwaj

.L_16_blocks_overflow_wBfAguilirsGwaj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_wBfAguilirsGwaj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DyucsgyabizrvCt





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DyucsgyabizrvCt
.L_small_initial_partial_block_DyucsgyabizrvCt:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DyucsgyabizrvCt:

	orq	%r8,%r8
	je	.L_after_reduction_DyucsgyabizrvCt
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DyucsgyabizrvCt:
	jmp	.L_last_blocks_done_gmgpscyiomkdFCx
.L_last_num_blocks_is_15_gmgpscyiomkdFCx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_zBfluyFqjGufzhg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_zBfluyFqjGufzhg

.L_16_blocks_overflow_zBfluyFqjGufzhg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_zBfluyFqjGufzhg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_FFCxFEcuAxiDnag





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_FFCxFEcuAxiDnag
.L_small_initial_partial_block_FFCxFEcuAxiDnag:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_FFCxFEcuAxiDnag:

	orq	%r8,%r8
	je	.L_after_reduction_FFCxFEcuAxiDnag
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_FFCxFEcuAxiDnag:
	jmp	.L_last_blocks_done_gmgpscyiomkdFCx
.L_last_num_blocks_is_16_gmgpscyiomkdFCx:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_wfmBCigezDCACgG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_wfmBCigezDCACgG

.L_16_blocks_overflow_wfmBCigezDCACgG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_wfmBCigezDCACgG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_lrevftbAeocdocE:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lrevftbAeocdocE:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_lrevftbAeocdocE:
	jmp	.L_last_blocks_done_gmgpscyiomkdFCx
.L_last_num_blocks_is_0_gmgpscyiomkdFCx:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_gmgpscyiomkdFCx:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_bshwBszxskuCafj

.L_message_below_equal_16_blocks_bshwBszxskuCafj:


	movl	%r8d,%r12d
	addl	$15,%r12d
	shrl	$4,%r12d
	cmpq	$8,%r12
	je	.L_small_initial_num_blocks_is_8_flxgwBpvexGooFf
	jl	.L_small_initial_num_blocks_is_7_1_flxgwBpvexGooFf


	cmpq	$12,%r12
	je	.L_small_initial_num_blocks_is_12_flxgwBpvexGooFf
	jl	.L_small_initial_num_blocks_is_11_9_flxgwBpvexGooFf


	cmpq	$16,%r12
	je	.L_small_initial_num_blocks_is_16_flxgwBpvexGooFf
	cmpq	$15,%r12
	je	.L_small_initial_num_blocks_is_15_flxgwBpvexGooFf
	cmpq	$14,%r12
	je	.L_small_initial_num_blocks_is_14_flxgwBpvexGooFf
	jmp	.L_small_initial_num_blocks_is_13_flxgwBpvexGooFf

.L_small_initial_num_blocks_is_11_9_flxgwBpvexGooFf:

	cmpq	$11,%r12
	je	.L_small_initial_num_blocks_is_11_flxgwBpvexGooFf
	cmpq	$10,%r12
	je	.L_small_initial_num_blocks_is_10_flxgwBpvexGooFf
	jmp	.L_small_initial_num_blocks_is_9_flxgwBpvexGooFf

.L_small_initial_num_blocks_is_7_1_flxgwBpvexGooFf:
	cmpq	$4,%r12
	je	.L_small_initial_num_blocks_is_4_flxgwBpvexGooFf
	jl	.L_small_initial_num_blocks_is_3_1_flxgwBpvexGooFf

	cmpq	$7,%r12
	je	.L_small_initial_num_blocks_is_7_flxgwBpvexGooFf
	cmpq	$6,%r12
	je	.L_small_initial_num_blocks_is_6_flxgwBpvexGooFf
	jmp	.L_small_initial_num_blocks_is_5_flxgwBpvexGooFf

.L_small_initial_num_blocks_is_3_1_flxgwBpvexGooFf:

	cmpq	$3,%r12
	je	.L_small_initial_num_blocks_is_3_flxgwBpvexGooFf
	cmpq	$2,%r12
	je	.L_small_initial_num_blocks_is_2_flxgwBpvexGooFf





.L_small_initial_num_blocks_is_1_flxgwBpvexGooFf:
	vmovdqa64	SHUF_MASK(%rip),%xmm29
	vpaddd	ONE(%rip),%xmm2,%xmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm0,%xmm2
	vpshufb	%xmm29,%xmm0,%xmm0
	vmovdqu8	0(%rcx,%rax,1),%xmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%xmm15,%xmm0,%xmm0
	vpxorq	%xmm6,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm6
	vextracti32x4	$0,%zmm6,%xmm13
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_wooFxoAkwCBngzD





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_wooFxoAkwCBngzD
.L_small_initial_partial_block_wooFxoAkwCBngzD:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)











	vpxorq	%xmm13,%xmm14,%xmm14

	jmp	.L_after_reduction_wooFxoAkwCBngzD
.L_small_initial_compute_done_wooFxoAkwCBngzD:
.L_after_reduction_wooFxoAkwCBngzD:
	jmp	.L_small_initial_blocks_encrypted_flxgwBpvexGooFf
.L_small_initial_num_blocks_is_2_flxgwBpvexGooFf:
	vmovdqa64	SHUF_MASK(%rip),%ymm29
	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm0,%xmm2
	vpshufb	%ymm29,%ymm0,%ymm0
	vmovdqu8	0(%rcx,%rax,1),%ymm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%ymm15,%ymm0,%ymm0
	vpxorq	%ymm6,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm6
	vextracti32x4	$1,%zmm6,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_FbqfAmrBqiGfqff





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_FbqfAmrBqiGfqff
.L_small_initial_partial_block_FbqfAmrBqiGfqff:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_FbqfAmrBqiGfqff:

	orq	%r8,%r8
	je	.L_after_reduction_FbqfAmrBqiGfqff
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_FbqfAmrBqiGfqff:
	jmp	.L_small_initial_blocks_encrypted_flxgwBpvexGooFf
.L_small_initial_num_blocks_is_3_flxgwBpvexGooFf:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vextracti32x4	$2,%zmm6,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DqoyFgDutBezDEt





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DqoyFgDutBezDEt
.L_small_initial_partial_block_DqoyFgDutBezDEt:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DqoyFgDutBezDEt:

	orq	%r8,%r8
	je	.L_after_reduction_DqoyFgDutBezDEt
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_DqoyFgDutBezDEt:
	jmp	.L_small_initial_blocks_encrypted_flxgwBpvexGooFf
.L_small_initial_num_blocks_is_4_flxgwBpvexGooFf:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vextracti32x4	$3,%zmm6,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_mpmGusqdBcyBuxt





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_mpmGusqdBcyBuxt
.L_small_initial_partial_block_mpmGusqdBcyBuxt:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_mpmGusqdBcyBuxt:

	orq	%r8,%r8
	je	.L_after_reduction_mpmGusqdBcyBuxt
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_mpmGusqdBcyBuxt:
	jmp	.L_small_initial_blocks_encrypted_flxgwBpvexGooFf
.L_small_initial_num_blocks_is_5_flxgwBpvexGooFf:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%xmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%xmm15,%xmm3,%xmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%xmm7,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%xmm29,%xmm3,%xmm7
	vextracti32x4	$0,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EdBCiyzByFFCoiz





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EdBCiyzByFFCoiz
.L_small_initial_partial_block_EdBCiyzByFFCoiz:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_EdBCiyzByFFCoiz:

	orq	%r8,%r8
	je	.L_after_reduction_EdBCiyzByFFCoiz
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_EdBCiyzByFFCoiz:
	jmp	.L_small_initial_blocks_encrypted_flxgwBpvexGooFf
.L_small_initial_num_blocks_is_6_flxgwBpvexGooFf:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%ymm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%ymm15,%ymm3,%ymm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%ymm7,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%ymm29,%ymm3,%ymm7
	vextracti32x4	$1,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gwDbfcidDufxhgC





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gwDbfcidDufxhgC
.L_small_initial_partial_block_gwDbfcidDufxhgC:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gwDbfcidDufxhgC:

	orq	%r8,%r8
	je	.L_after_reduction_gwDbfcidDufxhgC
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_gwDbfcidDufxhgC:
	jmp	.L_small_initial_blocks_encrypted_flxgwBpvexGooFf
.L_small_initial_num_blocks_is_7_flxgwBpvexGooFf:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vextracti32x4	$2,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bknGnqkepythAhl





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bknGnqkepythAhl
.L_small_initial_partial_block_bknGnqkepythAhl:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bknGnqkepythAhl:

	orq	%r8,%r8
	je	.L_after_reduction_bknGnqkepythAhl
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_bknGnqkepythAhl:
	jmp	.L_small_initial_blocks_encrypted_flxgwBpvexGooFf
.L_small_initial_num_blocks_is_8_flxgwBpvexGooFf:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vextracti32x4	$3,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_lvmFolqtAmqCCep





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_lvmFolqtAmqCCep
.L_small_initial_partial_block_lvmFolqtAmqCCep:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lvmFolqtAmqCCep:

	orq	%r8,%r8
	je	.L_after_reduction_lvmFolqtAmqCCep
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_lvmFolqtAmqCCep:
	jmp	.L_small_initial_blocks_encrypted_flxgwBpvexGooFf
.L_small_initial_num_blocks_is_9_flxgwBpvexGooFf:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%xmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%xmm15,%xmm4,%xmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%xmm10,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%xmm29,%xmm4,%xmm10
	vextracti32x4	$0,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_oipFkzxdixanqmc





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_oipFkzxdixanqmc
.L_small_initial_partial_block_oipFkzxdixanqmc:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_oipFkzxdixanqmc:

	orq	%r8,%r8
	je	.L_after_reduction_oipFkzxdixanqmc
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_oipFkzxdixanqmc:
	jmp	.L_small_initial_blocks_encrypted_flxgwBpvexGooFf
.L_small_initial_num_blocks_is_10_flxgwBpvexGooFf:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%ymm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%ymm15,%ymm4,%ymm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%ymm10,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%ymm29,%ymm4,%ymm10
	vextracti32x4	$1,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_hvqzDjvqaydAeFF





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_hvqzDjvqaydAeFF
.L_small_initial_partial_block_hvqzDjvqaydAeFF:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_hvqzDjvqaydAeFF:

	orq	%r8,%r8
	je	.L_after_reduction_hvqzDjvqaydAeFF
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_hvqzDjvqaydAeFF:
	jmp	.L_small_initial_blocks_encrypted_flxgwBpvexGooFf
.L_small_initial_num_blocks_is_11_flxgwBpvexGooFf:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vextracti32x4	$2,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ClpBiykioindGea





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ClpBiykioindGea
.L_small_initial_partial_block_ClpBiykioindGea:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ClpBiykioindGea:

	orq	%r8,%r8
	je	.L_after_reduction_ClpBiykioindGea
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_ClpBiykioindGea:
	jmp	.L_small_initial_blocks_encrypted_flxgwBpvexGooFf
.L_small_initial_num_blocks_is_12_flxgwBpvexGooFf:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vextracti32x4	$3,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_FndnqarltAAxwjm





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	64(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_FndnqarltAAxwjm
.L_small_initial_partial_block_FndnqarltAAxwjm:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_FndnqarltAAxwjm:

	orq	%r8,%r8
	je	.L_after_reduction_FndnqarltAAxwjm
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_FndnqarltAAxwjm:
	jmp	.L_small_initial_blocks_encrypted_flxgwBpvexGooFf
.L_small_initial_num_blocks_is_13_flxgwBpvexGooFf:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%xmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%xmm15,%xmm5,%xmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%xmm11,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%xmm29,%xmm5,%xmm11
	vextracti32x4	$0,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_AphmyBcawgqCBkv





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	48(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_AphmyBcawgqCBkv
.L_small_initial_partial_block_AphmyBcawgqCBkv:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	64(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_AphmyBcawgqCBkv:

	orq	%r8,%r8
	je	.L_after_reduction_AphmyBcawgqCBkv
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_AphmyBcawgqCBkv:
	jmp	.L_small_initial_blocks_encrypted_flxgwBpvexGooFf
.L_small_initial_num_blocks_is_14_flxgwBpvexGooFf:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%ymm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%ymm15,%ymm5,%ymm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%ymm11,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%ymm29,%ymm5,%ymm11
	vextracti32x4	$1,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DqFvahwlCickihg





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	32(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DqFvahwlCickihg
.L_small_initial_partial_block_DqFvahwlCickihg:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	48(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DqFvahwlCickihg:

	orq	%r8,%r8
	je	.L_after_reduction_DqFvahwlCickihg
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_DqFvahwlCickihg:
	jmp	.L_small_initial_blocks_encrypted_flxgwBpvexGooFf
.L_small_initial_num_blocks_is_15_flxgwBpvexGooFf:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%zmm29,%zmm5,%zmm11
	vextracti32x4	$2,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_iyCDqjynGqfnzui





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	16(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_iyCDqjynGqfnzui
.L_small_initial_partial_block_iyCDqjynGqfnzui:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	32(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_iyCDqjynGqfnzui:

	orq	%r8,%r8
	je	.L_after_reduction_iyCDqjynGqfnzui
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_iyCDqjynGqfnzui:
	jmp	.L_small_initial_blocks_encrypted_flxgwBpvexGooFf
.L_small_initial_num_blocks_is_16_flxgwBpvexGooFf:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%zmm29,%zmm5,%zmm11
	vextracti32x4	$3,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_uawzgiFwtpkfFFt:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	16(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_uawzgiFwtpkfFFt:
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_uawzgiFwtpkfFFt:
.L_small_initial_blocks_encrypted_flxgwBpvexGooFf:
.L_ghash_done_bshwBszxskuCafj:
	vmovdqu64	%xmm2,0(%rsi)
.L_enc_dec_done_bshwBszxskuCafj:

	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14
	vmovdqu64	%xmm14,64(%rsi)
.L_enc_dec_abort_bshwBszxskuCafj:
	jmp	.Lexit_gcm_encrypt
.align	32
.Laes_gcm_encrypt_192_avx512:
	orq	%r8,%r8
	je	.L_enc_dec_abort_EqpenwamdmlBqwm
	xorq	%r14,%r14
	vmovdqu64	64(%rsi),%xmm14
	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14

	movl	(%rdx),%eax
	orq	%rax,%rax
	je	.L_partial_block_done_xhnvGyetEDyCwgm
	movl	$16,%r10d
	leaq	byte_len_to_mask_table(%rip),%r12
	cmpq	%r10,%r8
	cmovcq	%r8,%r10
	kmovw	(%r12,%r10,2),%k1
	vmovdqu8	(%rcx),%xmm0{%k1}{z}

	vmovdqu64	16(%rsi),%xmm3

	leaq	96(%rsi),%r10
	vmovdqu64	240(%r10),%xmm4



	leaq	SHIFT_MASK(%rip),%r12
	addq	%rax,%r12
	vmovdqu64	(%r12),%xmm5
	vpshufb	%xmm5,%xmm3,%xmm3
	vpxorq	%xmm0,%xmm3,%xmm3


	leaq	(%r8,%rax,1),%r13
	subq	$16,%r13
	jge	.L_no_extra_mask_xhnvGyetEDyCwgm
	subq	%r13,%r12
.L_no_extra_mask_xhnvGyetEDyCwgm:



	vmovdqu64	16(%r12),%xmm0
	vpand	%xmm0,%xmm3,%xmm3
	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
	vpshufb	%xmm5,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm14,%xmm14
	cmpq	$0,%r13
	jl	.L_partial_incomplete_xhnvGyetEDyCwgm

	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm14,%xmm14

	vpsrldq	$8,%xmm14,%xmm11
	vpslldq	$8,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm7,%xmm7
	vpxorq	%xmm10,%xmm14,%xmm14



	vmovdqu64	POLY2(%rip),%xmm11

	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
	vpslldq	$8,%xmm10,%xmm10
	vpxorq	%xmm10,%xmm14,%xmm14



	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
	vpsrldq	$4,%xmm10,%xmm10
	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
	vpslldq	$4,%xmm14,%xmm14

	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14

	movl	$0,(%rdx)

	movq	%rax,%r12
	movq	$16,%rax
	subq	%r12,%rax
	jmp	.L_enc_dec_done_xhnvGyetEDyCwgm

.L_partial_incomplete_xhnvGyetEDyCwgm:
	addl	%r8d,(%rdx)
	movq	%r8,%rax

.L_enc_dec_done_xhnvGyetEDyCwgm:


	leaq	byte_len_to_mask_table(%rip),%r12
	kmovw	(%r12,%rax,2),%k1

	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
	vpshufb	%xmm5,%xmm3,%xmm3
	movq	%r9,%r12
	vmovdqu8	%xmm3,(%r12){%k1}
.L_partial_block_done_xhnvGyetEDyCwgm:
	vmovdqu64	0(%rsi),%xmm2
	subq	%rax,%r8
	je	.L_enc_dec_done_EqpenwamdmlBqwm
	cmpq	$256,%r8
	jbe	.L_message_below_equal_16_blocks_EqpenwamdmlBqwm

	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
	vmovdqa64	ddq_addbe_1234(%rip),%zmm28






	vmovd	%xmm2,%r15d
	andl	$255,%r15d

	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpshufb	%zmm29,%zmm2,%zmm2



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_DohwitxdsGzBcuA
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_DohwitxdsGzBcuA
.L_next_16_overflow_DohwitxdsGzBcuA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_DohwitxdsGzBcuA:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	0(%rcx,%rax,1),%zmm0
	vmovdqu8	64(%rcx,%rax,1),%zmm3
	vmovdqu8	128(%rcx,%rax,1),%zmm4
	vmovdqu8	192(%rcx,%rax,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,0(%r10,%rax,1)
	vmovdqu8	%zmm10,64(%r10,%rax,1)
	vmovdqu8	%zmm11,128(%r10,%rax,1)
	vmovdqu8	%zmm12,192(%r10,%rax,1)

	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
	vmovdqa64	%zmm7,768(%rsp)
	vmovdqa64	%zmm10,832(%rsp)
	vmovdqa64	%zmm11,896(%rsp)
	vmovdqa64	%zmm12,960(%rsp)
	leaq	96(%rsi),%r12
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_hArBilDzEeoBpyh

	vmovdqu64	192(%r12),%zmm0
	vmovdqu64	%zmm0,704(%rsp)

	vmovdqu64	128(%r12),%zmm3
	vmovdqu64	%zmm3,640(%rsp)


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	64(%r12),%zmm4
	vmovdqu64	%zmm4,576(%rsp)

	vmovdqu64	0(%r12),%zmm5
	vmovdqu64	%zmm5,512(%rsp)
.L_skip_hkeys_precomputation_hArBilDzEeoBpyh:
	cmpq	$512,%r8
	jb	.L_message_below_32_blocks_EqpenwamdmlBqwm



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_ghAdgcpEBgzyADb
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_ghAdgcpEBgzyADb
.L_next_16_overflow_ghAdgcpEBgzyADb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_ghAdgcpEBgzyADb:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	256(%rcx,%rax,1),%zmm0
	vmovdqu8	320(%rcx,%rax,1),%zmm3
	vmovdqu8	384(%rcx,%rax,1),%zmm4
	vmovdqu8	448(%rcx,%rax,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,256(%r10,%rax,1)
	vmovdqu8	%zmm10,320(%r10,%rax,1)
	vmovdqu8	%zmm11,384(%r10,%rax,1)
	vmovdqu8	%zmm12,448(%r10,%rax,1)

	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
	vmovdqa64	%zmm7,1024(%rsp)
	vmovdqa64	%zmm10,1088(%rsp)
	vmovdqa64	%zmm11,1152(%rsp)
	vmovdqa64	%zmm12,1216(%rsp)
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_lFoCmxGxnndwbfk
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,192(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,128(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,64(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,0(%rsp)
.L_skip_hkeys_precomputation_lFoCmxGxnndwbfk:
	movq	$1,%r14
	addq	$512,%rax
	subq	$512,%r8

	cmpq	$768,%r8
	jb	.L_no_more_big_nblocks_EqpenwamdmlBqwm
.L_encrypt_big_nblocks_EqpenwamdmlBqwm:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_BecnpuxAbxoBaGd
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_BecnpuxAbxoBaGd
.L_16_blocks_overflow_BecnpuxAbxoBaGd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_BecnpuxAbxoBaGd:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_mDDyAnkvaokDton
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_mDDyAnkvaokDton
.L_16_blocks_overflow_mDDyAnkvaokDton:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_mDDyAnkvaokDton:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%rax,1),%zmm17
	vmovdqu8	320(%rcx,%rax,1),%zmm19
	vmovdqu8	384(%rcx,%rax,1),%zmm20
	vmovdqu8	448(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%rax,1)
	vmovdqu8	%zmm3,320(%r10,%rax,1)
	vmovdqu8	%zmm4,384(%r10,%rax,1)
	vmovdqu8	%zmm5,448(%r10,%rax,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_xomBznBCzsbnCfA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_xomBznBCzsbnCfA
.L_16_blocks_overflow_xomBznBCzsbnCfA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_xomBznBCzsbnCfA:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	512(%rcx,%rax,1),%zmm17
	vmovdqu8	576(%rcx,%rax,1),%zmm19
	vmovdqu8	640(%rcx,%rax,1),%zmm20
	vmovdqu8	704(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30


	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10

	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
	vpxorq	%zmm24,%zmm6,%zmm6
	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
	vpxorq	%zmm25,%zmm7,%zmm7
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vextracti64x4	$1,%zmm6,%ymm12
	vpxorq	%ymm12,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm12
	vpxorq	%xmm12,%xmm6,%xmm6
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,512(%r10,%rax,1)
	vmovdqu8	%zmm3,576(%r10,%rax,1)
	vmovdqu8	%zmm4,640(%r10,%rax,1)
	vmovdqu8	%zmm5,704(%r10,%rax,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1024(%rsp)
	vmovdqa64	%zmm3,1088(%rsp)
	vmovdqa64	%zmm4,1152(%rsp)
	vmovdqa64	%zmm5,1216(%rsp)
	vmovdqa64	%zmm6,%zmm14

	addq	$768,%rax
	subq	$768,%r8
	cmpq	$768,%r8
	jae	.L_encrypt_big_nblocks_EqpenwamdmlBqwm

.L_no_more_big_nblocks_EqpenwamdmlBqwm:

	cmpq	$512,%r8
	jae	.L_encrypt_32_blocks_EqpenwamdmlBqwm

	cmpq	$256,%r8
	jae	.L_encrypt_16_blocks_EqpenwamdmlBqwm
.L_encrypt_0_blocks_ghash_32_EqpenwamdmlBqwm:
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$256,%ebx
	subl	%r10d,%ebx
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	addl	$256,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_yEpzgdhCirtwmxe

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_yEpzgdhCirtwmxe
	jb	.L_last_num_blocks_is_7_1_yEpzgdhCirtwmxe


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_yEpzgdhCirtwmxe
	jb	.L_last_num_blocks_is_11_9_yEpzgdhCirtwmxe


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_yEpzgdhCirtwmxe
	ja	.L_last_num_blocks_is_16_yEpzgdhCirtwmxe
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_yEpzgdhCirtwmxe
	jmp	.L_last_num_blocks_is_13_yEpzgdhCirtwmxe

.L_last_num_blocks_is_11_9_yEpzgdhCirtwmxe:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_yEpzgdhCirtwmxe
	ja	.L_last_num_blocks_is_11_yEpzgdhCirtwmxe
	jmp	.L_last_num_blocks_is_9_yEpzgdhCirtwmxe

.L_last_num_blocks_is_7_1_yEpzgdhCirtwmxe:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_yEpzgdhCirtwmxe
	jb	.L_last_num_blocks_is_3_1_yEpzgdhCirtwmxe

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_yEpzgdhCirtwmxe
	je	.L_last_num_blocks_is_6_yEpzgdhCirtwmxe
	jmp	.L_last_num_blocks_is_5_yEpzgdhCirtwmxe

.L_last_num_blocks_is_3_1_yEpzgdhCirtwmxe:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_yEpzgdhCirtwmxe
	je	.L_last_num_blocks_is_2_yEpzgdhCirtwmxe
.L_last_num_blocks_is_1_yEpzgdhCirtwmxe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_yFknzoACDhbcnlt
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_yFknzoACDhbcnlt

.L_16_blocks_overflow_yFknzoACDhbcnlt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_yFknzoACDhbcnlt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EklhivgCyoxypFB





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EklhivgCyoxypFB
.L_small_initial_partial_block_EklhivgCyoxypFB:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_EklhivgCyoxypFB
.L_small_initial_compute_done_EklhivgCyoxypFB:
.L_after_reduction_EklhivgCyoxypFB:
	jmp	.L_last_blocks_done_yEpzgdhCirtwmxe
.L_last_num_blocks_is_2_yEpzgdhCirtwmxe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_gikqrasDoEylpkh
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_gikqrasDoEylpkh

.L_16_blocks_overflow_gikqrasDoEylpkh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_gikqrasDoEylpkh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_lBhGhxsqGyhhicf





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_lBhGhxsqGyhhicf
.L_small_initial_partial_block_lBhGhxsqGyhhicf:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lBhGhxsqGyhhicf:

	orq	%r8,%r8
	je	.L_after_reduction_lBhGhxsqGyhhicf
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_lBhGhxsqGyhhicf:
	jmp	.L_last_blocks_done_yEpzgdhCirtwmxe
.L_last_num_blocks_is_3_yEpzgdhCirtwmxe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_gqcqvpvwbxxdrnj
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_gqcqvpvwbxxdrnj

.L_16_blocks_overflow_gqcqvpvwbxxdrnj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_gqcqvpvwbxxdrnj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DxFpkhhDlGoGzbo





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DxFpkhhDlGoGzbo
.L_small_initial_partial_block_DxFpkhhDlGoGzbo:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DxFpkhhDlGoGzbo:

	orq	%r8,%r8
	je	.L_after_reduction_DxFpkhhDlGoGzbo
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DxFpkhhDlGoGzbo:
	jmp	.L_last_blocks_done_yEpzgdhCirtwmxe
.L_last_num_blocks_is_4_yEpzgdhCirtwmxe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_bGpbcdaxltffyBu
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_bGpbcdaxltffyBu

.L_16_blocks_overflow_bGpbcdaxltffyBu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_bGpbcdaxltffyBu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vGFxjdwutDrezad





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vGFxjdwutDrezad
.L_small_initial_partial_block_vGFxjdwutDrezad:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_vGFxjdwutDrezad:

	orq	%r8,%r8
	je	.L_after_reduction_vGFxjdwutDrezad
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_vGFxjdwutDrezad:
	jmp	.L_last_blocks_done_yEpzgdhCirtwmxe
.L_last_num_blocks_is_5_yEpzgdhCirtwmxe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_ffrxvBrlBhbvfCz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_ffrxvBrlBhbvfCz

.L_16_blocks_overflow_ffrxvBrlBhbvfCz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_ffrxvBrlBhbvfCz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_erkyivvuaygBnrw





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_erkyivvuaygBnrw
.L_small_initial_partial_block_erkyivvuaygBnrw:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_erkyivvuaygBnrw:

	orq	%r8,%r8
	je	.L_after_reduction_erkyivvuaygBnrw
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_erkyivvuaygBnrw:
	jmp	.L_last_blocks_done_yEpzgdhCirtwmxe
.L_last_num_blocks_is_6_yEpzgdhCirtwmxe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_EnuadpxbCEmlvig
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_EnuadpxbCEmlvig

.L_16_blocks_overflow_EnuadpxbCEmlvig:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_EnuadpxbCEmlvig:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bfqjwxCqvtDwgqn





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bfqjwxCqvtDwgqn
.L_small_initial_partial_block_bfqjwxCqvtDwgqn:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bfqjwxCqvtDwgqn:

	orq	%r8,%r8
	je	.L_after_reduction_bfqjwxCqvtDwgqn
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_bfqjwxCqvtDwgqn:
	jmp	.L_last_blocks_done_yEpzgdhCirtwmxe
.L_last_num_blocks_is_7_yEpzgdhCirtwmxe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_nszpnzrBjjkeGFB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_nszpnzrBjjkeGFB

.L_16_blocks_overflow_nszpnzrBjjkeGFB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_nszpnzrBjjkeGFB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_pemqvkDCBzGqbmm





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_pemqvkDCBzGqbmm
.L_small_initial_partial_block_pemqvkDCBzGqbmm:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_pemqvkDCBzGqbmm:

	orq	%r8,%r8
	je	.L_after_reduction_pemqvkDCBzGqbmm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_pemqvkDCBzGqbmm:
	jmp	.L_last_blocks_done_yEpzgdhCirtwmxe
.L_last_num_blocks_is_8_yEpzgdhCirtwmxe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_nshiAbwyFavnpgu
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_nshiAbwyFavnpgu

.L_16_blocks_overflow_nshiAbwyFavnpgu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_nshiAbwyFavnpgu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dilecvmDqvzwGuf





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dilecvmDqvzwGuf
.L_small_initial_partial_block_dilecvmDqvzwGuf:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dilecvmDqvzwGuf:

	orq	%r8,%r8
	je	.L_after_reduction_dilecvmDqvzwGuf
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dilecvmDqvzwGuf:
	jmp	.L_last_blocks_done_yEpzgdhCirtwmxe
.L_last_num_blocks_is_9_yEpzgdhCirtwmxe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_fvDdfrxrhuiaAcD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_fvDdfrxrhuiaAcD

.L_16_blocks_overflow_fvDdfrxrhuiaAcD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_fvDdfrxrhuiaAcD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_fEqyvjyqtgDhfqv





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_fEqyvjyqtgDhfqv
.L_small_initial_partial_block_fEqyvjyqtgDhfqv:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_fEqyvjyqtgDhfqv:

	orq	%r8,%r8
	je	.L_after_reduction_fEqyvjyqtgDhfqv
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_fEqyvjyqtgDhfqv:
	jmp	.L_last_blocks_done_yEpzgdhCirtwmxe
.L_last_num_blocks_is_10_yEpzgdhCirtwmxe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_sCvlDajqndjFfdh
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_sCvlDajqndjFfdh

.L_16_blocks_overflow_sCvlDajqndjFfdh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_sCvlDajqndjFfdh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ktgrvtuczadxxrv





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ktgrvtuczadxxrv
.L_small_initial_partial_block_ktgrvtuczadxxrv:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ktgrvtuczadxxrv:

	orq	%r8,%r8
	je	.L_after_reduction_ktgrvtuczadxxrv
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ktgrvtuczadxxrv:
	jmp	.L_last_blocks_done_yEpzgdhCirtwmxe
.L_last_num_blocks_is_11_yEpzgdhCirtwmxe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_wqFutuArpCfDAGt
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_wqFutuArpCfDAGt

.L_16_blocks_overflow_wqFutuArpCfDAGt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_wqFutuArpCfDAGt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jyyEiFxegeDhBoD





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jyyEiFxegeDhBoD
.L_small_initial_partial_block_jyyEiFxegeDhBoD:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jyyEiFxegeDhBoD:

	orq	%r8,%r8
	je	.L_after_reduction_jyyEiFxegeDhBoD
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jyyEiFxegeDhBoD:
	jmp	.L_last_blocks_done_yEpzgdhCirtwmxe
.L_last_num_blocks_is_12_yEpzgdhCirtwmxe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_DgpBjaDuyzErcvr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_DgpBjaDuyzErcvr

.L_16_blocks_overflow_DgpBjaDuyzErcvr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_DgpBjaDuyzErcvr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rmDFcBkwdGhsAyy





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rmDFcBkwdGhsAyy
.L_small_initial_partial_block_rmDFcBkwdGhsAyy:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rmDFcBkwdGhsAyy:

	orq	%r8,%r8
	je	.L_after_reduction_rmDFcBkwdGhsAyy
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_rmDFcBkwdGhsAyy:
	jmp	.L_last_blocks_done_yEpzgdhCirtwmxe
.L_last_num_blocks_is_13_yEpzgdhCirtwmxe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_jsnEnBglaCbgtqE
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_jsnEnBglaCbgtqE

.L_16_blocks_overflow_jsnEnBglaCbgtqE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_jsnEnBglaCbgtqE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_wGBxkxeADbAyduC





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_wGBxkxeADbAyduC
.L_small_initial_partial_block_wGBxkxeADbAyduC:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_wGBxkxeADbAyduC:

	orq	%r8,%r8
	je	.L_after_reduction_wGBxkxeADbAyduC
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_wGBxkxeADbAyduC:
	jmp	.L_last_blocks_done_yEpzgdhCirtwmxe
.L_last_num_blocks_is_14_yEpzgdhCirtwmxe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_zcoEadFmBEfghcc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_zcoEadFmBEfghcc

.L_16_blocks_overflow_zcoEadFmBEfghcc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_zcoEadFmBEfghcc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_frciwyBgEjEGfnt





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_frciwyBgEjEGfnt
.L_small_initial_partial_block_frciwyBgEjEGfnt:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_frciwyBgEjEGfnt:

	orq	%r8,%r8
	je	.L_after_reduction_frciwyBgEjEGfnt
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_frciwyBgEjEGfnt:
	jmp	.L_last_blocks_done_yEpzgdhCirtwmxe
.L_last_num_blocks_is_15_yEpzgdhCirtwmxe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_GocozBhzeeocckc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_GocozBhzeeocckc

.L_16_blocks_overflow_GocozBhzeeocckc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_GocozBhzeeocckc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ctvkbhlAsjylnyC





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ctvkbhlAsjylnyC
.L_small_initial_partial_block_ctvkbhlAsjylnyC:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ctvkbhlAsjylnyC:

	orq	%r8,%r8
	je	.L_after_reduction_ctvkbhlAsjylnyC
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ctvkbhlAsjylnyC:
	jmp	.L_last_blocks_done_yEpzgdhCirtwmxe
.L_last_num_blocks_is_16_yEpzgdhCirtwmxe:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_hAqDecidskrysFs
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_hAqDecidskrysFs

.L_16_blocks_overflow_hAqDecidskrysFs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_hAqDecidskrysFs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_nhvjqkcfEGqiBka:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nhvjqkcfEGqiBka:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_nhvjqkcfEGqiBka:
	jmp	.L_last_blocks_done_yEpzgdhCirtwmxe
.L_last_num_blocks_is_0_yEpzgdhCirtwmxe:
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_yEpzgdhCirtwmxe:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_EqpenwamdmlBqwm
.L_encrypt_32_blocks_EqpenwamdmlBqwm:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_sckuiamerezCchD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_sckuiamerezCchD
.L_16_blocks_overflow_sckuiamerezCchD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_sckuiamerezCchD:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_ypxatvytvspewwk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_ypxatvytvspewwk
.L_16_blocks_overflow_ypxatvytvspewwk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_ypxatvytvspewwk:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%rax,1),%zmm17
	vmovdqu8	320(%rcx,%rax,1),%zmm19
	vmovdqu8	384(%rcx,%rax,1),%zmm20
	vmovdqu8	448(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%rax,1)
	vmovdqu8	%zmm3,320(%r10,%rax,1)
	vmovdqu8	%zmm4,384(%r10,%rax,1)
	vmovdqu8	%zmm5,448(%r10,%rax,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

	subq	$512,%r8
	addq	$512,%rax
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_qtbxpegyubisrwf

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_qtbxpegyubisrwf
	jb	.L_last_num_blocks_is_7_1_qtbxpegyubisrwf


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_qtbxpegyubisrwf
	jb	.L_last_num_blocks_is_11_9_qtbxpegyubisrwf


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_qtbxpegyubisrwf
	ja	.L_last_num_blocks_is_16_qtbxpegyubisrwf
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_qtbxpegyubisrwf
	jmp	.L_last_num_blocks_is_13_qtbxpegyubisrwf

.L_last_num_blocks_is_11_9_qtbxpegyubisrwf:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_qtbxpegyubisrwf
	ja	.L_last_num_blocks_is_11_qtbxpegyubisrwf
	jmp	.L_last_num_blocks_is_9_qtbxpegyubisrwf

.L_last_num_blocks_is_7_1_qtbxpegyubisrwf:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_qtbxpegyubisrwf
	jb	.L_last_num_blocks_is_3_1_qtbxpegyubisrwf

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_qtbxpegyubisrwf
	je	.L_last_num_blocks_is_6_qtbxpegyubisrwf
	jmp	.L_last_num_blocks_is_5_qtbxpegyubisrwf

.L_last_num_blocks_is_3_1_qtbxpegyubisrwf:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_qtbxpegyubisrwf
	je	.L_last_num_blocks_is_2_qtbxpegyubisrwf
.L_last_num_blocks_is_1_qtbxpegyubisrwf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_teCwcpogecadmot
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_teCwcpogecadmot

.L_16_blocks_overflow_teCwcpogecadmot:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_teCwcpogecadmot:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bAuuzfFlpDsACua





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bAuuzfFlpDsACua
.L_small_initial_partial_block_bAuuzfFlpDsACua:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_bAuuzfFlpDsACua
.L_small_initial_compute_done_bAuuzfFlpDsACua:
.L_after_reduction_bAuuzfFlpDsACua:
	jmp	.L_last_blocks_done_qtbxpegyubisrwf
.L_last_num_blocks_is_2_qtbxpegyubisrwf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_gxjhqnkEGnvtuhC
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_gxjhqnkEGnvtuhC

.L_16_blocks_overflow_gxjhqnkEGnvtuhC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_gxjhqnkEGnvtuhC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gkizvrqDtmuEqob





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gkizvrqDtmuEqob
.L_small_initial_partial_block_gkizvrqDtmuEqob:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gkizvrqDtmuEqob:

	orq	%r8,%r8
	je	.L_after_reduction_gkizvrqDtmuEqob
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_gkizvrqDtmuEqob:
	jmp	.L_last_blocks_done_qtbxpegyubisrwf
.L_last_num_blocks_is_3_qtbxpegyubisrwf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_vedofxinAxtCjql
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_vedofxinAxtCjql

.L_16_blocks_overflow_vedofxinAxtCjql:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_vedofxinAxtCjql:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_sjnmCkmcrrspdsb





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_sjnmCkmcrrspdsb
.L_small_initial_partial_block_sjnmCkmcrrspdsb:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_sjnmCkmcrrspdsb:

	orq	%r8,%r8
	je	.L_after_reduction_sjnmCkmcrrspdsb
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_sjnmCkmcrrspdsb:
	jmp	.L_last_blocks_done_qtbxpegyubisrwf
.L_last_num_blocks_is_4_qtbxpegyubisrwf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_slAmhcpvFvzhAse
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_slAmhcpvFvzhAse

.L_16_blocks_overflow_slAmhcpvFvzhAse:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_slAmhcpvFvzhAse:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rwdakaofbpzFwwn





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rwdakaofbpzFwwn
.L_small_initial_partial_block_rwdakaofbpzFwwn:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rwdakaofbpzFwwn:

	orq	%r8,%r8
	je	.L_after_reduction_rwdakaofbpzFwwn
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_rwdakaofbpzFwwn:
	jmp	.L_last_blocks_done_qtbxpegyubisrwf
.L_last_num_blocks_is_5_qtbxpegyubisrwf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_ybzCnoiFwluDwor
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_ybzCnoiFwluDwor

.L_16_blocks_overflow_ybzCnoiFwluDwor:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_ybzCnoiFwluDwor:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_twfnmyDhndEwEBi





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_twfnmyDhndEwEBi
.L_small_initial_partial_block_twfnmyDhndEwEBi:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_twfnmyDhndEwEBi:

	orq	%r8,%r8
	je	.L_after_reduction_twfnmyDhndEwEBi
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_twfnmyDhndEwEBi:
	jmp	.L_last_blocks_done_qtbxpegyubisrwf
.L_last_num_blocks_is_6_qtbxpegyubisrwf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_pldumFoqllbyvtp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_pldumFoqllbyvtp

.L_16_blocks_overflow_pldumFoqllbyvtp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_pldumFoqllbyvtp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_owBpfpCulnabhjj





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_owBpfpCulnabhjj
.L_small_initial_partial_block_owBpfpCulnabhjj:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_owBpfpCulnabhjj:

	orq	%r8,%r8
	je	.L_after_reduction_owBpfpCulnabhjj
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_owBpfpCulnabhjj:
	jmp	.L_last_blocks_done_qtbxpegyubisrwf
.L_last_num_blocks_is_7_qtbxpegyubisrwf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_bsxmdqDDkftfzyh
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_bsxmdqDDkftfzyh

.L_16_blocks_overflow_bsxmdqDDkftfzyh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_bsxmdqDDkftfzyh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_BGxnbDBkEyopcbn





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_BGxnbDBkEyopcbn
.L_small_initial_partial_block_BGxnbDBkEyopcbn:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_BGxnbDBkEyopcbn:

	orq	%r8,%r8
	je	.L_after_reduction_BGxnbDBkEyopcbn
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_BGxnbDBkEyopcbn:
	jmp	.L_last_blocks_done_qtbxpegyubisrwf
.L_last_num_blocks_is_8_qtbxpegyubisrwf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_xDuAButaCydgnwv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_xDuAButaCydgnwv

.L_16_blocks_overflow_xDuAButaCydgnwv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_xDuAButaCydgnwv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_eysFgrcngiDiAyp





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_eysFgrcngiDiAyp
.L_small_initial_partial_block_eysFgrcngiDiAyp:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_eysFgrcngiDiAyp:

	orq	%r8,%r8
	je	.L_after_reduction_eysFgrcngiDiAyp
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_eysFgrcngiDiAyp:
	jmp	.L_last_blocks_done_qtbxpegyubisrwf
.L_last_num_blocks_is_9_qtbxpegyubisrwf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_ggDdvyliBAoadzw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_ggDdvyliBAoadzw

.L_16_blocks_overflow_ggDdvyliBAoadzw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_ggDdvyliBAoadzw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_kuiuDohCmEaollg





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_kuiuDohCmEaollg
.L_small_initial_partial_block_kuiuDohCmEaollg:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_kuiuDohCmEaollg:

	orq	%r8,%r8
	je	.L_after_reduction_kuiuDohCmEaollg
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_kuiuDohCmEaollg:
	jmp	.L_last_blocks_done_qtbxpegyubisrwf
.L_last_num_blocks_is_10_qtbxpegyubisrwf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_ntoEoouAqnhfjwt
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_ntoEoouAqnhfjwt

.L_16_blocks_overflow_ntoEoouAqnhfjwt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_ntoEoouAqnhfjwt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vAozuctwmvuoAAD





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vAozuctwmvuoAAD
.L_small_initial_partial_block_vAozuctwmvuoAAD:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_vAozuctwmvuoAAD:

	orq	%r8,%r8
	je	.L_after_reduction_vAozuctwmvuoAAD
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_vAozuctwmvuoAAD:
	jmp	.L_last_blocks_done_qtbxpegyubisrwf
.L_last_num_blocks_is_11_qtbxpegyubisrwf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_iDrviodfxzbthGn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_iDrviodfxzbthGn

.L_16_blocks_overflow_iDrviodfxzbthGn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_iDrviodfxzbthGn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_BcAvucGgtpABzas





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_BcAvucGgtpABzas
.L_small_initial_partial_block_BcAvucGgtpABzas:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_BcAvucGgtpABzas:

	orq	%r8,%r8
	je	.L_after_reduction_BcAvucGgtpABzas
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_BcAvucGgtpABzas:
	jmp	.L_last_blocks_done_qtbxpegyubisrwf
.L_last_num_blocks_is_12_qtbxpegyubisrwf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_xampyxlvxDqfuoi
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_xampyxlvxDqfuoi

.L_16_blocks_overflow_xampyxlvxDqfuoi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_xampyxlvxDqfuoi:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_iGyADlovricGypb





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_iGyADlovricGypb
.L_small_initial_partial_block_iGyADlovricGypb:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_iGyADlovricGypb:

	orq	%r8,%r8
	je	.L_after_reduction_iGyADlovricGypb
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_iGyADlovricGypb:
	jmp	.L_last_blocks_done_qtbxpegyubisrwf
.L_last_num_blocks_is_13_qtbxpegyubisrwf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_asonyiqxGjuBACG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_asonyiqxGjuBACG

.L_16_blocks_overflow_asonyiqxGjuBACG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_asonyiqxGjuBACG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bvjkzsnGDEmGfEx





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bvjkzsnGDEmGfEx
.L_small_initial_partial_block_bvjkzsnGDEmGfEx:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bvjkzsnGDEmGfEx:

	orq	%r8,%r8
	je	.L_after_reduction_bvjkzsnGDEmGfEx
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_bvjkzsnGDEmGfEx:
	jmp	.L_last_blocks_done_qtbxpegyubisrwf
.L_last_num_blocks_is_14_qtbxpegyubisrwf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_ClvAnmadbiqpomy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_ClvAnmadbiqpomy

.L_16_blocks_overflow_ClvAnmadbiqpomy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_ClvAnmadbiqpomy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xuBptAkBnFtdjif





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xuBptAkBnFtdjif
.L_small_initial_partial_block_xuBptAkBnFtdjif:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xuBptAkBnFtdjif:

	orq	%r8,%r8
	je	.L_after_reduction_xuBptAkBnFtdjif
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xuBptAkBnFtdjif:
	jmp	.L_last_blocks_done_qtbxpegyubisrwf
.L_last_num_blocks_is_15_qtbxpegyubisrwf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_lqeCrsoygtCkquk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_lqeCrsoygtCkquk

.L_16_blocks_overflow_lqeCrsoygtCkquk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_lqeCrsoygtCkquk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jBozwvhdyqfsevA





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jBozwvhdyqfsevA
.L_small_initial_partial_block_jBozwvhdyqfsevA:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jBozwvhdyqfsevA:

	orq	%r8,%r8
	je	.L_after_reduction_jBozwvhdyqfsevA
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jBozwvhdyqfsevA:
	jmp	.L_last_blocks_done_qtbxpegyubisrwf
.L_last_num_blocks_is_16_qtbxpegyubisrwf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_zCzDkFFwjEDvzsd
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_zCzDkFFwjEDvzsd

.L_16_blocks_overflow_zCzDkFFwjEDvzsd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_zCzDkFFwjEDvzsd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_AuBzfkBzbcvkfcy:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_AuBzfkBzbcvkfcy:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_AuBzfkBzbcvkfcy:
	jmp	.L_last_blocks_done_qtbxpegyubisrwf
.L_last_num_blocks_is_0_qtbxpegyubisrwf:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_qtbxpegyubisrwf:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_EqpenwamdmlBqwm
.L_encrypt_16_blocks_EqpenwamdmlBqwm:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_ajcFBnAEGAyiGxy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_ajcFBnAEGAyiGxy
.L_16_blocks_overflow_ajcFBnAEGAyiGxy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_ajcFBnAEGAyiGxy:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	256(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	320(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	384(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	448(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	subq	$256,%r8
	addq	$256,%rax
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_GEADBxpkDgeExqf

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_GEADBxpkDgeExqf
	jb	.L_last_num_blocks_is_7_1_GEADBxpkDgeExqf


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_GEADBxpkDgeExqf
	jb	.L_last_num_blocks_is_11_9_GEADBxpkDgeExqf


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_GEADBxpkDgeExqf
	ja	.L_last_num_blocks_is_16_GEADBxpkDgeExqf
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_GEADBxpkDgeExqf
	jmp	.L_last_num_blocks_is_13_GEADBxpkDgeExqf

.L_last_num_blocks_is_11_9_GEADBxpkDgeExqf:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_GEADBxpkDgeExqf
	ja	.L_last_num_blocks_is_11_GEADBxpkDgeExqf
	jmp	.L_last_num_blocks_is_9_GEADBxpkDgeExqf

.L_last_num_blocks_is_7_1_GEADBxpkDgeExqf:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_GEADBxpkDgeExqf
	jb	.L_last_num_blocks_is_3_1_GEADBxpkDgeExqf

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_GEADBxpkDgeExqf
	je	.L_last_num_blocks_is_6_GEADBxpkDgeExqf
	jmp	.L_last_num_blocks_is_5_GEADBxpkDgeExqf

.L_last_num_blocks_is_3_1_GEADBxpkDgeExqf:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_GEADBxpkDgeExqf
	je	.L_last_num_blocks_is_2_GEADBxpkDgeExqf
.L_last_num_blocks_is_1_GEADBxpkDgeExqf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_Evmwrrlwmtglwuj
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_Evmwrrlwmtglwuj

.L_16_blocks_overflow_Evmwrrlwmtglwuj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_Evmwrrlwmtglwuj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%xmm31,%xmm0,%xmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jeCfppBpfovGrqD





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jeCfppBpfovGrqD
.L_small_initial_partial_block_jeCfppBpfovGrqD:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)











	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_jeCfppBpfovGrqD
.L_small_initial_compute_done_jeCfppBpfovGrqD:
.L_after_reduction_jeCfppBpfovGrqD:
	jmp	.L_last_blocks_done_GEADBxpkDgeExqf
.L_last_num_blocks_is_2_GEADBxpkDgeExqf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_DywDxFydCCyrlxy
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_DywDxFydCCyrlxy

.L_16_blocks_overflow_DywDxFydCCyrlxy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_DywDxFydCCyrlxy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%ymm31,%ymm0,%ymm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_siAADixoznqEedz





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_siAADixoznqEedz
.L_small_initial_partial_block_siAADixoznqEedz:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_siAADixoznqEedz:

	orq	%r8,%r8
	je	.L_after_reduction_siAADixoznqEedz
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_siAADixoznqEedz:
	jmp	.L_last_blocks_done_GEADBxpkDgeExqf
.L_last_num_blocks_is_3_GEADBxpkDgeExqf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_puaoAdCpDiupuDt
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_puaoAdCpDiupuDt

.L_16_blocks_overflow_puaoAdCpDiupuDt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_puaoAdCpDiupuDt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_AdjyisdCbBcADpl





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_AdjyisdCbBcADpl
.L_small_initial_partial_block_AdjyisdCbBcADpl:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_AdjyisdCbBcADpl:

	orq	%r8,%r8
	je	.L_after_reduction_AdjyisdCbBcADpl
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_AdjyisdCbBcADpl:
	jmp	.L_last_blocks_done_GEADBxpkDgeExqf
.L_last_num_blocks_is_4_GEADBxpkDgeExqf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_fCuFeCmmAaqDdra
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_fCuFeCmmAaqDdra

.L_16_blocks_overflow_fCuFeCmmAaqDdra:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_fCuFeCmmAaqDdra:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jCdrlcCmukkvcjm





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jCdrlcCmukkvcjm
.L_small_initial_partial_block_jCdrlcCmukkvcjm:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jCdrlcCmukkvcjm:

	orq	%r8,%r8
	je	.L_after_reduction_jCdrlcCmukkvcjm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jCdrlcCmukkvcjm:
	jmp	.L_last_blocks_done_GEADBxpkDgeExqf
.L_last_num_blocks_is_5_GEADBxpkDgeExqf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_wiDhhAAizicmdnB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_wiDhhAAizicmdnB

.L_16_blocks_overflow_wiDhhAAizicmdnB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_wiDhhAAizicmdnB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_doEoGFmztiekoCE





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_doEoGFmztiekoCE
.L_small_initial_partial_block_doEoGFmztiekoCE:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_doEoGFmztiekoCE:

	orq	%r8,%r8
	je	.L_after_reduction_doEoGFmztiekoCE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_doEoGFmztiekoCE:
	jmp	.L_last_blocks_done_GEADBxpkDgeExqf
.L_last_num_blocks_is_6_GEADBxpkDgeExqf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_wspoAzkevGCwjgl
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_wspoAzkevGCwjgl

.L_16_blocks_overflow_wspoAzkevGCwjgl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_wspoAzkevGCwjgl:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dBagFClchBurqie





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dBagFClchBurqie
.L_small_initial_partial_block_dBagFClchBurqie:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dBagFClchBurqie:

	orq	%r8,%r8
	je	.L_after_reduction_dBagFClchBurqie
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dBagFClchBurqie:
	jmp	.L_last_blocks_done_GEADBxpkDgeExqf
.L_last_num_blocks_is_7_GEADBxpkDgeExqf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_dDwaGFwfojskroz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_dDwaGFwfojskroz

.L_16_blocks_overflow_dDwaGFwfojskroz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_dDwaGFwfojskroz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DbBmDnciErfACaB





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DbBmDnciErfACaB
.L_small_initial_partial_block_DbBmDnciErfACaB:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DbBmDnciErfACaB:

	orq	%r8,%r8
	je	.L_after_reduction_DbBmDnciErfACaB
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DbBmDnciErfACaB:
	jmp	.L_last_blocks_done_GEADBxpkDgeExqf
.L_last_num_blocks_is_8_GEADBxpkDgeExqf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_ttBdgnpgogwnimr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_ttBdgnpgogwnimr

.L_16_blocks_overflow_ttBdgnpgogwnimr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_ttBdgnpgogwnimr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dtavulypouscBEk





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dtavulypouscBEk
.L_small_initial_partial_block_dtavulypouscBEk:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dtavulypouscBEk:

	orq	%r8,%r8
	je	.L_after_reduction_dtavulypouscBEk
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dtavulypouscBEk:
	jmp	.L_last_blocks_done_GEADBxpkDgeExqf
.L_last_num_blocks_is_9_GEADBxpkDgeExqf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_sDbixusitophwuF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_sDbixusitophwuF

.L_16_blocks_overflow_sDbixusitophwuF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_sDbixusitophwuF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_oCxlAurDsEqujlC





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_oCxlAurDsEqujlC
.L_small_initial_partial_block_oCxlAurDsEqujlC:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_oCxlAurDsEqujlC:

	orq	%r8,%r8
	je	.L_after_reduction_oCxlAurDsEqujlC
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_oCxlAurDsEqujlC:
	jmp	.L_last_blocks_done_GEADBxpkDgeExqf
.L_last_num_blocks_is_10_GEADBxpkDgeExqf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_snkzzrqEcDCxvDE
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_snkzzrqEcDCxvDE

.L_16_blocks_overflow_snkzzrqEcDCxvDE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_snkzzrqEcDCxvDE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_sxfjGnDhCxmEnrl





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_sxfjGnDhCxmEnrl
.L_small_initial_partial_block_sxfjGnDhCxmEnrl:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_sxfjGnDhCxmEnrl:

	orq	%r8,%r8
	je	.L_after_reduction_sxfjGnDhCxmEnrl
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_sxfjGnDhCxmEnrl:
	jmp	.L_last_blocks_done_GEADBxpkDgeExqf
.L_last_num_blocks_is_11_GEADBxpkDgeExqf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_wlrhxFlprGympmw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_wlrhxFlprGympmw

.L_16_blocks_overflow_wlrhxFlprGympmw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_wlrhxFlprGympmw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ysxzhncdolftxdp





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ysxzhncdolftxdp
.L_small_initial_partial_block_ysxzhncdolftxdp:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ysxzhncdolftxdp:

	orq	%r8,%r8
	je	.L_after_reduction_ysxzhncdolftxdp
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ysxzhncdolftxdp:
	jmp	.L_last_blocks_done_GEADBxpkDgeExqf
.L_last_num_blocks_is_12_GEADBxpkDgeExqf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_vdrhsxxDvtjpiFf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_vdrhsxxDvtjpiFf

.L_16_blocks_overflow_vdrhsxxDvtjpiFf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_vdrhsxxDvtjpiFf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_BegFbpEDbimrAve





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_BegFbpEDbimrAve
.L_small_initial_partial_block_BegFbpEDbimrAve:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_BegFbpEDbimrAve:

	orq	%r8,%r8
	je	.L_after_reduction_BegFbpEDbimrAve
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_BegFbpEDbimrAve:
	jmp	.L_last_blocks_done_GEADBxpkDgeExqf
.L_last_num_blocks_is_13_GEADBxpkDgeExqf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_bAtxxdCEesuuBtb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_bAtxxdCEesuuBtb

.L_16_blocks_overflow_bAtxxdCEesuuBtb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_bAtxxdCEesuuBtb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_uoeBsshmljCtchj





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_uoeBsshmljCtchj
.L_small_initial_partial_block_uoeBsshmljCtchj:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_uoeBsshmljCtchj:

	orq	%r8,%r8
	je	.L_after_reduction_uoeBsshmljCtchj
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_uoeBsshmljCtchj:
	jmp	.L_last_blocks_done_GEADBxpkDgeExqf
.L_last_num_blocks_is_14_GEADBxpkDgeExqf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_CldjkAukyFiyfdd
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_CldjkAukyFiyfdd

.L_16_blocks_overflow_CldjkAukyFiyfdd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_CldjkAukyFiyfdd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_iBDbsadwqzidGtE





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_iBDbsadwqzidGtE
.L_small_initial_partial_block_iBDbsadwqzidGtE:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_iBDbsadwqzidGtE:

	orq	%r8,%r8
	je	.L_after_reduction_iBDbsadwqzidGtE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_iBDbsadwqzidGtE:
	jmp	.L_last_blocks_done_GEADBxpkDgeExqf
.L_last_num_blocks_is_15_GEADBxpkDgeExqf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_ecGrwxbbaonfdnp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_ecGrwxbbaonfdnp

.L_16_blocks_overflow_ecGrwxbbaonfdnp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_ecGrwxbbaonfdnp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_klsEythtmjDotft





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_klsEythtmjDotft
.L_small_initial_partial_block_klsEythtmjDotft:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_klsEythtmjDotft:

	orq	%r8,%r8
	je	.L_after_reduction_klsEythtmjDotft
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_klsEythtmjDotft:
	jmp	.L_last_blocks_done_GEADBxpkDgeExqf
.L_last_num_blocks_is_16_GEADBxpkDgeExqf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_gwAmgClfwnvuvaq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_gwAmgClfwnvuvaq

.L_16_blocks_overflow_gwAmgClfwnvuvaq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_gwAmgClfwnvuvaq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_twEgEigrtrsGgEB:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_twEgEigrtrsGgEB:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_twEgEigrtrsGgEB:
	jmp	.L_last_blocks_done_GEADBxpkDgeExqf
.L_last_num_blocks_is_0_GEADBxpkDgeExqf:
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_GEADBxpkDgeExqf:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_EqpenwamdmlBqwm

.L_message_below_32_blocks_EqpenwamdmlBqwm:


	subq	$256,%r8
	addq	$256,%rax
	movl	%r8d,%r10d
	leaq	96(%rsi),%r12
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_slarnhDpDtksaEz
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)
.L_skip_hkeys_precomputation_slarnhDpDtksaEz:
	movq	$1,%r14
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_FqqfzDnfmhGacEk

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_FqqfzDnfmhGacEk
	jb	.L_last_num_blocks_is_7_1_FqqfzDnfmhGacEk


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_FqqfzDnfmhGacEk
	jb	.L_last_num_blocks_is_11_9_FqqfzDnfmhGacEk


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_FqqfzDnfmhGacEk
	ja	.L_last_num_blocks_is_16_FqqfzDnfmhGacEk
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_FqqfzDnfmhGacEk
	jmp	.L_last_num_blocks_is_13_FqqfzDnfmhGacEk

.L_last_num_blocks_is_11_9_FqqfzDnfmhGacEk:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_FqqfzDnfmhGacEk
	ja	.L_last_num_blocks_is_11_FqqfzDnfmhGacEk
	jmp	.L_last_num_blocks_is_9_FqqfzDnfmhGacEk

.L_last_num_blocks_is_7_1_FqqfzDnfmhGacEk:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_FqqfzDnfmhGacEk
	jb	.L_last_num_blocks_is_3_1_FqqfzDnfmhGacEk

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_FqqfzDnfmhGacEk
	je	.L_last_num_blocks_is_6_FqqfzDnfmhGacEk
	jmp	.L_last_num_blocks_is_5_FqqfzDnfmhGacEk

.L_last_num_blocks_is_3_1_FqqfzDnfmhGacEk:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_FqqfzDnfmhGacEk
	je	.L_last_num_blocks_is_2_FqqfzDnfmhGacEk
.L_last_num_blocks_is_1_FqqfzDnfmhGacEk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_DztCgCsrhDmulGk
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_DztCgCsrhDmulGk

.L_16_blocks_overflow_DztCgCsrhDmulGk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_DztCgCsrhDmulGk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ftorcDvkqysodCc





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ftorcDvkqysodCc
.L_small_initial_partial_block_ftorcDvkqysodCc:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_ftorcDvkqysodCc
.L_small_initial_compute_done_ftorcDvkqysodCc:
.L_after_reduction_ftorcDvkqysodCc:
	jmp	.L_last_blocks_done_FqqfzDnfmhGacEk
.L_last_num_blocks_is_2_FqqfzDnfmhGacEk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_AcGprvEGrqAmlti
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_AcGprvEGrqAmlti

.L_16_blocks_overflow_AcGprvEGrqAmlti:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_AcGprvEGrqAmlti:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GzyovmArbalcdFr





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GzyovmArbalcdFr
.L_small_initial_partial_block_GzyovmArbalcdFr:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GzyovmArbalcdFr:

	orq	%r8,%r8
	je	.L_after_reduction_GzyovmArbalcdFr
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_GzyovmArbalcdFr:
	jmp	.L_last_blocks_done_FqqfzDnfmhGacEk
.L_last_num_blocks_is_3_FqqfzDnfmhGacEk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_FmEDafkwyfncdaw
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_FmEDafkwyfncdaw

.L_16_blocks_overflow_FmEDafkwyfncdaw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_FmEDafkwyfncdaw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DorofhciFrBdoFs





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DorofhciFrBdoFs
.L_small_initial_partial_block_DorofhciFrBdoFs:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DorofhciFrBdoFs:

	orq	%r8,%r8
	je	.L_after_reduction_DorofhciFrBdoFs
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DorofhciFrBdoFs:
	jmp	.L_last_blocks_done_FqqfzDnfmhGacEk
.L_last_num_blocks_is_4_FqqfzDnfmhGacEk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_jamlktowcwgcxuh
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_jamlktowcwgcxuh

.L_16_blocks_overflow_jamlktowcwgcxuh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_jamlktowcwgcxuh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xDiqhFmBsdecjeu





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xDiqhFmBsdecjeu
.L_small_initial_partial_block_xDiqhFmBsdecjeu:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xDiqhFmBsdecjeu:

	orq	%r8,%r8
	je	.L_after_reduction_xDiqhFmBsdecjeu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xDiqhFmBsdecjeu:
	jmp	.L_last_blocks_done_FqqfzDnfmhGacEk
.L_last_num_blocks_is_5_FqqfzDnfmhGacEk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_ubbbGDEcdFmBoly
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_ubbbGDEcdFmBoly

.L_16_blocks_overflow_ubbbGDEcdFmBoly:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_ubbbGDEcdFmBoly:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_wdGEysACBhpGgGr





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_wdGEysACBhpGgGr
.L_small_initial_partial_block_wdGEysACBhpGgGr:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_wdGEysACBhpGgGr:

	orq	%r8,%r8
	je	.L_after_reduction_wdGEysACBhpGgGr
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_wdGEysACBhpGgGr:
	jmp	.L_last_blocks_done_FqqfzDnfmhGacEk
.L_last_num_blocks_is_6_FqqfzDnfmhGacEk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_BoBfccztvjoorhi
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_BoBfccztvjoorhi

.L_16_blocks_overflow_BoBfccztvjoorhi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_BoBfccztvjoorhi:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_kwGftmmgzhoExqn





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_kwGftmmgzhoExqn
.L_small_initial_partial_block_kwGftmmgzhoExqn:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_kwGftmmgzhoExqn:

	orq	%r8,%r8
	je	.L_after_reduction_kwGftmmgzhoExqn
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_kwGftmmgzhoExqn:
	jmp	.L_last_blocks_done_FqqfzDnfmhGacEk
.L_last_num_blocks_is_7_FqqfzDnfmhGacEk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_jsAhbwebieEDEgy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_jsAhbwebieEDEgy

.L_16_blocks_overflow_jsAhbwebieEDEgy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_jsAhbwebieEDEgy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_yCjkrpBlqexlilB





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_yCjkrpBlqexlilB
.L_small_initial_partial_block_yCjkrpBlqexlilB:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yCjkrpBlqexlilB:

	orq	%r8,%r8
	je	.L_after_reduction_yCjkrpBlqexlilB
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_yCjkrpBlqexlilB:
	jmp	.L_last_blocks_done_FqqfzDnfmhGacEk
.L_last_num_blocks_is_8_FqqfzDnfmhGacEk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_mhfFheEtvtEuvjF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_mhfFheEtvtEuvjF

.L_16_blocks_overflow_mhfFheEtvtEuvjF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_mhfFheEtvtEuvjF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CazjvlFnFtcDzGE





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CazjvlFnFtcDzGE
.L_small_initial_partial_block_CazjvlFnFtcDzGE:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CazjvlFnFtcDzGE:

	orq	%r8,%r8
	je	.L_after_reduction_CazjvlFnFtcDzGE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_CazjvlFnFtcDzGE:
	jmp	.L_last_blocks_done_FqqfzDnfmhGacEk
.L_last_num_blocks_is_9_FqqfzDnfmhGacEk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_uBhGgvhFzCEmyeB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_uBhGgvhFzCEmyeB

.L_16_blocks_overflow_uBhGgvhFzCEmyeB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_uBhGgvhFzCEmyeB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_pCAFjpzwCtFmkEw





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_pCAFjpzwCtFmkEw
.L_small_initial_partial_block_pCAFjpzwCtFmkEw:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_pCAFjpzwCtFmkEw:

	orq	%r8,%r8
	je	.L_after_reduction_pCAFjpzwCtFmkEw
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_pCAFjpzwCtFmkEw:
	jmp	.L_last_blocks_done_FqqfzDnfmhGacEk
.L_last_num_blocks_is_10_FqqfzDnfmhGacEk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_BEzhlqGxuevwfit
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_BEzhlqGxuevwfit

.L_16_blocks_overflow_BEzhlqGxuevwfit:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_BEzhlqGxuevwfit:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qBcpwpyDuDrpwvm





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qBcpwpyDuDrpwvm
.L_small_initial_partial_block_qBcpwpyDuDrpwvm:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qBcpwpyDuDrpwvm:

	orq	%r8,%r8
	je	.L_after_reduction_qBcpwpyDuDrpwvm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_qBcpwpyDuDrpwvm:
	jmp	.L_last_blocks_done_FqqfzDnfmhGacEk
.L_last_num_blocks_is_11_FqqfzDnfmhGacEk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_ACfsCvunjgclgfn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_ACfsCvunjgclgfn

.L_16_blocks_overflow_ACfsCvunjgclgfn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_ACfsCvunjgclgfn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ABGGiFDmjDEDvhk





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ABGGiFDmjDEDvhk
.L_small_initial_partial_block_ABGGiFDmjDEDvhk:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ABGGiFDmjDEDvhk:

	orq	%r8,%r8
	je	.L_after_reduction_ABGGiFDmjDEDvhk
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ABGGiFDmjDEDvhk:
	jmp	.L_last_blocks_done_FqqfzDnfmhGacEk
.L_last_num_blocks_is_12_FqqfzDnfmhGacEk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_yuEoGGpDtlqCrnb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_yuEoGGpDtlqCrnb

.L_16_blocks_overflow_yuEoGGpDtlqCrnb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_yuEoGGpDtlqCrnb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cCfxkEzBrxCxuCh





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cCfxkEzBrxCxuCh
.L_small_initial_partial_block_cCfxkEzBrxCxuCh:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cCfxkEzBrxCxuCh:

	orq	%r8,%r8
	je	.L_after_reduction_cCfxkEzBrxCxuCh
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cCfxkEzBrxCxuCh:
	jmp	.L_last_blocks_done_FqqfzDnfmhGacEk
.L_last_num_blocks_is_13_FqqfzDnfmhGacEk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_kctufFlqvkrhrdp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_kctufFlqvkrhrdp

.L_16_blocks_overflow_kctufFlqvkrhrdp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_kctufFlqvkrhrdp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ogljsGAkaskdxen





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ogljsGAkaskdxen
.L_small_initial_partial_block_ogljsGAkaskdxen:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ogljsGAkaskdxen:

	orq	%r8,%r8
	je	.L_after_reduction_ogljsGAkaskdxen
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ogljsGAkaskdxen:
	jmp	.L_last_blocks_done_FqqfzDnfmhGacEk
.L_last_num_blocks_is_14_FqqfzDnfmhGacEk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_fxhhoFjdFhGCFFF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_fxhhoFjdFhGCFFF

.L_16_blocks_overflow_fxhhoFjdFhGCFFF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_fxhhoFjdFhGCFFF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_udBxbFgrkjigyBu





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_udBxbFgrkjigyBu
.L_small_initial_partial_block_udBxbFgrkjigyBu:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_udBxbFgrkjigyBu:

	orq	%r8,%r8
	je	.L_after_reduction_udBxbFgrkjigyBu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_udBxbFgrkjigyBu:
	jmp	.L_last_blocks_done_FqqfzDnfmhGacEk
.L_last_num_blocks_is_15_FqqfzDnfmhGacEk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_utxgCrasakgepiv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_utxgCrasakgepiv

.L_16_blocks_overflow_utxgCrasakgepiv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_utxgCrasakgepiv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_lzjlfsensBEjqzr





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_lzjlfsensBEjqzr
.L_small_initial_partial_block_lzjlfsensBEjqzr:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lzjlfsensBEjqzr:

	orq	%r8,%r8
	je	.L_after_reduction_lzjlfsensBEjqzr
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_lzjlfsensBEjqzr:
	jmp	.L_last_blocks_done_FqqfzDnfmhGacEk
.L_last_num_blocks_is_16_FqqfzDnfmhGacEk:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_DtmldDlGjGBikcv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_DtmldDlGjGBikcv

.L_16_blocks_overflow_DtmldDlGjGBikcv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_DtmldDlGjGBikcv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_DqwnGdFCmFqrcvF:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DqwnGdFCmFqrcvF:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DqwnGdFCmFqrcvF:
	jmp	.L_last_blocks_done_FqqfzDnfmhGacEk
.L_last_num_blocks_is_0_FqqfzDnfmhGacEk:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_FqqfzDnfmhGacEk:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_EqpenwamdmlBqwm

.L_message_below_equal_16_blocks_EqpenwamdmlBqwm:


	movl	%r8d,%r12d
	addl	$15,%r12d
	shrl	$4,%r12d
	cmpq	$8,%r12
	je	.L_small_initial_num_blocks_is_8_pfBkBCquiwmlEbb
	jl	.L_small_initial_num_blocks_is_7_1_pfBkBCquiwmlEbb


	cmpq	$12,%r12
	je	.L_small_initial_num_blocks_is_12_pfBkBCquiwmlEbb
	jl	.L_small_initial_num_blocks_is_11_9_pfBkBCquiwmlEbb


	cmpq	$16,%r12
	je	.L_small_initial_num_blocks_is_16_pfBkBCquiwmlEbb
	cmpq	$15,%r12
	je	.L_small_initial_num_blocks_is_15_pfBkBCquiwmlEbb
	cmpq	$14,%r12
	je	.L_small_initial_num_blocks_is_14_pfBkBCquiwmlEbb
	jmp	.L_small_initial_num_blocks_is_13_pfBkBCquiwmlEbb

.L_small_initial_num_blocks_is_11_9_pfBkBCquiwmlEbb:

	cmpq	$11,%r12
	je	.L_small_initial_num_blocks_is_11_pfBkBCquiwmlEbb
	cmpq	$10,%r12
	je	.L_small_initial_num_blocks_is_10_pfBkBCquiwmlEbb
	jmp	.L_small_initial_num_blocks_is_9_pfBkBCquiwmlEbb

.L_small_initial_num_blocks_is_7_1_pfBkBCquiwmlEbb:
	cmpq	$4,%r12
	je	.L_small_initial_num_blocks_is_4_pfBkBCquiwmlEbb
	jl	.L_small_initial_num_blocks_is_3_1_pfBkBCquiwmlEbb

	cmpq	$7,%r12
	je	.L_small_initial_num_blocks_is_7_pfBkBCquiwmlEbb
	cmpq	$6,%r12
	je	.L_small_initial_num_blocks_is_6_pfBkBCquiwmlEbb
	jmp	.L_small_initial_num_blocks_is_5_pfBkBCquiwmlEbb

.L_small_initial_num_blocks_is_3_1_pfBkBCquiwmlEbb:

	cmpq	$3,%r12
	je	.L_small_initial_num_blocks_is_3_pfBkBCquiwmlEbb
	cmpq	$2,%r12
	je	.L_small_initial_num_blocks_is_2_pfBkBCquiwmlEbb





.L_small_initial_num_blocks_is_1_pfBkBCquiwmlEbb:
	vmovdqa64	SHUF_MASK(%rip),%xmm29
	vpaddd	ONE(%rip),%xmm2,%xmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm0,%xmm2
	vpshufb	%xmm29,%xmm0,%xmm0
	vmovdqu8	0(%rcx,%rax,1),%xmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%xmm15,%xmm0,%xmm0
	vpxorq	%xmm6,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm6
	vextracti32x4	$0,%zmm6,%xmm13
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nCdBhinEpDlAihm





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nCdBhinEpDlAihm
.L_small_initial_partial_block_nCdBhinEpDlAihm:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)











	vpxorq	%xmm13,%xmm14,%xmm14

	jmp	.L_after_reduction_nCdBhinEpDlAihm
.L_small_initial_compute_done_nCdBhinEpDlAihm:
.L_after_reduction_nCdBhinEpDlAihm:
	jmp	.L_small_initial_blocks_encrypted_pfBkBCquiwmlEbb
.L_small_initial_num_blocks_is_2_pfBkBCquiwmlEbb:
	vmovdqa64	SHUF_MASK(%rip),%ymm29
	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm0,%xmm2
	vpshufb	%ymm29,%ymm0,%ymm0
	vmovdqu8	0(%rcx,%rax,1),%ymm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%ymm15,%ymm0,%ymm0
	vpxorq	%ymm6,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm6
	vextracti32x4	$1,%zmm6,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ravwraCgatlirzm





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ravwraCgatlirzm
.L_small_initial_partial_block_ravwraCgatlirzm:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ravwraCgatlirzm:

	orq	%r8,%r8
	je	.L_after_reduction_ravwraCgatlirzm
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_ravwraCgatlirzm:
	jmp	.L_small_initial_blocks_encrypted_pfBkBCquiwmlEbb
.L_small_initial_num_blocks_is_3_pfBkBCquiwmlEbb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vextracti32x4	$2,%zmm6,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_snyrvmsjCfeDpfq





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_snyrvmsjCfeDpfq
.L_small_initial_partial_block_snyrvmsjCfeDpfq:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_snyrvmsjCfeDpfq:

	orq	%r8,%r8
	je	.L_after_reduction_snyrvmsjCfeDpfq
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_snyrvmsjCfeDpfq:
	jmp	.L_small_initial_blocks_encrypted_pfBkBCquiwmlEbb
.L_small_initial_num_blocks_is_4_pfBkBCquiwmlEbb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vextracti32x4	$3,%zmm6,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GeptAqvctbEkxoG





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GeptAqvctbEkxoG
.L_small_initial_partial_block_GeptAqvctbEkxoG:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GeptAqvctbEkxoG:

	orq	%r8,%r8
	je	.L_after_reduction_GeptAqvctbEkxoG
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_GeptAqvctbEkxoG:
	jmp	.L_small_initial_blocks_encrypted_pfBkBCquiwmlEbb
.L_small_initial_num_blocks_is_5_pfBkBCquiwmlEbb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%xmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%xmm15,%xmm3,%xmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%xmm7,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%xmm29,%xmm3,%xmm7
	vextracti32x4	$0,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GqsCcoaDyrernqe





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GqsCcoaDyrernqe
.L_small_initial_partial_block_GqsCcoaDyrernqe:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GqsCcoaDyrernqe:

	orq	%r8,%r8
	je	.L_after_reduction_GqsCcoaDyrernqe
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_GqsCcoaDyrernqe:
	jmp	.L_small_initial_blocks_encrypted_pfBkBCquiwmlEbb
.L_small_initial_num_blocks_is_6_pfBkBCquiwmlEbb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%ymm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%ymm15,%ymm3,%ymm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%ymm7,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%ymm29,%ymm3,%ymm7
	vextracti32x4	$1,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xpapqngBkgFFprm





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xpapqngBkgFFprm
.L_small_initial_partial_block_xpapqngBkgFFprm:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xpapqngBkgFFprm:

	orq	%r8,%r8
	je	.L_after_reduction_xpapqngBkgFFprm
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_xpapqngBkgFFprm:
	jmp	.L_small_initial_blocks_encrypted_pfBkBCquiwmlEbb
.L_small_initial_num_blocks_is_7_pfBkBCquiwmlEbb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vextracti32x4	$2,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_fBhxqzGgEkisjcz





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_fBhxqzGgEkisjcz
.L_small_initial_partial_block_fBhxqzGgEkisjcz:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_fBhxqzGgEkisjcz:

	orq	%r8,%r8
	je	.L_after_reduction_fBhxqzGgEkisjcz
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_fBhxqzGgEkisjcz:
	jmp	.L_small_initial_blocks_encrypted_pfBkBCquiwmlEbb
.L_small_initial_num_blocks_is_8_pfBkBCquiwmlEbb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vextracti32x4	$3,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EAkAlDDjpwegAto





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EAkAlDDjpwegAto
.L_small_initial_partial_block_EAkAlDDjpwegAto:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_EAkAlDDjpwegAto:

	orq	%r8,%r8
	je	.L_after_reduction_EAkAlDDjpwegAto
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_EAkAlDDjpwegAto:
	jmp	.L_small_initial_blocks_encrypted_pfBkBCquiwmlEbb
.L_small_initial_num_blocks_is_9_pfBkBCquiwmlEbb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%xmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%xmm15,%xmm4,%xmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%xmm10,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%xmm29,%xmm4,%xmm10
	vextracti32x4	$0,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_sovDbpsgpBgtass





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_sovDbpsgpBgtass
.L_small_initial_partial_block_sovDbpsgpBgtass:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_sovDbpsgpBgtass:

	orq	%r8,%r8
	je	.L_after_reduction_sovDbpsgpBgtass
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_sovDbpsgpBgtass:
	jmp	.L_small_initial_blocks_encrypted_pfBkBCquiwmlEbb
.L_small_initial_num_blocks_is_10_pfBkBCquiwmlEbb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%ymm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%ymm15,%ymm4,%ymm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%ymm10,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%ymm29,%ymm4,%ymm10
	vextracti32x4	$1,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_tuBpFvsAknGwygw





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_tuBpFvsAknGwygw
.L_small_initial_partial_block_tuBpFvsAknGwygw:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_tuBpFvsAknGwygw:

	orq	%r8,%r8
	je	.L_after_reduction_tuBpFvsAknGwygw
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_tuBpFvsAknGwygw:
	jmp	.L_small_initial_blocks_encrypted_pfBkBCquiwmlEbb
.L_small_initial_num_blocks_is_11_pfBkBCquiwmlEbb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vextracti32x4	$2,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_uhurdpEuiEwlmdE





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_uhurdpEuiEwlmdE
.L_small_initial_partial_block_uhurdpEuiEwlmdE:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_uhurdpEuiEwlmdE:

	orq	%r8,%r8
	je	.L_after_reduction_uhurdpEuiEwlmdE
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_uhurdpEuiEwlmdE:
	jmp	.L_small_initial_blocks_encrypted_pfBkBCquiwmlEbb
.L_small_initial_num_blocks_is_12_pfBkBCquiwmlEbb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vextracti32x4	$3,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_wbpGCvztpntDGrr





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	64(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_wbpGCvztpntDGrr
.L_small_initial_partial_block_wbpGCvztpntDGrr:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_wbpGCvztpntDGrr:

	orq	%r8,%r8
	je	.L_after_reduction_wbpGCvztpntDGrr
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_wbpGCvztpntDGrr:
	jmp	.L_small_initial_blocks_encrypted_pfBkBCquiwmlEbb
.L_small_initial_num_blocks_is_13_pfBkBCquiwmlEbb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%xmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%xmm15,%xmm5,%xmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%xmm11,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%xmm29,%xmm5,%xmm11
	vextracti32x4	$0,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_AgkBlDtmnxeAlmr





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	48(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_AgkBlDtmnxeAlmr
.L_small_initial_partial_block_AgkBlDtmnxeAlmr:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	64(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_AgkBlDtmnxeAlmr:

	orq	%r8,%r8
	je	.L_after_reduction_AgkBlDtmnxeAlmr
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_AgkBlDtmnxeAlmr:
	jmp	.L_small_initial_blocks_encrypted_pfBkBCquiwmlEbb
.L_small_initial_num_blocks_is_14_pfBkBCquiwmlEbb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%ymm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%ymm15,%ymm5,%ymm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%ymm11,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%ymm29,%ymm5,%ymm11
	vextracti32x4	$1,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ytFyGdFbsoBmcFw





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	32(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ytFyGdFbsoBmcFw
.L_small_initial_partial_block_ytFyGdFbsoBmcFw:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	48(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ytFyGdFbsoBmcFw:

	orq	%r8,%r8
	je	.L_after_reduction_ytFyGdFbsoBmcFw
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_ytFyGdFbsoBmcFw:
	jmp	.L_small_initial_blocks_encrypted_pfBkBCquiwmlEbb
.L_small_initial_num_blocks_is_15_pfBkBCquiwmlEbb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%zmm29,%zmm5,%zmm11
	vextracti32x4	$2,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gwohGioymuedfnG





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	16(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gwohGioymuedfnG
.L_small_initial_partial_block_gwohGioymuedfnG:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	32(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gwohGioymuedfnG:

	orq	%r8,%r8
	je	.L_after_reduction_gwohGioymuedfnG
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_gwohGioymuedfnG:
	jmp	.L_small_initial_blocks_encrypted_pfBkBCquiwmlEbb
.L_small_initial_num_blocks_is_16_pfBkBCquiwmlEbb:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%zmm29,%zmm5,%zmm11
	vextracti32x4	$3,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_ErqtlhBhmAphhap:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	16(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ErqtlhBhmAphhap:
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_ErqtlhBhmAphhap:
.L_small_initial_blocks_encrypted_pfBkBCquiwmlEbb:
.L_ghash_done_EqpenwamdmlBqwm:
	vmovdqu64	%xmm2,0(%rsi)
.L_enc_dec_done_EqpenwamdmlBqwm:

	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14
	vmovdqu64	%xmm14,64(%rsi)
.L_enc_dec_abort_EqpenwamdmlBqwm:
	jmp	.Lexit_gcm_encrypt
.align	32
.Laes_gcm_encrypt_256_avx512:
	orq	%r8,%r8
	je	.L_enc_dec_abort_uizbwpfDujhiqBd
	xorq	%r14,%r14
	vmovdqu64	64(%rsi),%xmm14
	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14

	movl	(%rdx),%eax
	orq	%rax,%rax
	je	.L_partial_block_done_aamzzDuwqFlhvdv
	movl	$16,%r10d
	leaq	byte_len_to_mask_table(%rip),%r12
	cmpq	%r10,%r8
	cmovcq	%r8,%r10
	kmovw	(%r12,%r10,2),%k1
	vmovdqu8	(%rcx),%xmm0{%k1}{z}

	vmovdqu64	16(%rsi),%xmm3

	leaq	96(%rsi),%r10
	vmovdqu64	240(%r10),%xmm4



	leaq	SHIFT_MASK(%rip),%r12
	addq	%rax,%r12
	vmovdqu64	(%r12),%xmm5
	vpshufb	%xmm5,%xmm3,%xmm3
	vpxorq	%xmm0,%xmm3,%xmm3


	leaq	(%r8,%rax,1),%r13
	subq	$16,%r13
	jge	.L_no_extra_mask_aamzzDuwqFlhvdv
	subq	%r13,%r12
.L_no_extra_mask_aamzzDuwqFlhvdv:



	vmovdqu64	16(%r12),%xmm0
	vpand	%xmm0,%xmm3,%xmm3
	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
	vpshufb	%xmm5,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm14,%xmm14
	cmpq	$0,%r13
	jl	.L_partial_incomplete_aamzzDuwqFlhvdv

	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm14,%xmm14

	vpsrldq	$8,%xmm14,%xmm11
	vpslldq	$8,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm7,%xmm7
	vpxorq	%xmm10,%xmm14,%xmm14



	vmovdqu64	POLY2(%rip),%xmm11

	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
	vpslldq	$8,%xmm10,%xmm10
	vpxorq	%xmm10,%xmm14,%xmm14



	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
	vpsrldq	$4,%xmm10,%xmm10
	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
	vpslldq	$4,%xmm14,%xmm14

	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14

	movl	$0,(%rdx)

	movq	%rax,%r12
	movq	$16,%rax
	subq	%r12,%rax
	jmp	.L_enc_dec_done_aamzzDuwqFlhvdv

.L_partial_incomplete_aamzzDuwqFlhvdv:
	addl	%r8d,(%rdx)
	movq	%r8,%rax

.L_enc_dec_done_aamzzDuwqFlhvdv:


	leaq	byte_len_to_mask_table(%rip),%r12
	kmovw	(%r12,%rax,2),%k1

	vpshufb	SHUF_MASK(%rip),%xmm3,%xmm3
	vpshufb	%xmm5,%xmm3,%xmm3
	movq	%r9,%r12
	vmovdqu8	%xmm3,(%r12){%k1}
.L_partial_block_done_aamzzDuwqFlhvdv:
	vmovdqu64	0(%rsi),%xmm2
	subq	%rax,%r8
	je	.L_enc_dec_done_uizbwpfDujhiqBd
	cmpq	$256,%r8
	jbe	.L_message_below_equal_16_blocks_uizbwpfDujhiqBd

	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
	vmovdqa64	ddq_addbe_1234(%rip),%zmm28






	vmovd	%xmm2,%r15d
	andl	$255,%r15d

	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpshufb	%zmm29,%zmm2,%zmm2



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_eiGhEsBGqfsFrfb
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_eiGhEsBGqfsFrfb
.L_next_16_overflow_eiGhEsBGqfsFrfb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_eiGhEsBGqfsFrfb:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	0(%rcx,%rax,1),%zmm0
	vmovdqu8	64(%rcx,%rax,1),%zmm3
	vmovdqu8	128(%rcx,%rax,1),%zmm4
	vmovdqu8	192(%rcx,%rax,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	208(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	224(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,0(%r10,%rax,1)
	vmovdqu8	%zmm10,64(%r10,%rax,1)
	vmovdqu8	%zmm11,128(%r10,%rax,1)
	vmovdqu8	%zmm12,192(%r10,%rax,1)

	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
	vmovdqa64	%zmm7,768(%rsp)
	vmovdqa64	%zmm10,832(%rsp)
	vmovdqa64	%zmm11,896(%rsp)
	vmovdqa64	%zmm12,960(%rsp)
	leaq	96(%rsi),%r12
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_dEErhuhyFssupbv

	vmovdqu64	192(%r12),%zmm0
	vmovdqu64	%zmm0,704(%rsp)

	vmovdqu64	128(%r12),%zmm3
	vmovdqu64	%zmm3,640(%rsp)


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	64(%r12),%zmm4
	vmovdqu64	%zmm4,576(%rsp)

	vmovdqu64	0(%r12),%zmm5
	vmovdqu64	%zmm5,512(%rsp)
.L_skip_hkeys_precomputation_dEErhuhyFssupbv:
	cmpq	$512,%r8
	jb	.L_message_below_32_blocks_uizbwpfDujhiqBd



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_bpuxEliDmGmmbkh
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_bpuxEliDmGmmbkh
.L_next_16_overflow_bpuxEliDmGmmbkh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_bpuxEliDmGmmbkh:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	256(%rcx,%rax,1),%zmm0
	vmovdqu8	320(%rcx,%rax,1),%zmm3
	vmovdqu8	384(%rcx,%rax,1),%zmm4
	vmovdqu8	448(%rcx,%rax,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	208(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	224(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,256(%r10,%rax,1)
	vmovdqu8	%zmm10,320(%r10,%rax,1)
	vmovdqu8	%zmm11,384(%r10,%rax,1)
	vmovdqu8	%zmm12,448(%r10,%rax,1)

	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
	vmovdqa64	%zmm7,1024(%rsp)
	vmovdqa64	%zmm10,1088(%rsp)
	vmovdqa64	%zmm11,1152(%rsp)
	vmovdqa64	%zmm12,1216(%rsp)
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_GwxvCCkwkBwryxE
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,192(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,128(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,64(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,0(%rsp)
.L_skip_hkeys_precomputation_GwxvCCkwkBwryxE:
	movq	$1,%r14
	addq	$512,%rax
	subq	$512,%r8

	cmpq	$768,%r8
	jb	.L_no_more_big_nblocks_uizbwpfDujhiqBd
.L_encrypt_big_nblocks_uizbwpfDujhiqBd:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_mrAeplnzcAEvpyl
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_mrAeplnzcAEvpyl
.L_16_blocks_overflow_mrAeplnzcAEvpyl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_mrAeplnzcAEvpyl:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_jqumetabuhlaBll
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_jqumetabuhlaBll
.L_16_blocks_overflow_jqumetabuhlaBll:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_jqumetabuhlaBll:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%rax,1),%zmm17
	vmovdqu8	320(%rcx,%rax,1),%zmm19
	vmovdqu8	384(%rcx,%rax,1),%zmm20
	vmovdqu8	448(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%rax,1)
	vmovdqu8	%zmm3,320(%r10,%rax,1)
	vmovdqu8	%zmm4,384(%r10,%rax,1)
	vmovdqu8	%zmm5,448(%r10,%rax,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_beirqdwfoihjvlz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_beirqdwfoihjvlz
.L_16_blocks_overflow_beirqdwfoihjvlz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_beirqdwfoihjvlz:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	512(%rcx,%rax,1),%zmm17
	vmovdqu8	576(%rcx,%rax,1),%zmm19
	vmovdqu8	640(%rcx,%rax,1),%zmm20
	vmovdqu8	704(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30


	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10

	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
	vpxorq	%zmm24,%zmm6,%zmm6
	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
	vpxorq	%zmm25,%zmm7,%zmm7
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vextracti64x4	$1,%zmm6,%ymm12
	vpxorq	%ymm12,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm12
	vpxorq	%xmm12,%xmm6,%xmm6
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,512(%r10,%rax,1)
	vmovdqu8	%zmm3,576(%r10,%rax,1)
	vmovdqu8	%zmm4,640(%r10,%rax,1)
	vmovdqu8	%zmm5,704(%r10,%rax,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1024(%rsp)
	vmovdqa64	%zmm3,1088(%rsp)
	vmovdqa64	%zmm4,1152(%rsp)
	vmovdqa64	%zmm5,1216(%rsp)
	vmovdqa64	%zmm6,%zmm14

	addq	$768,%rax
	subq	$768,%r8
	cmpq	$768,%r8
	jae	.L_encrypt_big_nblocks_uizbwpfDujhiqBd

.L_no_more_big_nblocks_uizbwpfDujhiqBd:

	cmpq	$512,%r8
	jae	.L_encrypt_32_blocks_uizbwpfDujhiqBd

	cmpq	$256,%r8
	jae	.L_encrypt_16_blocks_uizbwpfDujhiqBd
.L_encrypt_0_blocks_ghash_32_uizbwpfDujhiqBd:
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$256,%ebx
	subl	%r10d,%ebx
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	addl	$256,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_kxapkDtebDzrjDo

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_kxapkDtebDzrjDo
	jb	.L_last_num_blocks_is_7_1_kxapkDtebDzrjDo


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_kxapkDtebDzrjDo
	jb	.L_last_num_blocks_is_11_9_kxapkDtebDzrjDo


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_kxapkDtebDzrjDo
	ja	.L_last_num_blocks_is_16_kxapkDtebDzrjDo
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_kxapkDtebDzrjDo
	jmp	.L_last_num_blocks_is_13_kxapkDtebDzrjDo

.L_last_num_blocks_is_11_9_kxapkDtebDzrjDo:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_kxapkDtebDzrjDo
	ja	.L_last_num_blocks_is_11_kxapkDtebDzrjDo
	jmp	.L_last_num_blocks_is_9_kxapkDtebDzrjDo

.L_last_num_blocks_is_7_1_kxapkDtebDzrjDo:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_kxapkDtebDzrjDo
	jb	.L_last_num_blocks_is_3_1_kxapkDtebDzrjDo

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_kxapkDtebDzrjDo
	je	.L_last_num_blocks_is_6_kxapkDtebDzrjDo
	jmp	.L_last_num_blocks_is_5_kxapkDtebDzrjDo

.L_last_num_blocks_is_3_1_kxapkDtebDzrjDo:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_kxapkDtebDzrjDo
	je	.L_last_num_blocks_is_2_kxapkDtebDzrjDo
.L_last_num_blocks_is_1_kxapkDtebDzrjDo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_zaAnoGssmFjkhzp
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_zaAnoGssmFjkhzp

.L_16_blocks_overflow_zaAnoGssmFjkhzp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_zaAnoGssmFjkhzp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_tvyFCmhGqpvbcCF





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_tvyFCmhGqpvbcCF
.L_small_initial_partial_block_tvyFCmhGqpvbcCF:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_tvyFCmhGqpvbcCF
.L_small_initial_compute_done_tvyFCmhGqpvbcCF:
.L_after_reduction_tvyFCmhGqpvbcCF:
	jmp	.L_last_blocks_done_kxapkDtebDzrjDo
.L_last_num_blocks_is_2_kxapkDtebDzrjDo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_lcjFzfjhkoeBqEj
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_lcjFzfjhkoeBqEj

.L_16_blocks_overflow_lcjFzfjhkoeBqEj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_lcjFzfjhkoeBqEj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_FDtnlnhEfqcjlwG





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_FDtnlnhEfqcjlwG
.L_small_initial_partial_block_FDtnlnhEfqcjlwG:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_FDtnlnhEfqcjlwG:

	orq	%r8,%r8
	je	.L_after_reduction_FDtnlnhEfqcjlwG
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_FDtnlnhEfqcjlwG:
	jmp	.L_last_blocks_done_kxapkDtebDzrjDo
.L_last_num_blocks_is_3_kxapkDtebDzrjDo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_FcaAfxdBubAhnyl
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_FcaAfxdBubAhnyl

.L_16_blocks_overflow_FcaAfxdBubAhnyl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_FcaAfxdBubAhnyl:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_lcxdsAfvreoapkn





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_lcxdsAfvreoapkn
.L_small_initial_partial_block_lcxdsAfvreoapkn:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lcxdsAfvreoapkn:

	orq	%r8,%r8
	je	.L_after_reduction_lcxdsAfvreoapkn
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_lcxdsAfvreoapkn:
	jmp	.L_last_blocks_done_kxapkDtebDzrjDo
.L_last_num_blocks_is_4_kxapkDtebDzrjDo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_GnzzirdckpyckCC
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_GnzzirdckpyckCC

.L_16_blocks_overflow_GnzzirdckpyckCC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_GnzzirdckpyckCC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_AwtdwgnjsECmpoB





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_AwtdwgnjsECmpoB
.L_small_initial_partial_block_AwtdwgnjsECmpoB:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_AwtdwgnjsECmpoB:

	orq	%r8,%r8
	je	.L_after_reduction_AwtdwgnjsECmpoB
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_AwtdwgnjsECmpoB:
	jmp	.L_last_blocks_done_kxapkDtebDzrjDo
.L_last_num_blocks_is_5_kxapkDtebDzrjDo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_ukrlBDlAmBahqye
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_ukrlBDlAmBahqye

.L_16_blocks_overflow_ukrlBDlAmBahqye:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_ukrlBDlAmBahqye:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zievzvFmrApgFAt





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zievzvFmrApgFAt
.L_small_initial_partial_block_zievzvFmrApgFAt:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zievzvFmrApgFAt:

	orq	%r8,%r8
	je	.L_after_reduction_zievzvFmrApgFAt
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_zievzvFmrApgFAt:
	jmp	.L_last_blocks_done_kxapkDtebDzrjDo
.L_last_num_blocks_is_6_kxapkDtebDzrjDo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_dFFpllcpkEroqlj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_dFFpllcpkEroqlj

.L_16_blocks_overflow_dFFpllcpkEroqlj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_dFFpllcpkEroqlj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dcvbgtmzimvunyx





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dcvbgtmzimvunyx
.L_small_initial_partial_block_dcvbgtmzimvunyx:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dcvbgtmzimvunyx:

	orq	%r8,%r8
	je	.L_after_reduction_dcvbgtmzimvunyx
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dcvbgtmzimvunyx:
	jmp	.L_last_blocks_done_kxapkDtebDzrjDo
.L_last_num_blocks_is_7_kxapkDtebDzrjDo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_FvxDDcClxswrlxr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_FvxDDcClxswrlxr

.L_16_blocks_overflow_FvxDDcClxswrlxr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_FvxDDcClxswrlxr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ElifvmAwfpqDExf





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ElifvmAwfpqDExf
.L_small_initial_partial_block_ElifvmAwfpqDExf:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ElifvmAwfpqDExf:

	orq	%r8,%r8
	je	.L_after_reduction_ElifvmAwfpqDExf
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ElifvmAwfpqDExf:
	jmp	.L_last_blocks_done_kxapkDtebDzrjDo
.L_last_num_blocks_is_8_kxapkDtebDzrjDo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_vdBBEfCooghCbpx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_vdBBEfCooghCbpx

.L_16_blocks_overflow_vdBBEfCooghCbpx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_vdBBEfCooghCbpx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_kwkDxbferafdvie





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_kwkDxbferafdvie
.L_small_initial_partial_block_kwkDxbferafdvie:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_kwkDxbferafdvie:

	orq	%r8,%r8
	je	.L_after_reduction_kwkDxbferafdvie
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_kwkDxbferafdvie:
	jmp	.L_last_blocks_done_kxapkDtebDzrjDo
.L_last_num_blocks_is_9_kxapkDtebDzrjDo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_aGzrjeBacqExesq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_aGzrjeBacqExesq

.L_16_blocks_overflow_aGzrjeBacqExesq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_aGzrjeBacqExesq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_fsgzrshyFGzlgeC





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_fsgzrshyFGzlgeC
.L_small_initial_partial_block_fsgzrshyFGzlgeC:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_fsgzrshyFGzlgeC:

	orq	%r8,%r8
	je	.L_after_reduction_fsgzrshyFGzlgeC
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_fsgzrshyFGzlgeC:
	jmp	.L_last_blocks_done_kxapkDtebDzrjDo
.L_last_num_blocks_is_10_kxapkDtebDzrjDo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_EGkxoytiEEjBEpv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_EGkxoytiEEjBEpv

.L_16_blocks_overflow_EGkxoytiEEjBEpv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_EGkxoytiEEjBEpv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_pDlvcmCvyDdAkhs





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_pDlvcmCvyDdAkhs
.L_small_initial_partial_block_pDlvcmCvyDdAkhs:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_pDlvcmCvyDdAkhs:

	orq	%r8,%r8
	je	.L_after_reduction_pDlvcmCvyDdAkhs
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_pDlvcmCvyDdAkhs:
	jmp	.L_last_blocks_done_kxapkDtebDzrjDo
.L_last_num_blocks_is_11_kxapkDtebDzrjDo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_rknvdmwcpcFpqtv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_rknvdmwcpcFpqtv

.L_16_blocks_overflow_rknvdmwcpcFpqtv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_rknvdmwcpcFpqtv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_Craajvxwyuqphcq





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_Craajvxwyuqphcq
.L_small_initial_partial_block_Craajvxwyuqphcq:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_Craajvxwyuqphcq:

	orq	%r8,%r8
	je	.L_after_reduction_Craajvxwyuqphcq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_Craajvxwyuqphcq:
	jmp	.L_last_blocks_done_kxapkDtebDzrjDo
.L_last_num_blocks_is_12_kxapkDtebDzrjDo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_xjhEbmDvpjmnmvo
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_xjhEbmDvpjmnmvo

.L_16_blocks_overflow_xjhEbmDvpjmnmvo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_xjhEbmDvpjmnmvo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_iGbrrsGEhdytvck





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_iGbrrsGEhdytvck
.L_small_initial_partial_block_iGbrrsGEhdytvck:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_iGbrrsGEhdytvck:

	orq	%r8,%r8
	je	.L_after_reduction_iGbrrsGEhdytvck
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_iGbrrsGEhdytvck:
	jmp	.L_last_blocks_done_kxapkDtebDzrjDo
.L_last_num_blocks_is_13_kxapkDtebDzrjDo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_lBulCEFipucxarA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_lBulCEFipucxarA

.L_16_blocks_overflow_lBulCEFipucxarA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_lBulCEFipucxarA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EdcAlfnBrqcCFEC





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EdcAlfnBrqcCFEC
.L_small_initial_partial_block_EdcAlfnBrqcCFEC:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_EdcAlfnBrqcCFEC:

	orq	%r8,%r8
	je	.L_after_reduction_EdcAlfnBrqcCFEC
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_EdcAlfnBrqcCFEC:
	jmp	.L_last_blocks_done_kxapkDtebDzrjDo
.L_last_num_blocks_is_14_kxapkDtebDzrjDo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_nGblpumlfgGBxGm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_nGblpumlfgGBxGm

.L_16_blocks_overflow_nGblpumlfgGBxGm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_nGblpumlfgGBxGm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DvgGrtjaArGConl





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DvgGrtjaArGConl
.L_small_initial_partial_block_DvgGrtjaArGConl:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DvgGrtjaArGConl:

	orq	%r8,%r8
	je	.L_after_reduction_DvgGrtjaArGConl
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DvgGrtjaArGConl:
	jmp	.L_last_blocks_done_kxapkDtebDzrjDo
.L_last_num_blocks_is_15_kxapkDtebDzrjDo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_DdwpehsDrivCtmi
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_DdwpehsDrivCtmi

.L_16_blocks_overflow_DdwpehsDrivCtmi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_DdwpehsDrivCtmi:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_fEvgrhlnamaknfG





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_fEvgrhlnamaknfG
.L_small_initial_partial_block_fEvgrhlnamaknfG:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_fEvgrhlnamaknfG:

	orq	%r8,%r8
	je	.L_after_reduction_fEvgrhlnamaknfG
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_fEvgrhlnamaknfG:
	jmp	.L_last_blocks_done_kxapkDtebDzrjDo
.L_last_num_blocks_is_16_kxapkDtebDzrjDo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_CojeEDijdlrazaC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_CojeEDijdlrazaC

.L_16_blocks_overflow_CojeEDijdlrazaC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_CojeEDijdlrazaC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_eaidkwqzmaushFy:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_eaidkwqzmaushFy:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_eaidkwqzmaushFy:
	jmp	.L_last_blocks_done_kxapkDtebDzrjDo
.L_last_num_blocks_is_0_kxapkDtebDzrjDo:
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_kxapkDtebDzrjDo:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_uizbwpfDujhiqBd
.L_encrypt_32_blocks_uizbwpfDujhiqBd:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_xcfljsoirzngdik
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_xcfljsoirzngdik
.L_16_blocks_overflow_xcfljsoirzngdik:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_xcfljsoirzngdik:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_BhnamkjvqjpoBgf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_BhnamkjvqjpoBgf
.L_16_blocks_overflow_BhnamkjvqjpoBgf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_BhnamkjvqjpoBgf:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%rax,1),%zmm17
	vmovdqu8	320(%rcx,%rax,1),%zmm19
	vmovdqu8	384(%rcx,%rax,1),%zmm20
	vmovdqu8	448(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%rax,1)
	vmovdqu8	%zmm3,320(%r10,%rax,1)
	vmovdqu8	%zmm4,384(%r10,%rax,1)
	vmovdqu8	%zmm5,448(%r10,%rax,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

	subq	$512,%r8
	addq	$512,%rax
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_znnkrsjxazaBaFr

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_znnkrsjxazaBaFr
	jb	.L_last_num_blocks_is_7_1_znnkrsjxazaBaFr


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_znnkrsjxazaBaFr
	jb	.L_last_num_blocks_is_11_9_znnkrsjxazaBaFr


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_znnkrsjxazaBaFr
	ja	.L_last_num_blocks_is_16_znnkrsjxazaBaFr
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_znnkrsjxazaBaFr
	jmp	.L_last_num_blocks_is_13_znnkrsjxazaBaFr

.L_last_num_blocks_is_11_9_znnkrsjxazaBaFr:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_znnkrsjxazaBaFr
	ja	.L_last_num_blocks_is_11_znnkrsjxazaBaFr
	jmp	.L_last_num_blocks_is_9_znnkrsjxazaBaFr

.L_last_num_blocks_is_7_1_znnkrsjxazaBaFr:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_znnkrsjxazaBaFr
	jb	.L_last_num_blocks_is_3_1_znnkrsjxazaBaFr

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_znnkrsjxazaBaFr
	je	.L_last_num_blocks_is_6_znnkrsjxazaBaFr
	jmp	.L_last_num_blocks_is_5_znnkrsjxazaBaFr

.L_last_num_blocks_is_3_1_znnkrsjxazaBaFr:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_znnkrsjxazaBaFr
	je	.L_last_num_blocks_is_2_znnkrsjxazaBaFr
.L_last_num_blocks_is_1_znnkrsjxazaBaFr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_ClmzjGgGpyxEEvl
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_ClmzjGgGpyxEEvl

.L_16_blocks_overflow_ClmzjGgGpyxEEvl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_ClmzjGgGpyxEEvl:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ugrefyfvxmhCArF





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ugrefyfvxmhCArF
.L_small_initial_partial_block_ugrefyfvxmhCArF:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_ugrefyfvxmhCArF
.L_small_initial_compute_done_ugrefyfvxmhCArF:
.L_after_reduction_ugrefyfvxmhCArF:
	jmp	.L_last_blocks_done_znnkrsjxazaBaFr
.L_last_num_blocks_is_2_znnkrsjxazaBaFr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_xEslidCCDeBjzcy
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_xEslidCCDeBjzcy

.L_16_blocks_overflow_xEslidCCDeBjzcy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_xEslidCCDeBjzcy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_wuGfqjkhzkgaxrw





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_wuGfqjkhzkgaxrw
.L_small_initial_partial_block_wuGfqjkhzkgaxrw:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_wuGfqjkhzkgaxrw:

	orq	%r8,%r8
	je	.L_after_reduction_wuGfqjkhzkgaxrw
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_wuGfqjkhzkgaxrw:
	jmp	.L_last_blocks_done_znnkrsjxazaBaFr
.L_last_num_blocks_is_3_znnkrsjxazaBaFr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_jacEAhnzDisgArp
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_jacEAhnzDisgArp

.L_16_blocks_overflow_jacEAhnzDisgArp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_jacEAhnzDisgArp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EDFsquGqflEsjGm





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EDFsquGqflEsjGm
.L_small_initial_partial_block_EDFsquGqflEsjGm:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_EDFsquGqflEsjGm:

	orq	%r8,%r8
	je	.L_after_reduction_EDFsquGqflEsjGm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_EDFsquGqflEsjGm:
	jmp	.L_last_blocks_done_znnkrsjxazaBaFr
.L_last_num_blocks_is_4_znnkrsjxazaBaFr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_oscFFdaxaughgce
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_oscFFdaxaughgce

.L_16_blocks_overflow_oscFFdaxaughgce:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_oscFFdaxaughgce:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CDGgDmpFgxhApfF





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CDGgDmpFgxhApfF
.L_small_initial_partial_block_CDGgDmpFgxhApfF:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CDGgDmpFgxhApfF:

	orq	%r8,%r8
	je	.L_after_reduction_CDGgDmpFgxhApfF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_CDGgDmpFgxhApfF:
	jmp	.L_last_blocks_done_znnkrsjxazaBaFr
.L_last_num_blocks_is_5_znnkrsjxazaBaFr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_mxjhbGlddpvvCbB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_mxjhbGlddpvvCbB

.L_16_blocks_overflow_mxjhbGlddpvvCbB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_mxjhbGlddpvvCbB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_yalqiDBpoCpzEcz





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_yalqiDBpoCpzEcz
.L_small_initial_partial_block_yalqiDBpoCpzEcz:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yalqiDBpoCpzEcz:

	orq	%r8,%r8
	je	.L_after_reduction_yalqiDBpoCpzEcz
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_yalqiDBpoCpzEcz:
	jmp	.L_last_blocks_done_znnkrsjxazaBaFr
.L_last_num_blocks_is_6_znnkrsjxazaBaFr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_gmpAGivaztvCpGF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_gmpAGivaztvCpGF

.L_16_blocks_overflow_gmpAGivaztvCpGF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_gmpAGivaztvCpGF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ksAGmewphmcwxcz





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ksAGmewphmcwxcz
.L_small_initial_partial_block_ksAGmewphmcwxcz:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ksAGmewphmcwxcz:

	orq	%r8,%r8
	je	.L_after_reduction_ksAGmewphmcwxcz
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ksAGmewphmcwxcz:
	jmp	.L_last_blocks_done_znnkrsjxazaBaFr
.L_last_num_blocks_is_7_znnkrsjxazaBaFr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_yxcphraBpGvtDen
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_yxcphraBpGvtDen

.L_16_blocks_overflow_yxcphraBpGvtDen:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_yxcphraBpGvtDen:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_eaFrCbwnawdCdeo





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_eaFrCbwnawdCdeo
.L_small_initial_partial_block_eaFrCbwnawdCdeo:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_eaFrCbwnawdCdeo:

	orq	%r8,%r8
	je	.L_after_reduction_eaFrCbwnawdCdeo
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_eaFrCbwnawdCdeo:
	jmp	.L_last_blocks_done_znnkrsjxazaBaFr
.L_last_num_blocks_is_8_znnkrsjxazaBaFr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_bfEbgbgAvGntoBA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_bfEbgbgAvGntoBA

.L_16_blocks_overflow_bfEbgbgAvGntoBA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_bfEbgbgAvGntoBA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_kbrdyFsqpfaaCfk





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_kbrdyFsqpfaaCfk
.L_small_initial_partial_block_kbrdyFsqpfaaCfk:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_kbrdyFsqpfaaCfk:

	orq	%r8,%r8
	je	.L_after_reduction_kbrdyFsqpfaaCfk
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_kbrdyFsqpfaaCfk:
	jmp	.L_last_blocks_done_znnkrsjxazaBaFr
.L_last_num_blocks_is_9_znnkrsjxazaBaFr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_BDuordyhnqjzvlt
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_BDuordyhnqjzvlt

.L_16_blocks_overflow_BDuordyhnqjzvlt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_BDuordyhnqjzvlt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rBzElAyDtfEvbhk





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rBzElAyDtfEvbhk
.L_small_initial_partial_block_rBzElAyDtfEvbhk:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rBzElAyDtfEvbhk:

	orq	%r8,%r8
	je	.L_after_reduction_rBzElAyDtfEvbhk
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_rBzElAyDtfEvbhk:
	jmp	.L_last_blocks_done_znnkrsjxazaBaFr
.L_last_num_blocks_is_10_znnkrsjxazaBaFr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_GeeykynCsADtBta
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_GeeykynCsADtBta

.L_16_blocks_overflow_GeeykynCsADtBta:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_GeeykynCsADtBta:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_pshrfrnpDkvbsoc





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_pshrfrnpDkvbsoc
.L_small_initial_partial_block_pshrfrnpDkvbsoc:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_pshrfrnpDkvbsoc:

	orq	%r8,%r8
	je	.L_after_reduction_pshrfrnpDkvbsoc
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_pshrfrnpDkvbsoc:
	jmp	.L_last_blocks_done_znnkrsjxazaBaFr
.L_last_num_blocks_is_11_znnkrsjxazaBaFr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_oacvgBxdstjEAjc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_oacvgBxdstjEAjc

.L_16_blocks_overflow_oacvgBxdstjEAjc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_oacvgBxdstjEAjc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xzffwhgfttDwxsm





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xzffwhgfttDwxsm
.L_small_initial_partial_block_xzffwhgfttDwxsm:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xzffwhgfttDwxsm:

	orq	%r8,%r8
	je	.L_after_reduction_xzffwhgfttDwxsm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xzffwhgfttDwxsm:
	jmp	.L_last_blocks_done_znnkrsjxazaBaFr
.L_last_num_blocks_is_12_znnkrsjxazaBaFr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_abnAnczyijjyBnz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_abnAnczyijjyBnz

.L_16_blocks_overflow_abnAnczyijjyBnz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_abnAnczyijjyBnz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_BbCnkoihDdhpeGx





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_BbCnkoihDdhpeGx
.L_small_initial_partial_block_BbCnkoihDdhpeGx:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_BbCnkoihDdhpeGx:

	orq	%r8,%r8
	je	.L_after_reduction_BbCnkoihDdhpeGx
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_BbCnkoihDdhpeGx:
	jmp	.L_last_blocks_done_znnkrsjxazaBaFr
.L_last_num_blocks_is_13_znnkrsjxazaBaFr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_hylsdDzprfrEClB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_hylsdDzprfrEClB

.L_16_blocks_overflow_hylsdDzprfrEClB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_hylsdDzprfrEClB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vgwkbnbvjjmxeCB





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vgwkbnbvjjmxeCB
.L_small_initial_partial_block_vgwkbnbvjjmxeCB:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_vgwkbnbvjjmxeCB:

	orq	%r8,%r8
	je	.L_after_reduction_vgwkbnbvjjmxeCB
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_vgwkbnbvjjmxeCB:
	jmp	.L_last_blocks_done_znnkrsjxazaBaFr
.L_last_num_blocks_is_14_znnkrsjxazaBaFr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_jvBvbdtDknidwjD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_jvBvbdtDknidwjD

.L_16_blocks_overflow_jvBvbdtDknidwjD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_jvBvbdtDknidwjD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ztcfluhaoxvrqGl





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ztcfluhaoxvrqGl
.L_small_initial_partial_block_ztcfluhaoxvrqGl:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ztcfluhaoxvrqGl:

	orq	%r8,%r8
	je	.L_after_reduction_ztcfluhaoxvrqGl
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ztcfluhaoxvrqGl:
	jmp	.L_last_blocks_done_znnkrsjxazaBaFr
.L_last_num_blocks_is_15_znnkrsjxazaBaFr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_corxflmpekglgEz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_corxflmpekglgEz

.L_16_blocks_overflow_corxflmpekglgEz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_corxflmpekglgEz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_sxFBeloetjdEtvx





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_sxFBeloetjdEtvx
.L_small_initial_partial_block_sxFBeloetjdEtvx:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_sxFBeloetjdEtvx:

	orq	%r8,%r8
	je	.L_after_reduction_sxFBeloetjdEtvx
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_sxFBeloetjdEtvx:
	jmp	.L_last_blocks_done_znnkrsjxazaBaFr
.L_last_num_blocks_is_16_znnkrsjxazaBaFr:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_DwECeacaEtBofes
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_DwECeacaEtBofes

.L_16_blocks_overflow_DwECeacaEtBofes:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_DwECeacaEtBofes:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_mAopDcDfmGoEacx:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_mAopDcDfmGoEacx:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_mAopDcDfmGoEacx:
	jmp	.L_last_blocks_done_znnkrsjxazaBaFr
.L_last_num_blocks_is_0_znnkrsjxazaBaFr:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_znnkrsjxazaBaFr:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_uizbwpfDujhiqBd
.L_encrypt_16_blocks_uizbwpfDujhiqBd:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_pFhdemtqftlCkiu
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_pFhdemtqftlCkiu
.L_16_blocks_overflow_pFhdemtqftlCkiu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_pFhdemtqftlCkiu:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1)
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	256(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	320(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	384(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	448(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	subq	$256,%r8
	addq	$256,%rax
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_pdkDrxAainrxujd

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_pdkDrxAainrxujd
	jb	.L_last_num_blocks_is_7_1_pdkDrxAainrxujd


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_pdkDrxAainrxujd
	jb	.L_last_num_blocks_is_11_9_pdkDrxAainrxujd


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_pdkDrxAainrxujd
	ja	.L_last_num_blocks_is_16_pdkDrxAainrxujd
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_pdkDrxAainrxujd
	jmp	.L_last_num_blocks_is_13_pdkDrxAainrxujd

.L_last_num_blocks_is_11_9_pdkDrxAainrxujd:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_pdkDrxAainrxujd
	ja	.L_last_num_blocks_is_11_pdkDrxAainrxujd
	jmp	.L_last_num_blocks_is_9_pdkDrxAainrxujd

.L_last_num_blocks_is_7_1_pdkDrxAainrxujd:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_pdkDrxAainrxujd
	jb	.L_last_num_blocks_is_3_1_pdkDrxAainrxujd

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_pdkDrxAainrxujd
	je	.L_last_num_blocks_is_6_pdkDrxAainrxujd
	jmp	.L_last_num_blocks_is_5_pdkDrxAainrxujd

.L_last_num_blocks_is_3_1_pdkDrxAainrxujd:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_pdkDrxAainrxujd
	je	.L_last_num_blocks_is_2_pdkDrxAainrxujd
.L_last_num_blocks_is_1_pdkDrxAainrxujd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_ChbuguwdfgFfpBB
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_ChbuguwdfgFfpBB

.L_16_blocks_overflow_ChbuguwdfgFfpBB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_ChbuguwdfgFfpBB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%xmm31,%xmm0,%xmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zhgCidbEodrwABy





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zhgCidbEodrwABy
.L_small_initial_partial_block_zhgCidbEodrwABy:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)











	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_zhgCidbEodrwABy
.L_small_initial_compute_done_zhgCidbEodrwABy:
.L_after_reduction_zhgCidbEodrwABy:
	jmp	.L_last_blocks_done_pdkDrxAainrxujd
.L_last_num_blocks_is_2_pdkDrxAainrxujd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_pjqgDyvltfhvmio
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_pjqgDyvltfhvmio

.L_16_blocks_overflow_pjqgDyvltfhvmio:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_pjqgDyvltfhvmio:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%ymm31,%ymm0,%ymm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cwyesEElEfEjCru





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cwyesEElEfEjCru
.L_small_initial_partial_block_cwyesEElEfEjCru:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cwyesEElEfEjCru:

	orq	%r8,%r8
	je	.L_after_reduction_cwyesEElEfEjCru
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cwyesEElEfEjCru:
	jmp	.L_last_blocks_done_pdkDrxAainrxujd
.L_last_num_blocks_is_3_pdkDrxAainrxujd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_jCoaGhsdEAxwDae
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_jCoaGhsdEAxwDae

.L_16_blocks_overflow_jCoaGhsdEAxwDae:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_jCoaGhsdEAxwDae:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_pqxCpAqntgAsGgb





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_pqxCpAqntgAsGgb
.L_small_initial_partial_block_pqxCpAqntgAsGgb:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_pqxCpAqntgAsGgb:

	orq	%r8,%r8
	je	.L_after_reduction_pqxCpAqntgAsGgb
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_pqxCpAqntgAsGgb:
	jmp	.L_last_blocks_done_pdkDrxAainrxujd
.L_last_num_blocks_is_4_pdkDrxAainrxujd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_svFadkldGvinscl
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_svFadkldGvinscl

.L_16_blocks_overflow_svFadkldGvinscl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_svFadkldGvinscl:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_fFfzbcciyrgierx





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_fFfzbcciyrgierx
.L_small_initial_partial_block_fFfzbcciyrgierx:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_fFfzbcciyrgierx:

	orq	%r8,%r8
	je	.L_after_reduction_fFfzbcciyrgierx
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_fFfzbcciyrgierx:
	jmp	.L_last_blocks_done_pdkDrxAainrxujd
.L_last_num_blocks_is_5_pdkDrxAainrxujd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_hCzxldtuqDuBcqx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_hCzxldtuqDuBcqx

.L_16_blocks_overflow_hCzxldtuqDuBcqx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_hCzxldtuqDuBcqx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_mleaDmCbCkGxjbG





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_mleaDmCbCkGxjbG
.L_small_initial_partial_block_mleaDmCbCkGxjbG:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_mleaDmCbCkGxjbG:

	orq	%r8,%r8
	je	.L_after_reduction_mleaDmCbCkGxjbG
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_mleaDmCbCkGxjbG:
	jmp	.L_last_blocks_done_pdkDrxAainrxujd
.L_last_num_blocks_is_6_pdkDrxAainrxujd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_mocfigFsoxfEmbu
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_mocfigFsoxfEmbu

.L_16_blocks_overflow_mocfigFsoxfEmbu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_mocfigFsoxfEmbu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_eGGpaijADafADAa





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_eGGpaijADafADAa
.L_small_initial_partial_block_eGGpaijADafADAa:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_eGGpaijADafADAa:

	orq	%r8,%r8
	je	.L_after_reduction_eGGpaijADafADAa
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_eGGpaijADafADAa:
	jmp	.L_last_blocks_done_pdkDrxAainrxujd
.L_last_num_blocks_is_7_pdkDrxAainrxujd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_dhhlklcefxGuoiw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_dhhlklcefxGuoiw

.L_16_blocks_overflow_dhhlklcefxGuoiw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_dhhlklcefxGuoiw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_lqwahqeuBzvAnFc





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_lqwahqeuBzvAnFc
.L_small_initial_partial_block_lqwahqeuBzvAnFc:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lqwahqeuBzvAnFc:

	orq	%r8,%r8
	je	.L_after_reduction_lqwahqeuBzvAnFc
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_lqwahqeuBzvAnFc:
	jmp	.L_last_blocks_done_pdkDrxAainrxujd
.L_last_num_blocks_is_8_pdkDrxAainrxujd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_wzsppiktmwpwtyl
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_wzsppiktmwpwtyl

.L_16_blocks_overflow_wzsppiktmwpwtyl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_wzsppiktmwpwtyl:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_edcxgalvnqpACBj





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_edcxgalvnqpACBj
.L_small_initial_partial_block_edcxgalvnqpACBj:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_edcxgalvnqpACBj:

	orq	%r8,%r8
	je	.L_after_reduction_edcxgalvnqpACBj
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_edcxgalvnqpACBj:
	jmp	.L_last_blocks_done_pdkDrxAainrxujd
.L_last_num_blocks_is_9_pdkDrxAainrxujd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_FcpybfvbflyAszh
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_FcpybfvbflyAszh

.L_16_blocks_overflow_FcpybfvbflyAszh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_FcpybfvbflyAszh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_yewaumnrtBiAwAG





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_yewaumnrtBiAwAG
.L_small_initial_partial_block_yewaumnrtBiAwAG:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yewaumnrtBiAwAG:

	orq	%r8,%r8
	je	.L_after_reduction_yewaumnrtBiAwAG
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_yewaumnrtBiAwAG:
	jmp	.L_last_blocks_done_pdkDrxAainrxujd
.L_last_num_blocks_is_10_pdkDrxAainrxujd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_xtmgmidrvdgAAEq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_xtmgmidrvdgAAEq

.L_16_blocks_overflow_xtmgmidrvdgAAEq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_xtmgmidrvdgAAEq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CxylBclavrkztqo





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CxylBclavrkztqo
.L_small_initial_partial_block_CxylBclavrkztqo:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CxylBclavrkztqo:

	orq	%r8,%r8
	je	.L_after_reduction_CxylBclavrkztqo
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_CxylBclavrkztqo:
	jmp	.L_last_blocks_done_pdkDrxAainrxujd
.L_last_num_blocks_is_11_pdkDrxAainrxujd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_kmaDBlkiatrAdqc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_kmaDBlkiatrAdqc

.L_16_blocks_overflow_kmaDBlkiatrAdqc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_kmaDBlkiatrAdqc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_sCbojDqtvypwohi





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_sCbojDqtvypwohi
.L_small_initial_partial_block_sCbojDqtvypwohi:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_sCbojDqtvypwohi:

	orq	%r8,%r8
	je	.L_after_reduction_sCbojDqtvypwohi
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_sCbojDqtvypwohi:
	jmp	.L_last_blocks_done_pdkDrxAainrxujd
.L_last_num_blocks_is_12_pdkDrxAainrxujd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_vCkwnDxzAawknuF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_vCkwnDxzAawknuF

.L_16_blocks_overflow_vCkwnDxzAawknuF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_vCkwnDxzAawknuF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CEAaczvBxxCxDxG





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CEAaczvBxxCxDxG
.L_small_initial_partial_block_CEAaczvBxxCxDxG:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CEAaczvBxxCxDxG:

	orq	%r8,%r8
	je	.L_after_reduction_CEAaczvBxxCxDxG
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_CEAaczvBxxCxDxG:
	jmp	.L_last_blocks_done_pdkDrxAainrxujd
.L_last_num_blocks_is_13_pdkDrxAainrxujd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_GAzxjqFzFzzpptl
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_GAzxjqFzFzzpptl

.L_16_blocks_overflow_GAzxjqFzFzzpptl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_GAzxjqFzFzzpptl:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nBbdFfqyBairtrs





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nBbdFfqyBairtrs
.L_small_initial_partial_block_nBbdFfqyBairtrs:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nBbdFfqyBairtrs:

	orq	%r8,%r8
	je	.L_after_reduction_nBbdFfqyBairtrs
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_nBbdFfqyBairtrs:
	jmp	.L_last_blocks_done_pdkDrxAainrxujd
.L_last_num_blocks_is_14_pdkDrxAainrxujd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_FybpcinCEcCkDzF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_FybpcinCEcCkDzF

.L_16_blocks_overflow_FybpcinCEcCkDzF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_FybpcinCEcCkDzF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gGqesbaBjwDpiDE





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gGqesbaBjwDpiDE
.L_small_initial_partial_block_gGqesbaBjwDpiDE:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gGqesbaBjwDpiDE:

	orq	%r8,%r8
	je	.L_after_reduction_gGqesbaBjwDpiDE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_gGqesbaBjwDpiDE:
	jmp	.L_last_blocks_done_pdkDrxAainrxujd
.L_last_num_blocks_is_15_pdkDrxAainrxujd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_euegpsyigoxsfup
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_euegpsyigoxsfup

.L_16_blocks_overflow_euegpsyigoxsfup:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_euegpsyigoxsfup:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ajFhswhCfBcFdCb





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ajFhswhCfBcFdCb
.L_small_initial_partial_block_ajFhswhCfBcFdCb:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ajFhswhCfBcFdCb:

	orq	%r8,%r8
	je	.L_after_reduction_ajFhswhCfBcFdCb
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ajFhswhCfBcFdCb:
	jmp	.L_last_blocks_done_pdkDrxAainrxujd
.L_last_num_blocks_is_16_pdkDrxAainrxujd:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_oCsjjmreFqjzeyG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_oCsjjmreFqjzeyG

.L_16_blocks_overflow_oCsjjmreFqjzeyG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_oCsjjmreFqjzeyG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_yGxCyCzmEgiBiwm:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yGxCyCzmEgiBiwm:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_yGxCyCzmEgiBiwm:
	jmp	.L_last_blocks_done_pdkDrxAainrxujd
.L_last_num_blocks_is_0_pdkDrxAainrxujd:
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_pdkDrxAainrxujd:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_uizbwpfDujhiqBd

.L_message_below_32_blocks_uizbwpfDujhiqBd:


	subq	$256,%r8
	addq	$256,%rax
	movl	%r8d,%r10d
	leaq	96(%rsi),%r12
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_sfayAhEknbmFpne
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)
.L_skip_hkeys_precomputation_sfayAhEknbmFpne:
	movq	$1,%r14
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_jDGjdtrFkyzuijo

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_jDGjdtrFkyzuijo
	jb	.L_last_num_blocks_is_7_1_jDGjdtrFkyzuijo


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_jDGjdtrFkyzuijo
	jb	.L_last_num_blocks_is_11_9_jDGjdtrFkyzuijo


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_jDGjdtrFkyzuijo
	ja	.L_last_num_blocks_is_16_jDGjdtrFkyzuijo
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_jDGjdtrFkyzuijo
	jmp	.L_last_num_blocks_is_13_jDGjdtrFkyzuijo

.L_last_num_blocks_is_11_9_jDGjdtrFkyzuijo:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_jDGjdtrFkyzuijo
	ja	.L_last_num_blocks_is_11_jDGjdtrFkyzuijo
	jmp	.L_last_num_blocks_is_9_jDGjdtrFkyzuijo

.L_last_num_blocks_is_7_1_jDGjdtrFkyzuijo:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_jDGjdtrFkyzuijo
	jb	.L_last_num_blocks_is_3_1_jDGjdtrFkyzuijo

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_jDGjdtrFkyzuijo
	je	.L_last_num_blocks_is_6_jDGjdtrFkyzuijo
	jmp	.L_last_num_blocks_is_5_jDGjdtrFkyzuijo

.L_last_num_blocks_is_3_1_jDGjdtrFkyzuijo:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_jDGjdtrFkyzuijo
	je	.L_last_num_blocks_is_2_jDGjdtrFkyzuijo
.L_last_num_blocks_is_1_jDGjdtrFkyzuijo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_rmqBlzdxealddCh
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_rmqBlzdxealddCh

.L_16_blocks_overflow_rmqBlzdxealddCh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_rmqBlzdxealddCh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_yscigjlhoondDcs





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_yscigjlhoondDcs
.L_small_initial_partial_block_yscigjlhoondDcs:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_yscigjlhoondDcs
.L_small_initial_compute_done_yscigjlhoondDcs:
.L_after_reduction_yscigjlhoondDcs:
	jmp	.L_last_blocks_done_jDGjdtrFkyzuijo
.L_last_num_blocks_is_2_jDGjdtrFkyzuijo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_fwxagdmBfojlhCa
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_fwxagdmBfojlhCa

.L_16_blocks_overflow_fwxagdmBfojlhCa:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_fwxagdmBfojlhCa:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qfElklmrwmChypx





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qfElklmrwmChypx
.L_small_initial_partial_block_qfElklmrwmChypx:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qfElklmrwmChypx:

	orq	%r8,%r8
	je	.L_after_reduction_qfElklmrwmChypx
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_qfElklmrwmChypx:
	jmp	.L_last_blocks_done_jDGjdtrFkyzuijo
.L_last_num_blocks_is_3_jDGjdtrFkyzuijo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_tsDkxygvtdpdnlr
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_tsDkxygvtdpdnlr

.L_16_blocks_overflow_tsDkxygvtdpdnlr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_tsDkxygvtdpdnlr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rAlcpiFDxCazoBe





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rAlcpiFDxCazoBe
.L_small_initial_partial_block_rAlcpiFDxCazoBe:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rAlcpiFDxCazoBe:

	orq	%r8,%r8
	je	.L_after_reduction_rAlcpiFDxCazoBe
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_rAlcpiFDxCazoBe:
	jmp	.L_last_blocks_done_jDGjdtrFkyzuijo
.L_last_num_blocks_is_4_jDGjdtrFkyzuijo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_abkzanndusFaaGf
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_abkzanndusFaaGf

.L_16_blocks_overflow_abkzanndusFaaGf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_abkzanndusFaaGf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_laejtFGkqkFmFFf





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_laejtFGkqkFmFFf
.L_small_initial_partial_block_laejtFGkqkFmFFf:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_laejtFGkqkFmFFf:

	orq	%r8,%r8
	je	.L_after_reduction_laejtFGkqkFmFFf
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_laejtFGkqkFmFFf:
	jmp	.L_last_blocks_done_jDGjdtrFkyzuijo
.L_last_num_blocks_is_5_jDGjdtrFkyzuijo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_gFqmynjBabqlfhC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_gFqmynjBabqlfhC

.L_16_blocks_overflow_gFqmynjBabqlfhC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_gFqmynjBabqlfhC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%xmm29,%xmm3,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_BBxqfoyuojcoqpg





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_BBxqfoyuojcoqpg
.L_small_initial_partial_block_BBxqfoyuojcoqpg:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_BBxqfoyuojcoqpg:

	orq	%r8,%r8
	je	.L_after_reduction_BBxqfoyuojcoqpg
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_BBxqfoyuojcoqpg:
	jmp	.L_last_blocks_done_jDGjdtrFkyzuijo
.L_last_num_blocks_is_6_jDGjdtrFkyzuijo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_mpepfbaplaeozdg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_mpepfbaplaeozdg

.L_16_blocks_overflow_mpepfbaplaeozdg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_mpepfbaplaeozdg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%ymm29,%ymm3,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rcEkdlqsmixxwFs





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rcEkdlqsmixxwFs
.L_small_initial_partial_block_rcEkdlqsmixxwFs:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rcEkdlqsmixxwFs:

	orq	%r8,%r8
	je	.L_after_reduction_rcEkdlqsmixxwFs
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_rcEkdlqsmixxwFs:
	jmp	.L_last_blocks_done_jDGjdtrFkyzuijo
.L_last_num_blocks_is_7_jDGjdtrFkyzuijo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_wGBkigtyDeChDvm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_wGBkigtyDeChDvm

.L_16_blocks_overflow_wGBkigtyDeChDvm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_wGBkigtyDeChDvm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bihutvxmsolqwGx





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bihutvxmsolqwGx
.L_small_initial_partial_block_bihutvxmsolqwGx:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bihutvxmsolqwGx:

	orq	%r8,%r8
	je	.L_after_reduction_bihutvxmsolqwGx
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_bihutvxmsolqwGx:
	jmp	.L_last_blocks_done_jDGjdtrFkyzuijo
.L_last_num_blocks_is_8_jDGjdtrFkyzuijo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_ggfoohDbDAnFgvf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_ggfoohDbDAnFgvf

.L_16_blocks_overflow_ggfoohDbDAnFgvf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_ggfoohDbDAnFgvf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cbCfhvyoCgBnFvl





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cbCfhvyoCgBnFvl
.L_small_initial_partial_block_cbCfhvyoCgBnFvl:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cbCfhvyoCgBnFvl:

	orq	%r8,%r8
	je	.L_after_reduction_cbCfhvyoCgBnFvl
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cbCfhvyoCgBnFvl:
	jmp	.L_last_blocks_done_jDGjdtrFkyzuijo
.L_last_num_blocks_is_9_jDGjdtrFkyzuijo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_beFmzuzodmcDgft
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_beFmzuzodmcDgft

.L_16_blocks_overflow_beFmzuzodmcDgft:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_beFmzuzodmcDgft:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%xmm29,%xmm4,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_AyzzzEpsCnlByqu





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_AyzzzEpsCnlByqu
.L_small_initial_partial_block_AyzzzEpsCnlByqu:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_AyzzzEpsCnlByqu:

	orq	%r8,%r8
	je	.L_after_reduction_AyzzzEpsCnlByqu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_AyzzzEpsCnlByqu:
	jmp	.L_last_blocks_done_jDGjdtrFkyzuijo
.L_last_num_blocks_is_10_jDGjdtrFkyzuijo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_hBDkfxvpwDqeukB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_hBDkfxvpwDqeukB

.L_16_blocks_overflow_hBDkfxvpwDqeukB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_hBDkfxvpwDqeukB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%ymm29,%ymm4,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xDaqapfEmxdutgy





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xDaqapfEmxdutgy
.L_small_initial_partial_block_xDaqapfEmxdutgy:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xDaqapfEmxdutgy:

	orq	%r8,%r8
	je	.L_after_reduction_xDaqapfEmxdutgy
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xDaqapfEmxdutgy:
	jmp	.L_last_blocks_done_jDGjdtrFkyzuijo
.L_last_num_blocks_is_11_jDGjdtrFkyzuijo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_pnwicxGuywgoavi
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_pnwicxGuywgoavi

.L_16_blocks_overflow_pnwicxGuywgoavi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_pnwicxGuywgoavi:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DsuBbFFddhlvxbf





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DsuBbFFddhlvxbf
.L_small_initial_partial_block_DsuBbFFddhlvxbf:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DsuBbFFddhlvxbf:

	orq	%r8,%r8
	je	.L_after_reduction_DsuBbFFddhlvxbf
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DsuBbFFddhlvxbf:
	jmp	.L_last_blocks_done_jDGjdtrFkyzuijo
.L_last_num_blocks_is_12_jDGjdtrFkyzuijo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_uGDgqmwsADlkcBp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_uGDgqmwsADlkcBp

.L_16_blocks_overflow_uGDgqmwsADlkcBp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_uGDgqmwsADlkcBp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_saocDxyoqGoknfv





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_saocDxyoqGoknfv
.L_small_initial_partial_block_saocDxyoqGoknfv:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_saocDxyoqGoknfv:

	orq	%r8,%r8
	je	.L_after_reduction_saocDxyoqGoknfv
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_saocDxyoqGoknfv:
	jmp	.L_last_blocks_done_jDGjdtrFkyzuijo
.L_last_num_blocks_is_13_jDGjdtrFkyzuijo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_giezmztkoChtFmv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_giezmztkoChtFmv

.L_16_blocks_overflow_giezmztkoChtFmv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_giezmztkoChtFmv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%xmm29,%xmm5,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ElpkzyCydsyoome





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ElpkzyCydsyoome
.L_small_initial_partial_block_ElpkzyCydsyoome:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ElpkzyCydsyoome:

	orq	%r8,%r8
	je	.L_after_reduction_ElpkzyCydsyoome
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ElpkzyCydsyoome:
	jmp	.L_last_blocks_done_jDGjdtrFkyzuijo
.L_last_num_blocks_is_14_jDGjdtrFkyzuijo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_EtgyFakiAFmxpxg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_EtgyFakiAFmxpxg

.L_16_blocks_overflow_EtgyFakiAFmxpxg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_EtgyFakiAFmxpxg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%ymm29,%ymm5,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bcviwphAeAGycGr





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bcviwphAeAGycGr
.L_small_initial_partial_block_bcviwphAeAGycGr:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bcviwphAeAGycGr:

	orq	%r8,%r8
	je	.L_after_reduction_bcviwphAeAGycGr
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_bcviwphAeAGycGr:
	jmp	.L_last_blocks_done_jDGjdtrFkyzuijo
.L_last_num_blocks_is_15_jDGjdtrFkyzuijo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_weqautvohwzqqal
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_weqautvohwzqqal

.L_16_blocks_overflow_weqautvohwzqqal:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_weqautvohwzqqal:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CwGClmxiCguyqyw





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CwGClmxiCguyqyw
.L_small_initial_partial_block_CwGClmxiCguyqyw:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CwGClmxiCguyqyw:

	orq	%r8,%r8
	je	.L_after_reduction_CwGClmxiCguyqyw
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_CwGClmxiCguyqyw:
	jmp	.L_last_blocks_done_jDGjdtrFkyzuijo
.L_last_num_blocks_is_16_jDGjdtrFkyzuijo:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_sgnnsxErvmtEfnG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_sgnnsxErvmtEfnG

.L_16_blocks_overflow_sgnnsxErvmtEfnG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_sgnnsxErvmtEfnG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm17
	vpshufb	%zmm29,%zmm3,%zmm19
	vpshufb	%zmm29,%zmm4,%zmm20
	vpshufb	%zmm29,%zmm5,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_fDzhmbsEyyfhwib:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_fDzhmbsEyyfhwib:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_fDzhmbsEyyfhwib:
	jmp	.L_last_blocks_done_jDGjdtrFkyzuijo
.L_last_num_blocks_is_0_jDGjdtrFkyzuijo:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_jDGjdtrFkyzuijo:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_uizbwpfDujhiqBd

.L_message_below_equal_16_blocks_uizbwpfDujhiqBd:


	movl	%r8d,%r12d
	addl	$15,%r12d
	shrl	$4,%r12d
	cmpq	$8,%r12
	je	.L_small_initial_num_blocks_is_8_cticcGijxzhCayF
	jl	.L_small_initial_num_blocks_is_7_1_cticcGijxzhCayF


	cmpq	$12,%r12
	je	.L_small_initial_num_blocks_is_12_cticcGijxzhCayF
	jl	.L_small_initial_num_blocks_is_11_9_cticcGijxzhCayF


	cmpq	$16,%r12
	je	.L_small_initial_num_blocks_is_16_cticcGijxzhCayF
	cmpq	$15,%r12
	je	.L_small_initial_num_blocks_is_15_cticcGijxzhCayF
	cmpq	$14,%r12
	je	.L_small_initial_num_blocks_is_14_cticcGijxzhCayF
	jmp	.L_small_initial_num_blocks_is_13_cticcGijxzhCayF

.L_small_initial_num_blocks_is_11_9_cticcGijxzhCayF:

	cmpq	$11,%r12
	je	.L_small_initial_num_blocks_is_11_cticcGijxzhCayF
	cmpq	$10,%r12
	je	.L_small_initial_num_blocks_is_10_cticcGijxzhCayF
	jmp	.L_small_initial_num_blocks_is_9_cticcGijxzhCayF

.L_small_initial_num_blocks_is_7_1_cticcGijxzhCayF:
	cmpq	$4,%r12
	je	.L_small_initial_num_blocks_is_4_cticcGijxzhCayF
	jl	.L_small_initial_num_blocks_is_3_1_cticcGijxzhCayF

	cmpq	$7,%r12
	je	.L_small_initial_num_blocks_is_7_cticcGijxzhCayF
	cmpq	$6,%r12
	je	.L_small_initial_num_blocks_is_6_cticcGijxzhCayF
	jmp	.L_small_initial_num_blocks_is_5_cticcGijxzhCayF

.L_small_initial_num_blocks_is_3_1_cticcGijxzhCayF:

	cmpq	$3,%r12
	je	.L_small_initial_num_blocks_is_3_cticcGijxzhCayF
	cmpq	$2,%r12
	je	.L_small_initial_num_blocks_is_2_cticcGijxzhCayF





.L_small_initial_num_blocks_is_1_cticcGijxzhCayF:
	vmovdqa64	SHUF_MASK(%rip),%xmm29
	vpaddd	ONE(%rip),%xmm2,%xmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm0,%xmm2
	vpshufb	%xmm29,%xmm0,%xmm0
	vmovdqu8	0(%rcx,%rax,1),%xmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%xmm15,%xmm0,%xmm0
	vpxorq	%xmm6,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm0,%xmm6
	vextracti32x4	$0,%zmm6,%xmm13
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CqmtDcAtplbjjwb





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CqmtDcAtplbjjwb
.L_small_initial_partial_block_CqmtDcAtplbjjwb:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)











	vpxorq	%xmm13,%xmm14,%xmm14

	jmp	.L_after_reduction_CqmtDcAtplbjjwb
.L_small_initial_compute_done_CqmtDcAtplbjjwb:
.L_after_reduction_CqmtDcAtplbjjwb:
	jmp	.L_small_initial_blocks_encrypted_cticcGijxzhCayF
.L_small_initial_num_blocks_is_2_cticcGijxzhCayF:
	vmovdqa64	SHUF_MASK(%rip),%ymm29
	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm0,%xmm2
	vpshufb	%ymm29,%ymm0,%ymm0
	vmovdqu8	0(%rcx,%rax,1),%ymm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%ymm15,%ymm0,%ymm0
	vpxorq	%ymm6,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm0,%ymm6
	vextracti32x4	$1,%zmm6,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_hFBpnlBmpmoBEEc





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_hFBpnlBmpmoBEEc
.L_small_initial_partial_block_hFBpnlBmpmoBEEc:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_hFBpnlBmpmoBEEc:

	orq	%r8,%r8
	je	.L_after_reduction_hFBpnlBmpmoBEEc
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_hFBpnlBmpmoBEEc:
	jmp	.L_small_initial_blocks_encrypted_cticcGijxzhCayF
.L_small_initial_num_blocks_is_3_cticcGijxzhCayF:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vextracti32x4	$2,%zmm6,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_chrccFAcDbyvlBF





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_chrccFAcDbyvlBF
.L_small_initial_partial_block_chrccFAcDbyvlBF:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_chrccFAcDbyvlBF:

	orq	%r8,%r8
	je	.L_after_reduction_chrccFAcDbyvlBF
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_chrccFAcDbyvlBF:
	jmp	.L_small_initial_blocks_encrypted_cticcGijxzhCayF
.L_small_initial_num_blocks_is_4_cticcGijxzhCayF:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vextracti32x4	$3,%zmm6,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_hyrwjrDgwuppdxo





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_hyrwjrDgwuppdxo
.L_small_initial_partial_block_hyrwjrDgwuppdxo:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_hyrwjrDgwuppdxo:

	orq	%r8,%r8
	je	.L_after_reduction_hyrwjrDgwuppdxo
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_hyrwjrDgwuppdxo:
	jmp	.L_small_initial_blocks_encrypted_cticcGijxzhCayF
.L_small_initial_num_blocks_is_5_cticcGijxzhCayF:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%xmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%xmm15,%xmm3,%xmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%xmm7,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%xmm29,%xmm3,%xmm7
	vextracti32x4	$0,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_kptGtExgxmgdDtA





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_kptGtExgxmgdDtA
.L_small_initial_partial_block_kptGtExgxmgdDtA:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_kptGtExgxmgdDtA:

	orq	%r8,%r8
	je	.L_after_reduction_kptGtExgxmgdDtA
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_kptGtExgxmgdDtA:
	jmp	.L_small_initial_blocks_encrypted_cticcGijxzhCayF
.L_small_initial_num_blocks_is_6_cticcGijxzhCayF:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%ymm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%ymm15,%ymm3,%ymm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%ymm7,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%ymm29,%ymm3,%ymm7
	vextracti32x4	$1,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jzAhrwtrCovywyj





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jzAhrwtrCovywyj
.L_small_initial_partial_block_jzAhrwtrCovywyj:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jzAhrwtrCovywyj:

	orq	%r8,%r8
	je	.L_after_reduction_jzAhrwtrCovywyj
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_jzAhrwtrCovywyj:
	jmp	.L_small_initial_blocks_encrypted_cticcGijxzhCayF
.L_small_initial_num_blocks_is_7_cticcGijxzhCayF:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vextracti32x4	$2,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_acvEmsmGuwuyizh





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_acvEmsmGuwuyizh
.L_small_initial_partial_block_acvEmsmGuwuyizh:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_acvEmsmGuwuyizh:

	orq	%r8,%r8
	je	.L_after_reduction_acvEmsmGuwuyizh
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_acvEmsmGuwuyizh:
	jmp	.L_small_initial_blocks_encrypted_cticcGijxzhCayF
.L_small_initial_num_blocks_is_8_cticcGijxzhCayF:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vextracti32x4	$3,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_yohmqaFeswxpqFs





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_yohmqaFeswxpqFs
.L_small_initial_partial_block_yohmqaFeswxpqFs:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yohmqaFeswxpqFs:

	orq	%r8,%r8
	je	.L_after_reduction_yohmqaFeswxpqFs
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_yohmqaFeswxpqFs:
	jmp	.L_small_initial_blocks_encrypted_cticcGijxzhCayF
.L_small_initial_num_blocks_is_9_cticcGijxzhCayF:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%xmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%xmm15,%xmm4,%xmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%xmm10,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%xmm29,%xmm4,%xmm10
	vextracti32x4	$0,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nbmopeGBigrFDng





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nbmopeGBigrFDng
.L_small_initial_partial_block_nbmopeGBigrFDng:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nbmopeGBigrFDng:

	orq	%r8,%r8
	je	.L_after_reduction_nbmopeGBigrFDng
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_nbmopeGBigrFDng:
	jmp	.L_small_initial_blocks_encrypted_cticcGijxzhCayF
.L_small_initial_num_blocks_is_10_cticcGijxzhCayF:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%ymm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%ymm15,%ymm4,%ymm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%ymm10,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%ymm29,%ymm4,%ymm10
	vextracti32x4	$1,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zcmzwyxCjzrwumx





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zcmzwyxCjzrwumx
.L_small_initial_partial_block_zcmzwyxCjzrwumx:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zcmzwyxCjzrwumx:

	orq	%r8,%r8
	je	.L_after_reduction_zcmzwyxCjzrwumx
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_zcmzwyxCjzrwumx:
	jmp	.L_small_initial_blocks_encrypted_cticcGijxzhCayF
.L_small_initial_num_blocks_is_11_cticcGijxzhCayF:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vextracti32x4	$2,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rqDtazbBriCCang





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rqDtazbBriCCang
.L_small_initial_partial_block_rqDtazbBriCCang:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rqDtazbBriCCang:

	orq	%r8,%r8
	je	.L_after_reduction_rqDtazbBriCCang
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_rqDtazbBriCCang:
	jmp	.L_small_initial_blocks_encrypted_cticcGijxzhCayF
.L_small_initial_num_blocks_is_12_cticcGijxzhCayF:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vextracti32x4	$3,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zxvDwgjqepzFgCE





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	64(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zxvDwgjqepzFgCE
.L_small_initial_partial_block_zxvDwgjqepzFgCE:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zxvDwgjqepzFgCE:

	orq	%r8,%r8
	je	.L_after_reduction_zxvDwgjqepzFgCE
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_zxvDwgjqepzFgCE:
	jmp	.L_small_initial_blocks_encrypted_cticcGijxzhCayF
.L_small_initial_num_blocks_is_13_cticcGijxzhCayF:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%xmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%xmm15,%xmm5,%xmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%xmm11,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%xmm29,%xmm5,%xmm11
	vextracti32x4	$0,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dcceBkmvjugvwoq





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	48(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dcceBkmvjugvwoq
.L_small_initial_partial_block_dcceBkmvjugvwoq:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	64(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dcceBkmvjugvwoq:

	orq	%r8,%r8
	je	.L_after_reduction_dcceBkmvjugvwoq
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_dcceBkmvjugvwoq:
	jmp	.L_small_initial_blocks_encrypted_cticcGijxzhCayF
.L_small_initial_num_blocks_is_14_cticcGijxzhCayF:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%ymm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%ymm15,%ymm5,%ymm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%ymm11,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%ymm29,%ymm5,%ymm11
	vextracti32x4	$1,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_iDidBolonprlCBv





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	32(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_iDidBolonprlCBv
.L_small_initial_partial_block_iDidBolonprlCBv:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	48(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_iDidBolonprlCBv:

	orq	%r8,%r8
	je	.L_after_reduction_iDidBolonprlCBv
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_iDidBolonprlCBv:
	jmp	.L_small_initial_blocks_encrypted_cticcGijxzhCayF
.L_small_initial_num_blocks_is_15_cticcGijxzhCayF:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%zmm29,%zmm5,%zmm11
	vextracti32x4	$2,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dobgqrzBcgdGBwj





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	16(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dobgqrzBcgdGBwj
.L_small_initial_partial_block_dobgqrzBcgdGBwj:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	32(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dobgqrzBcgdGBwj:

	orq	%r8,%r8
	je	.L_after_reduction_dobgqrzBcgdGBwj
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_dobgqrzBcgdGBwj:
	jmp	.L_small_initial_blocks_encrypted_cticcGijxzhCayF
.L_small_initial_num_blocks_is_16_cticcGijxzhCayF:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm0,%zmm6
	vpshufb	%zmm29,%zmm3,%zmm7
	vpshufb	%zmm29,%zmm4,%zmm10
	vpshufb	%zmm29,%zmm5,%zmm11
	vextracti32x4	$3,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_lkrcpbsAuqCahtf:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	16(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lkrcpbsAuqCahtf:
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_lkrcpbsAuqCahtf:
.L_small_initial_blocks_encrypted_cticcGijxzhCayF:
.L_ghash_done_uizbwpfDujhiqBd:
	vmovdqu64	%xmm2,0(%rsi)
.L_enc_dec_done_uizbwpfDujhiqBd:

	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14
	vmovdqu64	%xmm14,64(%rsi)
.L_enc_dec_abort_uizbwpfDujhiqBd:
	jmp	.Lexit_gcm_encrypt
.Lexit_gcm_encrypt:
	cmpq	$256,%r8
	jbe	.Lskip_hkeys_cleanup_teewoBwvwnDyAhx
	vpxor	%xmm0,%xmm0,%xmm0
	vmovdqa64	%zmm0,0(%rsp)
	vmovdqa64	%zmm0,64(%rsp)
	vmovdqa64	%zmm0,128(%rsp)
	vmovdqa64	%zmm0,192(%rsp)
	vmovdqa64	%zmm0,256(%rsp)
	vmovdqa64	%zmm0,320(%rsp)
	vmovdqa64	%zmm0,384(%rsp)
	vmovdqa64	%zmm0,448(%rsp)
	vmovdqa64	%zmm0,512(%rsp)
	vmovdqa64	%zmm0,576(%rsp)
	vmovdqa64	%zmm0,640(%rsp)
	vmovdqa64	%zmm0,704(%rsp)
.Lskip_hkeys_cleanup_teewoBwvwnDyAhx:
	vzeroupper
	leaq	(%rbp),%rsp
.cfi_def_cfa_register	%rsp
	popq	%r15
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r15
	popq	%r14
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r14
	popq	%r13
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r13
	popq	%r12
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r12
	popq	%rbp
.cfi_adjust_cfa_offset	-8
.cfi_restore	%rbp
	popq	%rbx
.cfi_adjust_cfa_offset	-8
.cfi_restore	%rbx
	.byte	0xf3,0xc3
.Lencrypt_seh_end:
.cfi_endproc	
.size	aes_gcm_encrypt_avx512, .-aes_gcm_encrypt_avx512
.globl	aes_gcm_decrypt_avx512
.hidden aes_gcm_decrypt_avx512
.type	aes_gcm_decrypt_avx512,@function
.align	32
aes_gcm_decrypt_avx512:
.cfi_startproc	
.Ldecrypt_seh_begin:
.byte	243,15,30,250
	pushq	%rbx
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbx,-16
.Ldecrypt_seh_push_rbx:
	pushq	%rbp
.cfi_adjust_cfa_offset	8
.cfi_offset	%rbp,-24
.Ldecrypt_seh_push_rbp:
	pushq	%r12
.cfi_adjust_cfa_offset	8
.cfi_offset	%r12,-32
.Ldecrypt_seh_push_r12:
	pushq	%r13
.cfi_adjust_cfa_offset	8
.cfi_offset	%r13,-40
.Ldecrypt_seh_push_r13:
	pushq	%r14
.cfi_adjust_cfa_offset	8
.cfi_offset	%r14,-48
.Ldecrypt_seh_push_r14:
	pushq	%r15
.cfi_adjust_cfa_offset	8
.cfi_offset	%r15,-56
.Ldecrypt_seh_push_r15:










	leaq	0(%rsp),%rbp
.cfi_def_cfa_register	%rbp
.Ldecrypt_seh_setfp:

.Ldecrypt_seh_prolog_end:
	subq	$1588,%rsp
	andq	$(-64),%rsp


	movl	240(%rdi),%eax
	cmpl	$9,%eax
	je	.Laes_gcm_decrypt_128_avx512
	cmpl	$11,%eax
	je	.Laes_gcm_decrypt_192_avx512
	cmpl	$13,%eax
	je	.Laes_gcm_decrypt_256_avx512
	xorl	%eax,%eax
	jmp	.Lexit_gcm_decrypt
.align	32
.Laes_gcm_decrypt_128_avx512:
	orq	%r8,%r8
	je	.L_enc_dec_abort_isvfdtgkuyAliym
	xorq	%r14,%r14
	vmovdqu64	64(%rsi),%xmm14
	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14

	movl	(%rdx),%eax
	orq	%rax,%rax
	je	.L_partial_block_done_ybGmsttkraGoCCf
	movl	$16,%r10d
	leaq	byte_len_to_mask_table(%rip),%r12
	cmpq	%r10,%r8
	cmovcq	%r8,%r10
	kmovw	(%r12,%r10,2),%k1
	vmovdqu8	(%rcx),%xmm0{%k1}{z}

	vmovdqu64	16(%rsi),%xmm3

	leaq	96(%rsi),%r10
	vmovdqu64	240(%r10),%xmm4



	leaq	SHIFT_MASK(%rip),%r12
	addq	%rax,%r12
	vmovdqu64	(%r12),%xmm5
	vpshufb	%xmm5,%xmm3,%xmm3

	vmovdqa64	%xmm0,%xmm6
	vpxorq	%xmm0,%xmm3,%xmm3


	leaq	(%r8,%rax,1),%r13
	subq	$16,%r13
	jge	.L_no_extra_mask_ybGmsttkraGoCCf
	subq	%r13,%r12
.L_no_extra_mask_ybGmsttkraGoCCf:



	vmovdqu64	16(%r12),%xmm0
	vpand	%xmm0,%xmm3,%xmm3
	vpand	%xmm0,%xmm6,%xmm6
	vpshufb	SHUF_MASK(%rip),%xmm6,%xmm6
	vpshufb	%xmm5,%xmm6,%xmm6
	vpxorq	%xmm6,%xmm14,%xmm14
	cmpq	$0,%r13
	jl	.L_partial_incomplete_ybGmsttkraGoCCf

	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm14,%xmm14

	vpsrldq	$8,%xmm14,%xmm11
	vpslldq	$8,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm7,%xmm7
	vpxorq	%xmm10,%xmm14,%xmm14



	vmovdqu64	POLY2(%rip),%xmm11

	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
	vpslldq	$8,%xmm10,%xmm10
	vpxorq	%xmm10,%xmm14,%xmm14



	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
	vpsrldq	$4,%xmm10,%xmm10
	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
	vpslldq	$4,%xmm14,%xmm14

	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14

	movl	$0,(%rdx)

	movq	%rax,%r12
	movq	$16,%rax
	subq	%r12,%rax
	jmp	.L_enc_dec_done_ybGmsttkraGoCCf

.L_partial_incomplete_ybGmsttkraGoCCf:
	addl	%r8d,(%rdx)
	movq	%r8,%rax

.L_enc_dec_done_ybGmsttkraGoCCf:


	leaq	byte_len_to_mask_table(%rip),%r12
	kmovw	(%r12,%rax,2),%k1
	movq	%r9,%r12
	vmovdqu8	%xmm3,(%r12){%k1}
.L_partial_block_done_ybGmsttkraGoCCf:
	vmovdqu64	0(%rsi),%xmm2
	subq	%rax,%r8
	je	.L_enc_dec_done_isvfdtgkuyAliym
	cmpq	$256,%r8
	jbe	.L_message_below_equal_16_blocks_isvfdtgkuyAliym

	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
	vmovdqa64	ddq_addbe_1234(%rip),%zmm28






	vmovd	%xmm2,%r15d
	andl	$255,%r15d

	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpshufb	%zmm29,%zmm2,%zmm2



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_gAstiGGcvmitcmj
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_gAstiGGcvmitcmj
.L_next_16_overflow_gAstiGGcvmitcmj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_gAstiGGcvmitcmj:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	0(%rcx,%rax,1),%zmm0
	vmovdqu8	64(%rcx,%rax,1),%zmm3
	vmovdqu8	128(%rcx,%rax,1),%zmm4
	vmovdqu8	192(%rcx,%rax,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,0(%r10,%rax,1)
	vmovdqu8	%zmm10,64(%r10,%rax,1)
	vmovdqu8	%zmm11,128(%r10,%rax,1)
	vmovdqu8	%zmm12,192(%r10,%rax,1)

	vpshufb	%zmm29,%zmm0,%zmm7
	vpshufb	%zmm29,%zmm3,%zmm10
	vpshufb	%zmm29,%zmm4,%zmm11
	vpshufb	%zmm29,%zmm5,%zmm12
	vmovdqa64	%zmm7,768(%rsp)
	vmovdqa64	%zmm10,832(%rsp)
	vmovdqa64	%zmm11,896(%rsp)
	vmovdqa64	%zmm12,960(%rsp)
	leaq	96(%rsi),%r12
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_GFqtumeqrnynydz

	vmovdqu64	192(%r12),%zmm0
	vmovdqu64	%zmm0,704(%rsp)

	vmovdqu64	128(%r12),%zmm3
	vmovdqu64	%zmm3,640(%rsp)


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	64(%r12),%zmm4
	vmovdqu64	%zmm4,576(%rsp)

	vmovdqu64	0(%r12),%zmm5
	vmovdqu64	%zmm5,512(%rsp)
.L_skip_hkeys_precomputation_GFqtumeqrnynydz:
	cmpq	$512,%r8
	jb	.L_message_below_32_blocks_isvfdtgkuyAliym



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_GusFDygyqEEwwxE
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_GusFDygyqEEwwxE
.L_next_16_overflow_GusFDygyqEEwwxE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_GusFDygyqEEwwxE:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	256(%rcx,%rax,1),%zmm0
	vmovdqu8	320(%rcx,%rax,1),%zmm3
	vmovdqu8	384(%rcx,%rax,1),%zmm4
	vmovdqu8	448(%rcx,%rax,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,256(%r10,%rax,1)
	vmovdqu8	%zmm10,320(%r10,%rax,1)
	vmovdqu8	%zmm11,384(%r10,%rax,1)
	vmovdqu8	%zmm12,448(%r10,%rax,1)

	vpshufb	%zmm29,%zmm0,%zmm7
	vpshufb	%zmm29,%zmm3,%zmm10
	vpshufb	%zmm29,%zmm4,%zmm11
	vpshufb	%zmm29,%zmm5,%zmm12
	vmovdqa64	%zmm7,1024(%rsp)
	vmovdqa64	%zmm10,1088(%rsp)
	vmovdqa64	%zmm11,1152(%rsp)
	vmovdqa64	%zmm12,1216(%rsp)
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_EovkoeGADrppkwh
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,192(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,128(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,64(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,0(%rsp)
.L_skip_hkeys_precomputation_EovkoeGADrppkwh:
	movq	$1,%r14
	addq	$512,%rax
	subq	$512,%r8

	cmpq	$768,%r8
	jb	.L_no_more_big_nblocks_isvfdtgkuyAliym
.L_encrypt_big_nblocks_isvfdtgkuyAliym:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_fskyknklsarclzu
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_fskyknklsarclzu
.L_16_blocks_overflow_fskyknklsarclzu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_fskyknklsarclzu:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_lyAszAxpEEsjfhj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_lyAszAxpEEsjfhj
.L_16_blocks_overflow_lyAszAxpEEsjfhj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_lyAszAxpEEsjfhj:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%rax,1),%zmm17
	vmovdqu8	320(%rcx,%rax,1),%zmm19
	vmovdqu8	384(%rcx,%rax,1),%zmm20
	vmovdqu8	448(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%rax,1)
	vmovdqu8	%zmm3,320(%r10,%rax,1)
	vmovdqu8	%zmm4,384(%r10,%rax,1)
	vmovdqu8	%zmm5,448(%r10,%rax,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_FpdFrcEucrjmdFd
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_FpdFrcEucrjmdFd
.L_16_blocks_overflow_FpdFrcEucrjmdFd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_FpdFrcEucrjmdFd:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	512(%rcx,%rax,1),%zmm17
	vmovdqu8	576(%rcx,%rax,1),%zmm19
	vmovdqu8	640(%rcx,%rax,1),%zmm20
	vmovdqu8	704(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30


	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10

	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
	vpxorq	%zmm24,%zmm6,%zmm6
	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
	vpxorq	%zmm25,%zmm7,%zmm7
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vextracti64x4	$1,%zmm6,%ymm12
	vpxorq	%ymm12,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm12
	vpxorq	%xmm12,%xmm6,%xmm6
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,512(%r10,%rax,1)
	vmovdqu8	%zmm3,576(%r10,%rax,1)
	vmovdqu8	%zmm4,640(%r10,%rax,1)
	vmovdqu8	%zmm5,704(%r10,%rax,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1024(%rsp)
	vmovdqa64	%zmm3,1088(%rsp)
	vmovdqa64	%zmm4,1152(%rsp)
	vmovdqa64	%zmm5,1216(%rsp)
	vmovdqa64	%zmm6,%zmm14

	addq	$768,%rax
	subq	$768,%r8
	cmpq	$768,%r8
	jae	.L_encrypt_big_nblocks_isvfdtgkuyAliym

.L_no_more_big_nblocks_isvfdtgkuyAliym:

	cmpq	$512,%r8
	jae	.L_encrypt_32_blocks_isvfdtgkuyAliym

	cmpq	$256,%r8
	jae	.L_encrypt_16_blocks_isvfdtgkuyAliym
.L_encrypt_0_blocks_ghash_32_isvfdtgkuyAliym:
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$256,%ebx
	subl	%r10d,%ebx
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	addl	$256,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_wubglirCzgzbwCp

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_wubglirCzgzbwCp
	jb	.L_last_num_blocks_is_7_1_wubglirCzgzbwCp


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_wubglirCzgzbwCp
	jb	.L_last_num_blocks_is_11_9_wubglirCzgzbwCp


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_wubglirCzgzbwCp
	ja	.L_last_num_blocks_is_16_wubglirCzgzbwCp
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_wubglirCzgzbwCp
	jmp	.L_last_num_blocks_is_13_wubglirCzgzbwCp

.L_last_num_blocks_is_11_9_wubglirCzgzbwCp:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_wubglirCzgzbwCp
	ja	.L_last_num_blocks_is_11_wubglirCzgzbwCp
	jmp	.L_last_num_blocks_is_9_wubglirCzgzbwCp

.L_last_num_blocks_is_7_1_wubglirCzgzbwCp:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_wubglirCzgzbwCp
	jb	.L_last_num_blocks_is_3_1_wubglirCzgzbwCp

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_wubglirCzgzbwCp
	je	.L_last_num_blocks_is_6_wubglirCzgzbwCp
	jmp	.L_last_num_blocks_is_5_wubglirCzgzbwCp

.L_last_num_blocks_is_3_1_wubglirCzgzbwCp:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_wubglirCzgzbwCp
	je	.L_last_num_blocks_is_2_wubglirCzgzbwCp
.L_last_num_blocks_is_1_wubglirCzgzbwCp:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_npqngolqFfuqvjz
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_npqngolqFfuqvjz

.L_16_blocks_overflow_npqngolqFfuqvjz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_npqngolqFfuqvjz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_eErtuwCwbbDcjxd





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_eErtuwCwbbDcjxd
.L_small_initial_partial_block_eErtuwCwbbDcjxd:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_eErtuwCwbbDcjxd
.L_small_initial_compute_done_eErtuwCwbbDcjxd:
.L_after_reduction_eErtuwCwbbDcjxd:
	jmp	.L_last_blocks_done_wubglirCzgzbwCp
.L_last_num_blocks_is_2_wubglirCzgzbwCp:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_deoqoacbklhGyjC
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_deoqoacbklhGyjC

.L_16_blocks_overflow_deoqoacbklhGyjC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_deoqoacbklhGyjC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ogfCoypnwlmpFoF





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ogfCoypnwlmpFoF
.L_small_initial_partial_block_ogfCoypnwlmpFoF:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ogfCoypnwlmpFoF:

	orq	%r8,%r8
	je	.L_after_reduction_ogfCoypnwlmpFoF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ogfCoypnwlmpFoF:
	jmp	.L_last_blocks_done_wubglirCzgzbwCp
.L_last_num_blocks_is_3_wubglirCzgzbwCp:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_biqDzwCCFuiwmyc
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_biqDzwCCFuiwmyc

.L_16_blocks_overflow_biqDzwCCFuiwmyc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_biqDzwCCFuiwmyc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CEEBamcDvBcmnqD





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CEEBamcDvBcmnqD
.L_small_initial_partial_block_CEEBamcDvBcmnqD:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CEEBamcDvBcmnqD:

	orq	%r8,%r8
	je	.L_after_reduction_CEEBamcDvBcmnqD
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_CEEBamcDvBcmnqD:
	jmp	.L_last_blocks_done_wubglirCzgzbwCp
.L_last_num_blocks_is_4_wubglirCzgzbwCp:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_rfctErfguwAeFFC
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_rfctErfguwAeFFC

.L_16_blocks_overflow_rfctErfguwAeFFC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_rfctErfguwAeFFC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rzmDjwEbiifqnyh





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rzmDjwEbiifqnyh
.L_small_initial_partial_block_rzmDjwEbiifqnyh:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rzmDjwEbiifqnyh:

	orq	%r8,%r8
	je	.L_after_reduction_rzmDjwEbiifqnyh
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_rzmDjwEbiifqnyh:
	jmp	.L_last_blocks_done_wubglirCzgzbwCp
.L_last_num_blocks_is_5_wubglirCzgzbwCp:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_tlbDlFgkDgGwzcr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_tlbDlFgkDgGwzcr

.L_16_blocks_overflow_tlbDlFgkDgGwzcr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_tlbDlFgkDgGwzcr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_AxwrjnicwaasGpB





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_AxwrjnicwaasGpB
.L_small_initial_partial_block_AxwrjnicwaasGpB:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_AxwrjnicwaasGpB:

	orq	%r8,%r8
	je	.L_after_reduction_AxwrjnicwaasGpB
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_AxwrjnicwaasGpB:
	jmp	.L_last_blocks_done_wubglirCzgzbwCp
.L_last_num_blocks_is_6_wubglirCzgzbwCp:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_FEpfmEggwrwjumy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_FEpfmEggwrwjumy

.L_16_blocks_overflow_FEpfmEggwrwjumy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_FEpfmEggwrwjumy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_daqkCxhrguiccsm





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_daqkCxhrguiccsm
.L_small_initial_partial_block_daqkCxhrguiccsm:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_daqkCxhrguiccsm:

	orq	%r8,%r8
	je	.L_after_reduction_daqkCxhrguiccsm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_daqkCxhrguiccsm:
	jmp	.L_last_blocks_done_wubglirCzgzbwCp
.L_last_num_blocks_is_7_wubglirCzgzbwCp:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_DcvepCiCqqhjztt
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_DcvepCiCqqhjztt

.L_16_blocks_overflow_DcvepCiCqqhjztt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_DcvepCiCqqhjztt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rdCwkjndCCvcffq





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rdCwkjndCCvcffq
.L_small_initial_partial_block_rdCwkjndCCvcffq:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rdCwkjndCCvcffq:

	orq	%r8,%r8
	je	.L_after_reduction_rdCwkjndCCvcffq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_rdCwkjndCCvcffq:
	jmp	.L_last_blocks_done_wubglirCzgzbwCp
.L_last_num_blocks_is_8_wubglirCzgzbwCp:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_GckiuccibBaevll
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_GckiuccibBaevll

.L_16_blocks_overflow_GckiuccibBaevll:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_GckiuccibBaevll:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xpswCoviBruFlwB





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xpswCoviBruFlwB
.L_small_initial_partial_block_xpswCoviBruFlwB:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xpswCoviBruFlwB:

	orq	%r8,%r8
	je	.L_after_reduction_xpswCoviBruFlwB
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xpswCoviBruFlwB:
	jmp	.L_last_blocks_done_wubglirCzgzbwCp
.L_last_num_blocks_is_9_wubglirCzgzbwCp:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_yBCoehbdExnpjBi
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_yBCoehbdExnpjBi

.L_16_blocks_overflow_yBCoehbdExnpjBi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_yBCoehbdExnpjBi:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_onzzwBujDqtakab





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_onzzwBujDqtakab
.L_small_initial_partial_block_onzzwBujDqtakab:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_onzzwBujDqtakab:

	orq	%r8,%r8
	je	.L_after_reduction_onzzwBujDqtakab
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_onzzwBujDqtakab:
	jmp	.L_last_blocks_done_wubglirCzgzbwCp
.L_last_num_blocks_is_10_wubglirCzgzbwCp:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_FCntfsukyEvwwDf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_FCntfsukyEvwwDf

.L_16_blocks_overflow_FCntfsukyEvwwDf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_FCntfsukyEvwwDf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qGmfuogxFmlEGGr





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qGmfuogxFmlEGGr
.L_small_initial_partial_block_qGmfuogxFmlEGGr:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qGmfuogxFmlEGGr:

	orq	%r8,%r8
	je	.L_after_reduction_qGmfuogxFmlEGGr
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_qGmfuogxFmlEGGr:
	jmp	.L_last_blocks_done_wubglirCzgzbwCp
.L_last_num_blocks_is_11_wubglirCzgzbwCp:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_bnikvbayqtCloht
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_bnikvbayqtCloht

.L_16_blocks_overflow_bnikvbayqtCloht:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_bnikvbayqtCloht:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_huEuAkdiaCkpvsq





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_huEuAkdiaCkpvsq
.L_small_initial_partial_block_huEuAkdiaCkpvsq:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_huEuAkdiaCkpvsq:

	orq	%r8,%r8
	je	.L_after_reduction_huEuAkdiaCkpvsq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_huEuAkdiaCkpvsq:
	jmp	.L_last_blocks_done_wubglirCzgzbwCp
.L_last_num_blocks_is_12_wubglirCzgzbwCp:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_eFbjBAxicksgtrp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_eFbjBAxicksgtrp

.L_16_blocks_overflow_eFbjBAxicksgtrp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_eFbjBAxicksgtrp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_drroksxhrDksqak





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_drroksxhrDksqak
.L_small_initial_partial_block_drroksxhrDksqak:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_drroksxhrDksqak:

	orq	%r8,%r8
	je	.L_after_reduction_drroksxhrDksqak
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_drroksxhrDksqak:
	jmp	.L_last_blocks_done_wubglirCzgzbwCp
.L_last_num_blocks_is_13_wubglirCzgzbwCp:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_rdGywdnxnztjhrf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_rdGywdnxnztjhrf

.L_16_blocks_overflow_rdGywdnxnztjhrf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_rdGywdnxnztjhrf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_Dwkmwjezofbtwaj





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_Dwkmwjezofbtwaj
.L_small_initial_partial_block_Dwkmwjezofbtwaj:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_Dwkmwjezofbtwaj:

	orq	%r8,%r8
	je	.L_after_reduction_Dwkmwjezofbtwaj
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_Dwkmwjezofbtwaj:
	jmp	.L_last_blocks_done_wubglirCzgzbwCp
.L_last_num_blocks_is_14_wubglirCzgzbwCp:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_nAvFjokFshijpdE
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_nAvFjokFshijpdE

.L_16_blocks_overflow_nAvFjokFshijpdE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_nAvFjokFshijpdE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_tgtDmFvkDdyFglk





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_tgtDmFvkDdyFglk
.L_small_initial_partial_block_tgtDmFvkDdyFglk:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_tgtDmFvkDdyFglk:

	orq	%r8,%r8
	je	.L_after_reduction_tgtDmFvkDdyFglk
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_tgtDmFvkDdyFglk:
	jmp	.L_last_blocks_done_wubglirCzgzbwCp
.L_last_num_blocks_is_15_wubglirCzgzbwCp:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_DnGxhfifbgljtDh
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_DnGxhfifbgljtDh

.L_16_blocks_overflow_DnGxhfifbgljtDh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_DnGxhfifbgljtDh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_iuzCmFvyolxFdaz





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_iuzCmFvyolxFdaz
.L_small_initial_partial_block_iuzCmFvyolxFdaz:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_iuzCmFvyolxFdaz:

	orq	%r8,%r8
	je	.L_after_reduction_iuzCmFvyolxFdaz
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_iuzCmFvyolxFdaz:
	jmp	.L_last_blocks_done_wubglirCzgzbwCp
.L_last_num_blocks_is_16_wubglirCzgzbwCp:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_awapirselgonCtG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_awapirselgonCtG

.L_16_blocks_overflow_awapirselgonCtG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_awapirselgonCtG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_vAhGDvmmqmDwpkg:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_vAhGDvmmqmDwpkg:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_vAhGDvmmqmDwpkg:
	jmp	.L_last_blocks_done_wubglirCzgzbwCp
.L_last_num_blocks_is_0_wubglirCzgzbwCp:
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_wubglirCzgzbwCp:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_isvfdtgkuyAliym
.L_encrypt_32_blocks_isvfdtgkuyAliym:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_yjxcepyjGribhqc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_yjxcepyjGribhqc
.L_16_blocks_overflow_yjxcepyjGribhqc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_yjxcepyjGribhqc:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_pnmiFxgibctowll
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_pnmiFxgibctowll
.L_16_blocks_overflow_pnmiFxgibctowll:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_pnmiFxgibctowll:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%rax,1),%zmm17
	vmovdqu8	320(%rcx,%rax,1),%zmm19
	vmovdqu8	384(%rcx,%rax,1),%zmm20
	vmovdqu8	448(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%rax,1)
	vmovdqu8	%zmm3,320(%r10,%rax,1)
	vmovdqu8	%zmm4,384(%r10,%rax,1)
	vmovdqu8	%zmm5,448(%r10,%rax,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

	subq	$512,%r8
	addq	$512,%rax
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_dqmfiDGCfbfcGzq

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_dqmfiDGCfbfcGzq
	jb	.L_last_num_blocks_is_7_1_dqmfiDGCfbfcGzq


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_dqmfiDGCfbfcGzq
	jb	.L_last_num_blocks_is_11_9_dqmfiDGCfbfcGzq


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_dqmfiDGCfbfcGzq
	ja	.L_last_num_blocks_is_16_dqmfiDGCfbfcGzq
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_dqmfiDGCfbfcGzq
	jmp	.L_last_num_blocks_is_13_dqmfiDGCfbfcGzq

.L_last_num_blocks_is_11_9_dqmfiDGCfbfcGzq:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_dqmfiDGCfbfcGzq
	ja	.L_last_num_blocks_is_11_dqmfiDGCfbfcGzq
	jmp	.L_last_num_blocks_is_9_dqmfiDGCfbfcGzq

.L_last_num_blocks_is_7_1_dqmfiDGCfbfcGzq:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_dqmfiDGCfbfcGzq
	jb	.L_last_num_blocks_is_3_1_dqmfiDGCfbfcGzq

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_dqmfiDGCfbfcGzq
	je	.L_last_num_blocks_is_6_dqmfiDGCfbfcGzq
	jmp	.L_last_num_blocks_is_5_dqmfiDGCfbfcGzq

.L_last_num_blocks_is_3_1_dqmfiDGCfbfcGzq:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_dqmfiDGCfbfcGzq
	je	.L_last_num_blocks_is_2_dqmfiDGCfbfcGzq
.L_last_num_blocks_is_1_dqmfiDGCfbfcGzq:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_dqvGapyxFnGgdFm
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_dqvGapyxFnGgdFm

.L_16_blocks_overflow_dqvGapyxFnGgdFm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_dqvGapyxFnGgdFm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_aGyjFpAAplikcgp





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_aGyjFpAAplikcgp
.L_small_initial_partial_block_aGyjFpAAplikcgp:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_aGyjFpAAplikcgp
.L_small_initial_compute_done_aGyjFpAAplikcgp:
.L_after_reduction_aGyjFpAAplikcgp:
	jmp	.L_last_blocks_done_dqmfiDGCfbfcGzq
.L_last_num_blocks_is_2_dqmfiDGCfbfcGzq:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_xfohfkbqDbikctE
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_xfohfkbqDbikctE

.L_16_blocks_overflow_xfohfkbqDbikctE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_xfohfkbqDbikctE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_kzihhrDevlGlxGo





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_kzihhrDevlGlxGo
.L_small_initial_partial_block_kzihhrDevlGlxGo:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_kzihhrDevlGlxGo:

	orq	%r8,%r8
	je	.L_after_reduction_kzihhrDevlGlxGo
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_kzihhrDevlGlxGo:
	jmp	.L_last_blocks_done_dqmfiDGCfbfcGzq
.L_last_num_blocks_is_3_dqmfiDGCfbfcGzq:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_ajwtezCssgsfsFr
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_ajwtezCssgsfsFr

.L_16_blocks_overflow_ajwtezCssgsfsFr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_ajwtezCssgsfsFr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_azyivhmgDGetadd





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_azyivhmgDGetadd
.L_small_initial_partial_block_azyivhmgDGetadd:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_azyivhmgDGetadd:

	orq	%r8,%r8
	je	.L_after_reduction_azyivhmgDGetadd
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_azyivhmgDGetadd:
	jmp	.L_last_blocks_done_dqmfiDGCfbfcGzq
.L_last_num_blocks_is_4_dqmfiDGCfbfcGzq:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_mpsxmzvhiFoljug
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_mpsxmzvhiFoljug

.L_16_blocks_overflow_mpsxmzvhiFoljug:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_mpsxmzvhiFoljug:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_Bcckgjgltybcwpz





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_Bcckgjgltybcwpz
.L_small_initial_partial_block_Bcckgjgltybcwpz:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_Bcckgjgltybcwpz:

	orq	%r8,%r8
	je	.L_after_reduction_Bcckgjgltybcwpz
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_Bcckgjgltybcwpz:
	jmp	.L_last_blocks_done_dqmfiDGCfbfcGzq
.L_last_num_blocks_is_5_dqmfiDGCfbfcGzq:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_trFsganrCkFabEb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_trFsganrCkFabEb

.L_16_blocks_overflow_trFsganrCkFabEb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_trFsganrCkFabEb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dCAbEhcGmgzwoaG





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dCAbEhcGmgzwoaG
.L_small_initial_partial_block_dCAbEhcGmgzwoaG:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dCAbEhcGmgzwoaG:

	orq	%r8,%r8
	je	.L_after_reduction_dCAbEhcGmgzwoaG
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dCAbEhcGmgzwoaG:
	jmp	.L_last_blocks_done_dqmfiDGCfbfcGzq
.L_last_num_blocks_is_6_dqmfiDGCfbfcGzq:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_AivuDziEkhCwkod
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_AivuDziEkhCwkod

.L_16_blocks_overflow_AivuDziEkhCwkod:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_AivuDziEkhCwkod:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ftgCcDwpEbzAvoe





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ftgCcDwpEbzAvoe
.L_small_initial_partial_block_ftgCcDwpEbzAvoe:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ftgCcDwpEbzAvoe:

	orq	%r8,%r8
	je	.L_after_reduction_ftgCcDwpEbzAvoe
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ftgCcDwpEbzAvoe:
	jmp	.L_last_blocks_done_dqmfiDGCfbfcGzq
.L_last_num_blocks_is_7_dqmfiDGCfbfcGzq:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_ppbcocppEyqAeeB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_ppbcocppEyqAeeB

.L_16_blocks_overflow_ppbcocppEyqAeeB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_ppbcocppEyqAeeB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_lctybntjwbjpgtp





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_lctybntjwbjpgtp
.L_small_initial_partial_block_lctybntjwbjpgtp:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lctybntjwbjpgtp:

	orq	%r8,%r8
	je	.L_after_reduction_lctybntjwbjpgtp
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_lctybntjwbjpgtp:
	jmp	.L_last_blocks_done_dqmfiDGCfbfcGzq
.L_last_num_blocks_is_8_dqmfiDGCfbfcGzq:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_gpwvaECngdkbcyb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_gpwvaECngdkbcyb

.L_16_blocks_overflow_gpwvaECngdkbcyb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_gpwvaECngdkbcyb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cwwBBrFvnxgfoxn





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cwwBBrFvnxgfoxn
.L_small_initial_partial_block_cwwBBrFvnxgfoxn:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cwwBBrFvnxgfoxn:

	orq	%r8,%r8
	je	.L_after_reduction_cwwBBrFvnxgfoxn
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cwwBBrFvnxgfoxn:
	jmp	.L_last_blocks_done_dqmfiDGCfbfcGzq
.L_last_num_blocks_is_9_dqmfiDGCfbfcGzq:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_vDnudAbziyFpAqj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_vDnudAbziyFpAqj

.L_16_blocks_overflow_vDnudAbziyFpAqj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_vDnudAbziyFpAqj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_FqBCnvoqgFrBiil





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_FqBCnvoqgFrBiil
.L_small_initial_partial_block_FqBCnvoqgFrBiil:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_FqBCnvoqgFrBiil:

	orq	%r8,%r8
	je	.L_after_reduction_FqBCnvoqgFrBiil
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_FqBCnvoqgFrBiil:
	jmp	.L_last_blocks_done_dqmfiDGCfbfcGzq
.L_last_num_blocks_is_10_dqmfiDGCfbfcGzq:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_ynAoBBfDezjdxss
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_ynAoBBfDezjdxss

.L_16_blocks_overflow_ynAoBBfDezjdxss:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_ynAoBBfDezjdxss:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_fjlFqxirsdeuthD





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_fjlFqxirsdeuthD
.L_small_initial_partial_block_fjlFqxirsdeuthD:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_fjlFqxirsdeuthD:

	orq	%r8,%r8
	je	.L_after_reduction_fjlFqxirsdeuthD
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_fjlFqxirsdeuthD:
	jmp	.L_last_blocks_done_dqmfiDGCfbfcGzq
.L_last_num_blocks_is_11_dqmfiDGCfbfcGzq:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_DnnfBbqtdiBuAlf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_DnnfBbqtdiBuAlf

.L_16_blocks_overflow_DnnfBbqtdiBuAlf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_DnnfBbqtdiBuAlf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bjuncwfGhGpwcFd





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bjuncwfGhGpwcFd
.L_small_initial_partial_block_bjuncwfGhGpwcFd:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bjuncwfGhGpwcFd:

	orq	%r8,%r8
	je	.L_after_reduction_bjuncwfGhGpwcFd
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_bjuncwfGhGpwcFd:
	jmp	.L_last_blocks_done_dqmfiDGCfbfcGzq
.L_last_num_blocks_is_12_dqmfiDGCfbfcGzq:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_xoxjxaCbbsFCxEe
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_xoxjxaCbbsFCxEe

.L_16_blocks_overflow_xoxjxaCbbsFCxEe:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_xoxjxaCbbsFCxEe:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jmyhbparFgjaccf





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jmyhbparFgjaccf
.L_small_initial_partial_block_jmyhbparFgjaccf:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jmyhbparFgjaccf:

	orq	%r8,%r8
	je	.L_after_reduction_jmyhbparFgjaccf
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jmyhbparFgjaccf:
	jmp	.L_last_blocks_done_dqmfiDGCfbfcGzq
.L_last_num_blocks_is_13_dqmfiDGCfbfcGzq:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_FzjfkDstnsdtrqx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_FzjfkDstnsdtrqx

.L_16_blocks_overflow_FzjfkDstnsdtrqx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_FzjfkDstnsdtrqx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_FhsAimyFoqwsqrf





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_FhsAimyFoqwsqrf
.L_small_initial_partial_block_FhsAimyFoqwsqrf:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_FhsAimyFoqwsqrf:

	orq	%r8,%r8
	je	.L_after_reduction_FhsAimyFoqwsqrf
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_FhsAimyFoqwsqrf:
	jmp	.L_last_blocks_done_dqmfiDGCfbfcGzq
.L_last_num_blocks_is_14_dqmfiDGCfbfcGzq:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_ucbFzagiggEaehs
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_ucbFzagiggEaehs

.L_16_blocks_overflow_ucbFzagiggEaehs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_ucbFzagiggEaehs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cmkiyspCycddwhv





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cmkiyspCycddwhv
.L_small_initial_partial_block_cmkiyspCycddwhv:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cmkiyspCycddwhv:

	orq	%r8,%r8
	je	.L_after_reduction_cmkiyspCycddwhv
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cmkiyspCycddwhv:
	jmp	.L_last_blocks_done_dqmfiDGCfbfcGzq
.L_last_num_blocks_is_15_dqmfiDGCfbfcGzq:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_fBvDxkwErogEFhF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_fBvDxkwErogEFhF

.L_16_blocks_overflow_fBvDxkwErogEFhF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_fBvDxkwErogEFhF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_sberagFjGxbvtxD





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_sberagFjGxbvtxD
.L_small_initial_partial_block_sberagFjGxbvtxD:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_sberagFjGxbvtxD:

	orq	%r8,%r8
	je	.L_after_reduction_sberagFjGxbvtxD
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_sberagFjGxbvtxD:
	jmp	.L_last_blocks_done_dqmfiDGCfbfcGzq
.L_last_num_blocks_is_16_dqmfiDGCfbfcGzq:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_gtjprnEGEBcoidx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_gtjprnEGEBcoidx

.L_16_blocks_overflow_gtjprnEGEBcoidx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_gtjprnEGEBcoidx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_xiqltGosGBCqsFE:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xiqltGosGBCqsFE:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xiqltGosGBCqsFE:
	jmp	.L_last_blocks_done_dqmfiDGCfbfcGzq
.L_last_num_blocks_is_0_dqmfiDGCfbfcGzq:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_dqmfiDGCfbfcGzq:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_isvfdtgkuyAliym
.L_encrypt_16_blocks_isvfdtgkuyAliym:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_kzhmqzidvmbsEhk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_kzhmqzidvmbsEhk
.L_16_blocks_overflow_kzhmqzidvmbsEhk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_kzhmqzidvmbsEhk:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	256(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	320(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	384(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	448(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	subq	$256,%r8
	addq	$256,%rax
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_sliigFnyaqBDzzF

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_sliigFnyaqBDzzF
	jb	.L_last_num_blocks_is_7_1_sliigFnyaqBDzzF


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_sliigFnyaqBDzzF
	jb	.L_last_num_blocks_is_11_9_sliigFnyaqBDzzF


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_sliigFnyaqBDzzF
	ja	.L_last_num_blocks_is_16_sliigFnyaqBDzzF
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_sliigFnyaqBDzzF
	jmp	.L_last_num_blocks_is_13_sliigFnyaqBDzzF

.L_last_num_blocks_is_11_9_sliigFnyaqBDzzF:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_sliigFnyaqBDzzF
	ja	.L_last_num_blocks_is_11_sliigFnyaqBDzzF
	jmp	.L_last_num_blocks_is_9_sliigFnyaqBDzzF

.L_last_num_blocks_is_7_1_sliigFnyaqBDzzF:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_sliigFnyaqBDzzF
	jb	.L_last_num_blocks_is_3_1_sliigFnyaqBDzzF

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_sliigFnyaqBDzzF
	je	.L_last_num_blocks_is_6_sliigFnyaqBDzzF
	jmp	.L_last_num_blocks_is_5_sliigFnyaqBDzzF

.L_last_num_blocks_is_3_1_sliigFnyaqBDzzF:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_sliigFnyaqBDzzF
	je	.L_last_num_blocks_is_2_sliigFnyaqBDzzF
.L_last_num_blocks_is_1_sliigFnyaqBDzzF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_ltopAqmDphiDGmg
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_ltopAqmDphiDGmg

.L_16_blocks_overflow_ltopAqmDphiDGmg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_ltopAqmDphiDGmg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%xmm31,%xmm0,%xmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vycrEwfvwmjxCky





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vycrEwfvwmjxCky
.L_small_initial_partial_block_vycrEwfvwmjxCky:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)











	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_vycrEwfvwmjxCky
.L_small_initial_compute_done_vycrEwfvwmjxCky:
.L_after_reduction_vycrEwfvwmjxCky:
	jmp	.L_last_blocks_done_sliigFnyaqBDzzF
.L_last_num_blocks_is_2_sliigFnyaqBDzzF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_GhDvucapvGCosdl
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_GhDvucapvGCosdl

.L_16_blocks_overflow_GhDvucapvGCosdl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_GhDvucapvGCosdl:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%ymm31,%ymm0,%ymm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dGtuxAmEnorsaBF





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dGtuxAmEnorsaBF
.L_small_initial_partial_block_dGtuxAmEnorsaBF:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dGtuxAmEnorsaBF:

	orq	%r8,%r8
	je	.L_after_reduction_dGtuxAmEnorsaBF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dGtuxAmEnorsaBF:
	jmp	.L_last_blocks_done_sliigFnyaqBDzzF
.L_last_num_blocks_is_3_sliigFnyaqBDzzF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_hroFzrxzfhzfrah
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_hroFzrxzfhzfrah

.L_16_blocks_overflow_hroFzrxzfhzfrah:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_hroFzrxzfhzfrah:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_BlpByfxiqggcrEt





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_BlpByfxiqggcrEt
.L_small_initial_partial_block_BlpByfxiqggcrEt:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_BlpByfxiqggcrEt:

	orq	%r8,%r8
	je	.L_after_reduction_BlpByfxiqggcrEt
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_BlpByfxiqggcrEt:
	jmp	.L_last_blocks_done_sliigFnyaqBDzzF
.L_last_num_blocks_is_4_sliigFnyaqBDzzF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_CyecwmsrGktBqDg
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_CyecwmsrGktBqDg

.L_16_blocks_overflow_CyecwmsrGktBqDg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_CyecwmsrGktBqDg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_itBjqvDmoEkhcaE





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_itBjqvDmoEkhcaE
.L_small_initial_partial_block_itBjqvDmoEkhcaE:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_itBjqvDmoEkhcaE:

	orq	%r8,%r8
	je	.L_after_reduction_itBjqvDmoEkhcaE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_itBjqvDmoEkhcaE:
	jmp	.L_last_blocks_done_sliigFnyaqBDzzF
.L_last_num_blocks_is_5_sliigFnyaqBDzzF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_lswztEzoqjeqkkn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_lswztEzoqjeqkkn

.L_16_blocks_overflow_lswztEzoqjeqkkn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_lswztEzoqjeqkkn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zrmChrjdyuwuozg





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zrmChrjdyuwuozg
.L_small_initial_partial_block_zrmChrjdyuwuozg:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zrmChrjdyuwuozg:

	orq	%r8,%r8
	je	.L_after_reduction_zrmChrjdyuwuozg
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_zrmChrjdyuwuozg:
	jmp	.L_last_blocks_done_sliigFnyaqBDzzF
.L_last_num_blocks_is_6_sliigFnyaqBDzzF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_hidiDscykfsbysv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_hidiDscykfsbysv

.L_16_blocks_overflow_hidiDscykfsbysv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_hidiDscykfsbysv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_odvqaEveguiuxkr





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_odvqaEveguiuxkr
.L_small_initial_partial_block_odvqaEveguiuxkr:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_odvqaEveguiuxkr:

	orq	%r8,%r8
	je	.L_after_reduction_odvqaEveguiuxkr
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_odvqaEveguiuxkr:
	jmp	.L_last_blocks_done_sliigFnyaqBDzzF
.L_last_num_blocks_is_7_sliigFnyaqBDzzF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_uejGtBwtcwaijqG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_uejGtBwtcwaijqG

.L_16_blocks_overflow_uejGtBwtcwaijqG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_uejGtBwtcwaijqG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DvzocvBdbbvsshp





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DvzocvBdbbvsshp
.L_small_initial_partial_block_DvzocvBdbbvsshp:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DvzocvBdbbvsshp:

	orq	%r8,%r8
	je	.L_after_reduction_DvzocvBdbbvsshp
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DvzocvBdbbvsshp:
	jmp	.L_last_blocks_done_sliigFnyaqBDzzF
.L_last_num_blocks_is_8_sliigFnyaqBDzzF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_alsankggBlwzfgB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_alsankggBlwzfgB

.L_16_blocks_overflow_alsankggBlwzfgB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_alsankggBlwzfgB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ywyyzmroBCizefd





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ywyyzmroBCizefd
.L_small_initial_partial_block_ywyyzmroBCizefd:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ywyyzmroBCizefd:

	orq	%r8,%r8
	je	.L_after_reduction_ywyyzmroBCizefd
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ywyyzmroBCizefd:
	jmp	.L_last_blocks_done_sliigFnyaqBDzzF
.L_last_num_blocks_is_9_sliigFnyaqBDzzF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_vEicxCBapfbnolz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_vEicxCBapfbnolz

.L_16_blocks_overflow_vEicxCBapfbnolz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_vEicxCBapfbnolz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_foBqDFbCnrFtfxn





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_foBqDFbCnrFtfxn
.L_small_initial_partial_block_foBqDFbCnrFtfxn:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_foBqDFbCnrFtfxn:

	orq	%r8,%r8
	je	.L_after_reduction_foBqDFbCnrFtfxn
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_foBqDFbCnrFtfxn:
	jmp	.L_last_blocks_done_sliigFnyaqBDzzF
.L_last_num_blocks_is_10_sliigFnyaqBDzzF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_ECtkcCGuEzqdkcm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_ECtkcCGuEzqdkcm

.L_16_blocks_overflow_ECtkcCGuEzqdkcm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_ECtkcCGuEzqdkcm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_onBiduuousbggBC





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_onBiduuousbggBC
.L_small_initial_partial_block_onBiduuousbggBC:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_onBiduuousbggBC:

	orq	%r8,%r8
	je	.L_after_reduction_onBiduuousbggBC
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_onBiduuousbggBC:
	jmp	.L_last_blocks_done_sliigFnyaqBDzzF
.L_last_num_blocks_is_11_sliigFnyaqBDzzF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_GAdputtDxiwhpkn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_GAdputtDxiwhpkn

.L_16_blocks_overflow_GAdputtDxiwhpkn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_GAdputtDxiwhpkn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rvdwwvqouypgtrE





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rvdwwvqouypgtrE
.L_small_initial_partial_block_rvdwwvqouypgtrE:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rvdwwvqouypgtrE:

	orq	%r8,%r8
	je	.L_after_reduction_rvdwwvqouypgtrE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_rvdwwvqouypgtrE:
	jmp	.L_last_blocks_done_sliigFnyaqBDzzF
.L_last_num_blocks_is_12_sliigFnyaqBDzzF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_ebDgiivyGsnlhnw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_ebDgiivyGsnlhnw

.L_16_blocks_overflow_ebDgiivyGsnlhnw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_ebDgiivyGsnlhnw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EupbEyotsDEFFgz





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EupbEyotsDEFFgz
.L_small_initial_partial_block_EupbEyotsDEFFgz:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_EupbEyotsDEFFgz:

	orq	%r8,%r8
	je	.L_after_reduction_EupbEyotsDEFFgz
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_EupbEyotsDEFFgz:
	jmp	.L_last_blocks_done_sliigFnyaqBDzzF
.L_last_num_blocks_is_13_sliigFnyaqBDzzF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_DaFvFsByqjfhEFq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_DaFvFsByqjfhEFq

.L_16_blocks_overflow_DaFvFsByqjfhEFq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_DaFvFsByqjfhEFq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_isvuandcaDbzfax





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_isvuandcaDbzfax
.L_small_initial_partial_block_isvuandcaDbzfax:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_isvuandcaDbzfax:

	orq	%r8,%r8
	je	.L_after_reduction_isvuandcaDbzfax
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_isvuandcaDbzfax:
	jmp	.L_last_blocks_done_sliigFnyaqBDzzF
.L_last_num_blocks_is_14_sliigFnyaqBDzzF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_vugakwlqkrxDybs
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_vugakwlqkrxDybs

.L_16_blocks_overflow_vugakwlqkrxDybs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_vugakwlqkrxDybs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dznhfukwcninDow





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dznhfukwcninDow
.L_small_initial_partial_block_dznhfukwcninDow:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dznhfukwcninDow:

	orq	%r8,%r8
	je	.L_after_reduction_dznhfukwcninDow
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dznhfukwcninDow:
	jmp	.L_last_blocks_done_sliigFnyaqBDzzF
.L_last_num_blocks_is_15_sliigFnyaqBDzzF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_AfoEiawgekGAjBG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_AfoEiawgekGAjBG

.L_16_blocks_overflow_AfoEiawgekGAjBG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_AfoEiawgekGAjBG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ksguBabkqFwatAi





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ksguBabkqFwatAi
.L_small_initial_partial_block_ksguBabkqFwatAi:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ksguBabkqFwatAi:

	orq	%r8,%r8
	je	.L_after_reduction_ksguBabkqFwatAi
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ksguBabkqFwatAi:
	jmp	.L_last_blocks_done_sliigFnyaqBDzzF
.L_last_num_blocks_is_16_sliigFnyaqBDzzF:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_AxdyjyewgxGCamA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_AxdyjyewgxGCamA

.L_16_blocks_overflow_AxdyjyewgxGCamA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_AxdyjyewgxGCamA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_tyfvhvFouoakseh:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_tyfvhvFouoakseh:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_tyfvhvFouoakseh:
	jmp	.L_last_blocks_done_sliigFnyaqBDzzF
.L_last_num_blocks_is_0_sliigFnyaqBDzzF:
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_sliigFnyaqBDzzF:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_isvfdtgkuyAliym

.L_message_below_32_blocks_isvfdtgkuyAliym:


	subq	$256,%r8
	addq	$256,%rax
	movl	%r8d,%r10d
	leaq	96(%rsi),%r12
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_wayDAsjxqcwmija
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)
.L_skip_hkeys_precomputation_wayDAsjxqcwmija:
	movq	$1,%r14
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_enwimDbxumidpjj

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_enwimDbxumidpjj
	jb	.L_last_num_blocks_is_7_1_enwimDbxumidpjj


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_enwimDbxumidpjj
	jb	.L_last_num_blocks_is_11_9_enwimDbxumidpjj


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_enwimDbxumidpjj
	ja	.L_last_num_blocks_is_16_enwimDbxumidpjj
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_enwimDbxumidpjj
	jmp	.L_last_num_blocks_is_13_enwimDbxumidpjj

.L_last_num_blocks_is_11_9_enwimDbxumidpjj:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_enwimDbxumidpjj
	ja	.L_last_num_blocks_is_11_enwimDbxumidpjj
	jmp	.L_last_num_blocks_is_9_enwimDbxumidpjj

.L_last_num_blocks_is_7_1_enwimDbxumidpjj:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_enwimDbxumidpjj
	jb	.L_last_num_blocks_is_3_1_enwimDbxumidpjj

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_enwimDbxumidpjj
	je	.L_last_num_blocks_is_6_enwimDbxumidpjj
	jmp	.L_last_num_blocks_is_5_enwimDbxumidpjj

.L_last_num_blocks_is_3_1_enwimDbxumidpjj:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_enwimDbxumidpjj
	je	.L_last_num_blocks_is_2_enwimDbxumidpjj
.L_last_num_blocks_is_1_enwimDbxumidpjj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_tncqntwCwpFroEc
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_tncqntwCwpFroEc

.L_16_blocks_overflow_tncqntwCwpFroEc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_tncqntwCwpFroEc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_kjhegsysiAqnoyd





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_kjhegsysiAqnoyd
.L_small_initial_partial_block_kjhegsysiAqnoyd:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_kjhegsysiAqnoyd
.L_small_initial_compute_done_kjhegsysiAqnoyd:
.L_after_reduction_kjhegsysiAqnoyd:
	jmp	.L_last_blocks_done_enwimDbxumidpjj
.L_last_num_blocks_is_2_enwimDbxumidpjj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_oFwfjumonGkophe
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_oFwfjumonGkophe

.L_16_blocks_overflow_oFwfjumonGkophe:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_oFwfjumonGkophe:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jrkfnlxexqcllvs





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jrkfnlxexqcllvs
.L_small_initial_partial_block_jrkfnlxexqcllvs:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jrkfnlxexqcllvs:

	orq	%r8,%r8
	je	.L_after_reduction_jrkfnlxexqcllvs
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jrkfnlxexqcllvs:
	jmp	.L_last_blocks_done_enwimDbxumidpjj
.L_last_num_blocks_is_3_enwimDbxumidpjj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_sonfmFggsFjGzht
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_sonfmFggsFjGzht

.L_16_blocks_overflow_sonfmFggsFjGzht:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_sonfmFggsFjGzht:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EsCzgilbbiiidox





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EsCzgilbbiiidox
.L_small_initial_partial_block_EsCzgilbbiiidox:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_EsCzgilbbiiidox:

	orq	%r8,%r8
	je	.L_after_reduction_EsCzgilbbiiidox
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_EsCzgilbbiiidox:
	jmp	.L_last_blocks_done_enwimDbxumidpjj
.L_last_num_blocks_is_4_enwimDbxumidpjj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_oiCsirohCryhgCw
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_oiCsirohCryhgCw

.L_16_blocks_overflow_oiCsirohCryhgCw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_oiCsirohCryhgCw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jbqftmGyeyeGjBE





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jbqftmGyeyeGjBE
.L_small_initial_partial_block_jbqftmGyeyeGjBE:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jbqftmGyeyeGjBE:

	orq	%r8,%r8
	je	.L_after_reduction_jbqftmGyeyeGjBE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jbqftmGyeyeGjBE:
	jmp	.L_last_blocks_done_enwimDbxumidpjj
.L_last_num_blocks_is_5_enwimDbxumidpjj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_hmyuGxFxikrsfra
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_hmyuGxFxikrsfra

.L_16_blocks_overflow_hmyuGxFxikrsfra:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_hmyuGxFxikrsfra:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_kvGBpvxudrlEGpc





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_kvGBpvxudrlEGpc
.L_small_initial_partial_block_kvGBpvxudrlEGpc:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_kvGBpvxudrlEGpc:

	orq	%r8,%r8
	je	.L_after_reduction_kvGBpvxudrlEGpc
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_kvGBpvxudrlEGpc:
	jmp	.L_last_blocks_done_enwimDbxumidpjj
.L_last_num_blocks_is_6_enwimDbxumidpjj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_nopppzAwrDkwFFy
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_nopppzAwrDkwFFy

.L_16_blocks_overflow_nopppzAwrDkwFFy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_nopppzAwrDkwFFy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_pskgaClmdGnqzlv





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_pskgaClmdGnqzlv
.L_small_initial_partial_block_pskgaClmdGnqzlv:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_pskgaClmdGnqzlv:

	orq	%r8,%r8
	je	.L_after_reduction_pskgaClmdGnqzlv
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_pskgaClmdGnqzlv:
	jmp	.L_last_blocks_done_enwimDbxumidpjj
.L_last_num_blocks_is_7_enwimDbxumidpjj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_FlenbfiqFddApln
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_FlenbfiqFddApln

.L_16_blocks_overflow_FlenbfiqFddApln:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_FlenbfiqFddApln:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xasdcgibcErrdvs





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xasdcgibcErrdvs
.L_small_initial_partial_block_xasdcgibcErrdvs:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xasdcgibcErrdvs:

	orq	%r8,%r8
	je	.L_after_reduction_xasdcgibcErrdvs
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xasdcgibcErrdvs:
	jmp	.L_last_blocks_done_enwimDbxumidpjj
.L_last_num_blocks_is_8_enwimDbxumidpjj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_tggspmtbgjxAseC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_tggspmtbgjxAseC

.L_16_blocks_overflow_tggspmtbgjxAseC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_tggspmtbgjxAseC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vocdpCzzgafbqsb





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vocdpCzzgafbqsb
.L_small_initial_partial_block_vocdpCzzgafbqsb:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_vocdpCzzgafbqsb:

	orq	%r8,%r8
	je	.L_after_reduction_vocdpCzzgafbqsb
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_vocdpCzzgafbqsb:
	jmp	.L_last_blocks_done_enwimDbxumidpjj
.L_last_num_blocks_is_9_enwimDbxumidpjj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_jDuDsnlEnfldses
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_jDuDsnlEnfldses

.L_16_blocks_overflow_jDuDsnlEnfldses:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_jDuDsnlEnfldses:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_sxjzxfbpylkhacx





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_sxjzxfbpylkhacx
.L_small_initial_partial_block_sxjzxfbpylkhacx:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_sxjzxfbpylkhacx:

	orq	%r8,%r8
	je	.L_after_reduction_sxjzxfbpylkhacx
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_sxjzxfbpylkhacx:
	jmp	.L_last_blocks_done_enwimDbxumidpjj
.L_last_num_blocks_is_10_enwimDbxumidpjj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_ajdfAhrCAlgxCCl
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_ajdfAhrCAlgxCCl

.L_16_blocks_overflow_ajdfAhrCAlgxCCl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_ajdfAhrCAlgxCCl:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_aocwpGbAoqqekpF





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_aocwpGbAoqqekpF
.L_small_initial_partial_block_aocwpGbAoqqekpF:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_aocwpGbAoqqekpF:

	orq	%r8,%r8
	je	.L_after_reduction_aocwpGbAoqqekpF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_aocwpGbAoqqekpF:
	jmp	.L_last_blocks_done_enwimDbxumidpjj
.L_last_num_blocks_is_11_enwimDbxumidpjj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_appzgxngynuxeha
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_appzgxngynuxeha

.L_16_blocks_overflow_appzgxngynuxeha:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_appzgxngynuxeha:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xhlbeDuDjkmyfbs





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xhlbeDuDjkmyfbs
.L_small_initial_partial_block_xhlbeDuDjkmyfbs:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xhlbeDuDjkmyfbs:

	orq	%r8,%r8
	je	.L_after_reduction_xhlbeDuDjkmyfbs
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xhlbeDuDjkmyfbs:
	jmp	.L_last_blocks_done_enwimDbxumidpjj
.L_last_num_blocks_is_12_enwimDbxumidpjj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_jlnrqoiednzuggD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_jlnrqoiednzuggD

.L_16_blocks_overflow_jlnrqoiednzuggD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_jlnrqoiednzuggD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_tAsdGmBbbeetssv





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_tAsdGmBbbeetssv
.L_small_initial_partial_block_tAsdGmBbbeetssv:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_tAsdGmBbbeetssv:

	orq	%r8,%r8
	je	.L_after_reduction_tAsdGmBbbeetssv
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_tAsdGmBbbeetssv:
	jmp	.L_last_blocks_done_enwimDbxumidpjj
.L_last_num_blocks_is_13_enwimDbxumidpjj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_xzirlEBlsCxocgx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_xzirlEBlsCxocgx

.L_16_blocks_overflow_xzirlEBlsCxocgx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_xzirlEBlsCxocgx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_AzxrCCFBawejpBG





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_AzxrCCFBawejpBG
.L_small_initial_partial_block_AzxrCCFBawejpBG:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_AzxrCCFBawejpBG:

	orq	%r8,%r8
	je	.L_after_reduction_AzxrCCFBawejpBG
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_AzxrCCFBawejpBG:
	jmp	.L_last_blocks_done_enwimDbxumidpjj
.L_last_num_blocks_is_14_enwimDbxumidpjj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_dlinswmeafCvDhB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_dlinswmeafCvDhB

.L_16_blocks_overflow_dlinswmeafCvDhB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_dlinswmeafCvDhB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dmuGsDvemutBrhD





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dmuGsDvemutBrhD
.L_small_initial_partial_block_dmuGsDvemutBrhD:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dmuGsDvemutBrhD:

	orq	%r8,%r8
	je	.L_after_reduction_dmuGsDvemutBrhD
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dmuGsDvemutBrhD:
	jmp	.L_last_blocks_done_enwimDbxumidpjj
.L_last_num_blocks_is_15_enwimDbxumidpjj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_bsrqBfhvGqnErjd
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_bsrqBfhvGqnErjd

.L_16_blocks_overflow_bsrqBfhvGqnErjd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_bsrqBfhvGqnErjd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_kfiEcdidpEutowc





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_kfiEcdidpEutowc
.L_small_initial_partial_block_kfiEcdidpEutowc:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_kfiEcdidpEutowc:

	orq	%r8,%r8
	je	.L_after_reduction_kfiEcdidpEutowc
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_kfiEcdidpEutowc:
	jmp	.L_last_blocks_done_enwimDbxumidpjj
.L_last_num_blocks_is_16_enwimDbxumidpjj:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_GwqqdfhdtDsyfDm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_GwqqdfhdtDsyfDm

.L_16_blocks_overflow_GwqqdfhdtDsyfDm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_GwqqdfhdtDsyfDm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_BvmAogDnqqxtfjy:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_BvmAogDnqqxtfjy:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_BvmAogDnqqxtfjy:
	jmp	.L_last_blocks_done_enwimDbxumidpjj
.L_last_num_blocks_is_0_enwimDbxumidpjj:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_enwimDbxumidpjj:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_isvfdtgkuyAliym

.L_message_below_equal_16_blocks_isvfdtgkuyAliym:


	movl	%r8d,%r12d
	addl	$15,%r12d
	shrl	$4,%r12d
	cmpq	$8,%r12
	je	.L_small_initial_num_blocks_is_8_vkCbtfkBdGByuso
	jl	.L_small_initial_num_blocks_is_7_1_vkCbtfkBdGByuso


	cmpq	$12,%r12
	je	.L_small_initial_num_blocks_is_12_vkCbtfkBdGByuso
	jl	.L_small_initial_num_blocks_is_11_9_vkCbtfkBdGByuso


	cmpq	$16,%r12
	je	.L_small_initial_num_blocks_is_16_vkCbtfkBdGByuso
	cmpq	$15,%r12
	je	.L_small_initial_num_blocks_is_15_vkCbtfkBdGByuso
	cmpq	$14,%r12
	je	.L_small_initial_num_blocks_is_14_vkCbtfkBdGByuso
	jmp	.L_small_initial_num_blocks_is_13_vkCbtfkBdGByuso

.L_small_initial_num_blocks_is_11_9_vkCbtfkBdGByuso:

	cmpq	$11,%r12
	je	.L_small_initial_num_blocks_is_11_vkCbtfkBdGByuso
	cmpq	$10,%r12
	je	.L_small_initial_num_blocks_is_10_vkCbtfkBdGByuso
	jmp	.L_small_initial_num_blocks_is_9_vkCbtfkBdGByuso

.L_small_initial_num_blocks_is_7_1_vkCbtfkBdGByuso:
	cmpq	$4,%r12
	je	.L_small_initial_num_blocks_is_4_vkCbtfkBdGByuso
	jl	.L_small_initial_num_blocks_is_3_1_vkCbtfkBdGByuso

	cmpq	$7,%r12
	je	.L_small_initial_num_blocks_is_7_vkCbtfkBdGByuso
	cmpq	$6,%r12
	je	.L_small_initial_num_blocks_is_6_vkCbtfkBdGByuso
	jmp	.L_small_initial_num_blocks_is_5_vkCbtfkBdGByuso

.L_small_initial_num_blocks_is_3_1_vkCbtfkBdGByuso:

	cmpq	$3,%r12
	je	.L_small_initial_num_blocks_is_3_vkCbtfkBdGByuso
	cmpq	$2,%r12
	je	.L_small_initial_num_blocks_is_2_vkCbtfkBdGByuso





.L_small_initial_num_blocks_is_1_vkCbtfkBdGByuso:
	vmovdqa64	SHUF_MASK(%rip),%xmm29
	vpaddd	ONE(%rip),%xmm2,%xmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm0,%xmm2
	vpshufb	%xmm29,%xmm0,%xmm0
	vmovdqu8	0(%rcx,%rax,1),%xmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%xmm15,%xmm0,%xmm0
	vpxorq	%xmm6,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm6,%xmm6
	vextracti32x4	$0,%zmm6,%xmm13
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DxtEyptejpzacFv





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DxtEyptejpzacFv
.L_small_initial_partial_block_DxtEyptejpzacFv:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)











	vpxorq	%xmm13,%xmm14,%xmm14

	jmp	.L_after_reduction_DxtEyptejpzacFv
.L_small_initial_compute_done_DxtEyptejpzacFv:
.L_after_reduction_DxtEyptejpzacFv:
	jmp	.L_small_initial_blocks_encrypted_vkCbtfkBdGByuso
.L_small_initial_num_blocks_is_2_vkCbtfkBdGByuso:
	vmovdqa64	SHUF_MASK(%rip),%ymm29
	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm0,%xmm2
	vpshufb	%ymm29,%ymm0,%ymm0
	vmovdqu8	0(%rcx,%rax,1),%ymm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%ymm15,%ymm0,%ymm0
	vpxorq	%ymm6,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm6,%ymm6
	vextracti32x4	$1,%zmm6,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_pGtFcBazBrecfpt





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_pGtFcBazBrecfpt
.L_small_initial_partial_block_pGtFcBazBrecfpt:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_pGtFcBazBrecfpt:

	orq	%r8,%r8
	je	.L_after_reduction_pGtFcBazBrecfpt
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_pGtFcBazBrecfpt:
	jmp	.L_small_initial_blocks_encrypted_vkCbtfkBdGByuso
.L_small_initial_num_blocks_is_3_vkCbtfkBdGByuso:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vextracti32x4	$2,%zmm6,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EiscosxxlwtBapi





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EiscosxxlwtBapi
.L_small_initial_partial_block_EiscosxxlwtBapi:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_EiscosxxlwtBapi:

	orq	%r8,%r8
	je	.L_after_reduction_EiscosxxlwtBapi
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_EiscosxxlwtBapi:
	jmp	.L_small_initial_blocks_encrypted_vkCbtfkBdGByuso
.L_small_initial_num_blocks_is_4_vkCbtfkBdGByuso:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vextracti32x4	$3,%zmm6,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nsxdhdshbdqxemq





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nsxdhdshbdqxemq
.L_small_initial_partial_block_nsxdhdshbdqxemq:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nsxdhdshbdqxemq:

	orq	%r8,%r8
	je	.L_after_reduction_nsxdhdshbdqxemq
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_nsxdhdshbdqxemq:
	jmp	.L_small_initial_blocks_encrypted_vkCbtfkBdGByuso
.L_small_initial_num_blocks_is_5_vkCbtfkBdGByuso:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%xmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%xmm15,%xmm3,%xmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%xmm7,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%xmm29,%xmm7,%xmm7
	vextracti32x4	$0,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_oDFbfgtephGeFzn





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_oDFbfgtephGeFzn
.L_small_initial_partial_block_oDFbfgtephGeFzn:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_oDFbfgtephGeFzn:

	orq	%r8,%r8
	je	.L_after_reduction_oDFbfgtephGeFzn
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_oDFbfgtephGeFzn:
	jmp	.L_small_initial_blocks_encrypted_vkCbtfkBdGByuso
.L_small_initial_num_blocks_is_6_vkCbtfkBdGByuso:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%ymm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%ymm15,%ymm3,%ymm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%ymm7,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%ymm29,%ymm7,%ymm7
	vextracti32x4	$1,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_stnjrsAfnbotguy





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_stnjrsAfnbotguy
.L_small_initial_partial_block_stnjrsAfnbotguy:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_stnjrsAfnbotguy:

	orq	%r8,%r8
	je	.L_after_reduction_stnjrsAfnbotguy
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_stnjrsAfnbotguy:
	jmp	.L_small_initial_blocks_encrypted_vkCbtfkBdGByuso
.L_small_initial_num_blocks_is_7_vkCbtfkBdGByuso:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vextracti32x4	$2,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gtBqqvkCdlpzyqC





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gtBqqvkCdlpzyqC
.L_small_initial_partial_block_gtBqqvkCdlpzyqC:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gtBqqvkCdlpzyqC:

	orq	%r8,%r8
	je	.L_after_reduction_gtBqqvkCdlpzyqC
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_gtBqqvkCdlpzyqC:
	jmp	.L_small_initial_blocks_encrypted_vkCbtfkBdGByuso
.L_small_initial_num_blocks_is_8_vkCbtfkBdGByuso:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vextracti32x4	$3,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bbrgazzqGrqdggz





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bbrgazzqGrqdggz
.L_small_initial_partial_block_bbrgazzqGrqdggz:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bbrgazzqGrqdggz:

	orq	%r8,%r8
	je	.L_after_reduction_bbrgazzqGrqdggz
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_bbrgazzqGrqdggz:
	jmp	.L_small_initial_blocks_encrypted_vkCbtfkBdGByuso
.L_small_initial_num_blocks_is_9_vkCbtfkBdGByuso:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%xmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%xmm15,%xmm4,%xmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%xmm10,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%xmm29,%xmm10,%xmm10
	vextracti32x4	$0,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_mmizADsBlwAfivm





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_mmizADsBlwAfivm
.L_small_initial_partial_block_mmizADsBlwAfivm:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_mmizADsBlwAfivm:

	orq	%r8,%r8
	je	.L_after_reduction_mmizADsBlwAfivm
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_mmizADsBlwAfivm:
	jmp	.L_small_initial_blocks_encrypted_vkCbtfkBdGByuso
.L_small_initial_num_blocks_is_10_vkCbtfkBdGByuso:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%ymm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%ymm15,%ymm4,%ymm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%ymm10,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%ymm29,%ymm10,%ymm10
	vextracti32x4	$1,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_fyGdyatmjuccugF





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_fyGdyatmjuccugF
.L_small_initial_partial_block_fyGdyatmjuccugF:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_fyGdyatmjuccugF:

	orq	%r8,%r8
	je	.L_after_reduction_fyGdyatmjuccugF
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_fyGdyatmjuccugF:
	jmp	.L_small_initial_blocks_encrypted_vkCbtfkBdGByuso
.L_small_initial_num_blocks_is_11_vkCbtfkBdGByuso:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vextracti32x4	$2,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nxrszeakispCwhb





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nxrszeakispCwhb
.L_small_initial_partial_block_nxrszeakispCwhb:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nxrszeakispCwhb:

	orq	%r8,%r8
	je	.L_after_reduction_nxrszeakispCwhb
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_nxrszeakispCwhb:
	jmp	.L_small_initial_blocks_encrypted_vkCbtfkBdGByuso
.L_small_initial_num_blocks_is_12_vkCbtfkBdGByuso:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vextracti32x4	$3,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ubvBoEDgwoBrdtz





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	64(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ubvBoEDgwoBrdtz
.L_small_initial_partial_block_ubvBoEDgwoBrdtz:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ubvBoEDgwoBrdtz:

	orq	%r8,%r8
	je	.L_after_reduction_ubvBoEDgwoBrdtz
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_ubvBoEDgwoBrdtz:
	jmp	.L_small_initial_blocks_encrypted_vkCbtfkBdGByuso
.L_small_initial_num_blocks_is_13_vkCbtfkBdGByuso:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%xmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%xmm15,%xmm5,%xmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%xmm11,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%xmm29,%xmm11,%xmm11
	vextracti32x4	$0,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CuwDppvfuzvtrGz





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	48(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CuwDppvfuzvtrGz
.L_small_initial_partial_block_CuwDppvfuzvtrGz:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	64(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CuwDppvfuzvtrGz:

	orq	%r8,%r8
	je	.L_after_reduction_CuwDppvfuzvtrGz
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_CuwDppvfuzvtrGz:
	jmp	.L_small_initial_blocks_encrypted_vkCbtfkBdGByuso
.L_small_initial_num_blocks_is_14_vkCbtfkBdGByuso:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%ymm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%ymm15,%ymm5,%ymm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%ymm11,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%ymm29,%ymm11,%ymm11
	vextracti32x4	$1,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ezowyzFhEtGesrv





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	32(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ezowyzFhEtGesrv
.L_small_initial_partial_block_ezowyzFhEtGesrv:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	48(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ezowyzFhEtGesrv:

	orq	%r8,%r8
	je	.L_after_reduction_ezowyzFhEtGesrv
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_ezowyzFhEtGesrv:
	jmp	.L_small_initial_blocks_encrypted_vkCbtfkBdGByuso
.L_small_initial_num_blocks_is_15_vkCbtfkBdGByuso:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vextracti32x4	$2,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_pjdakiecjFuCfCx





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	16(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_pjdakiecjFuCfCx
.L_small_initial_partial_block_pjdakiecjFuCfCx:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	32(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_pjdakiecjFuCfCx:

	orq	%r8,%r8
	je	.L_after_reduction_pjdakiecjFuCfCx
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_pjdakiecjFuCfCx:
	jmp	.L_small_initial_blocks_encrypted_vkCbtfkBdGByuso
.L_small_initial_num_blocks_is_16_vkCbtfkBdGByuso:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vextracti32x4	$3,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_zBjvEmqeaxGpwEp:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	16(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zBjvEmqeaxGpwEp:
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_zBjvEmqeaxGpwEp:
.L_small_initial_blocks_encrypted_vkCbtfkBdGByuso:
.L_ghash_done_isvfdtgkuyAliym:
	vmovdqu64	%xmm2,0(%rsi)
.L_enc_dec_done_isvfdtgkuyAliym:

	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14
	vmovdqu64	%xmm14,64(%rsi)
.L_enc_dec_abort_isvfdtgkuyAliym:
	jmp	.Lexit_gcm_decrypt
.align	32
.Laes_gcm_decrypt_192_avx512:
	orq	%r8,%r8
	je	.L_enc_dec_abort_yoCwxdbotnlDAxA
	xorq	%r14,%r14
	vmovdqu64	64(%rsi),%xmm14
	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14

	movl	(%rdx),%eax
	orq	%rax,%rax
	je	.L_partial_block_done_kDxdijptzymmxhu
	movl	$16,%r10d
	leaq	byte_len_to_mask_table(%rip),%r12
	cmpq	%r10,%r8
	cmovcq	%r8,%r10
	kmovw	(%r12,%r10,2),%k1
	vmovdqu8	(%rcx),%xmm0{%k1}{z}

	vmovdqu64	16(%rsi),%xmm3

	leaq	96(%rsi),%r10
	vmovdqu64	240(%r10),%xmm4



	leaq	SHIFT_MASK(%rip),%r12
	addq	%rax,%r12
	vmovdqu64	(%r12),%xmm5
	vpshufb	%xmm5,%xmm3,%xmm3

	vmovdqa64	%xmm0,%xmm6
	vpxorq	%xmm0,%xmm3,%xmm3


	leaq	(%r8,%rax,1),%r13
	subq	$16,%r13
	jge	.L_no_extra_mask_kDxdijptzymmxhu
	subq	%r13,%r12
.L_no_extra_mask_kDxdijptzymmxhu:



	vmovdqu64	16(%r12),%xmm0
	vpand	%xmm0,%xmm3,%xmm3
	vpand	%xmm0,%xmm6,%xmm6
	vpshufb	SHUF_MASK(%rip),%xmm6,%xmm6
	vpshufb	%xmm5,%xmm6,%xmm6
	vpxorq	%xmm6,%xmm14,%xmm14
	cmpq	$0,%r13
	jl	.L_partial_incomplete_kDxdijptzymmxhu

	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm14,%xmm14

	vpsrldq	$8,%xmm14,%xmm11
	vpslldq	$8,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm7,%xmm7
	vpxorq	%xmm10,%xmm14,%xmm14



	vmovdqu64	POLY2(%rip),%xmm11

	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
	vpslldq	$8,%xmm10,%xmm10
	vpxorq	%xmm10,%xmm14,%xmm14



	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
	vpsrldq	$4,%xmm10,%xmm10
	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
	vpslldq	$4,%xmm14,%xmm14

	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14

	movl	$0,(%rdx)

	movq	%rax,%r12
	movq	$16,%rax
	subq	%r12,%rax
	jmp	.L_enc_dec_done_kDxdijptzymmxhu

.L_partial_incomplete_kDxdijptzymmxhu:
	addl	%r8d,(%rdx)
	movq	%r8,%rax

.L_enc_dec_done_kDxdijptzymmxhu:


	leaq	byte_len_to_mask_table(%rip),%r12
	kmovw	(%r12,%rax,2),%k1
	movq	%r9,%r12
	vmovdqu8	%xmm3,(%r12){%k1}
.L_partial_block_done_kDxdijptzymmxhu:
	vmovdqu64	0(%rsi),%xmm2
	subq	%rax,%r8
	je	.L_enc_dec_done_yoCwxdbotnlDAxA
	cmpq	$256,%r8
	jbe	.L_message_below_equal_16_blocks_yoCwxdbotnlDAxA

	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
	vmovdqa64	ddq_addbe_1234(%rip),%zmm28






	vmovd	%xmm2,%r15d
	andl	$255,%r15d

	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpshufb	%zmm29,%zmm2,%zmm2



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_ltgpEruoiidlwEC
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_ltgpEruoiidlwEC
.L_next_16_overflow_ltgpEruoiidlwEC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_ltgpEruoiidlwEC:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	0(%rcx,%rax,1),%zmm0
	vmovdqu8	64(%rcx,%rax,1),%zmm3
	vmovdqu8	128(%rcx,%rax,1),%zmm4
	vmovdqu8	192(%rcx,%rax,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,0(%r10,%rax,1)
	vmovdqu8	%zmm10,64(%r10,%rax,1)
	vmovdqu8	%zmm11,128(%r10,%rax,1)
	vmovdqu8	%zmm12,192(%r10,%rax,1)

	vpshufb	%zmm29,%zmm0,%zmm7
	vpshufb	%zmm29,%zmm3,%zmm10
	vpshufb	%zmm29,%zmm4,%zmm11
	vpshufb	%zmm29,%zmm5,%zmm12
	vmovdqa64	%zmm7,768(%rsp)
	vmovdqa64	%zmm10,832(%rsp)
	vmovdqa64	%zmm11,896(%rsp)
	vmovdqa64	%zmm12,960(%rsp)
	leaq	96(%rsi),%r12
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_yDkwBGaoheAFpuv

	vmovdqu64	192(%r12),%zmm0
	vmovdqu64	%zmm0,704(%rsp)

	vmovdqu64	128(%r12),%zmm3
	vmovdqu64	%zmm3,640(%rsp)


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	64(%r12),%zmm4
	vmovdqu64	%zmm4,576(%rsp)

	vmovdqu64	0(%r12),%zmm5
	vmovdqu64	%zmm5,512(%rsp)
.L_skip_hkeys_precomputation_yDkwBGaoheAFpuv:
	cmpq	$512,%r8
	jb	.L_message_below_32_blocks_yoCwxdbotnlDAxA



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_vaiEzdojBzwjBqu
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_vaiEzdojBzwjBqu
.L_next_16_overflow_vaiEzdojBzwjBqu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_vaiEzdojBzwjBqu:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	256(%rcx,%rax,1),%zmm0
	vmovdqu8	320(%rcx,%rax,1),%zmm3
	vmovdqu8	384(%rcx,%rax,1),%zmm4
	vmovdqu8	448(%rcx,%rax,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,256(%r10,%rax,1)
	vmovdqu8	%zmm10,320(%r10,%rax,1)
	vmovdqu8	%zmm11,384(%r10,%rax,1)
	vmovdqu8	%zmm12,448(%r10,%rax,1)

	vpshufb	%zmm29,%zmm0,%zmm7
	vpshufb	%zmm29,%zmm3,%zmm10
	vpshufb	%zmm29,%zmm4,%zmm11
	vpshufb	%zmm29,%zmm5,%zmm12
	vmovdqa64	%zmm7,1024(%rsp)
	vmovdqa64	%zmm10,1088(%rsp)
	vmovdqa64	%zmm11,1152(%rsp)
	vmovdqa64	%zmm12,1216(%rsp)
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_figCsfhGrfzECra
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,192(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,128(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,64(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,0(%rsp)
.L_skip_hkeys_precomputation_figCsfhGrfzECra:
	movq	$1,%r14
	addq	$512,%rax
	subq	$512,%r8

	cmpq	$768,%r8
	jb	.L_no_more_big_nblocks_yoCwxdbotnlDAxA
.L_encrypt_big_nblocks_yoCwxdbotnlDAxA:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_whFEvmexxxjcweh
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_whFEvmexxxjcweh
.L_16_blocks_overflow_whFEvmexxxjcweh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_whFEvmexxxjcweh:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_zCedAlDvBlfAeAl
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_zCedAlDvBlfAeAl
.L_16_blocks_overflow_zCedAlDvBlfAeAl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_zCedAlDvBlfAeAl:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%rax,1),%zmm17
	vmovdqu8	320(%rcx,%rax,1),%zmm19
	vmovdqu8	384(%rcx,%rax,1),%zmm20
	vmovdqu8	448(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%rax,1)
	vmovdqu8	%zmm3,320(%r10,%rax,1)
	vmovdqu8	%zmm4,384(%r10,%rax,1)
	vmovdqu8	%zmm5,448(%r10,%rax,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_aqFpfjDnEbrqrCg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_aqFpfjDnEbrqrCg
.L_16_blocks_overflow_aqFpfjDnEbrqrCg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_aqFpfjDnEbrqrCg:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	512(%rcx,%rax,1),%zmm17
	vmovdqu8	576(%rcx,%rax,1),%zmm19
	vmovdqu8	640(%rcx,%rax,1),%zmm20
	vmovdqu8	704(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30


	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10

	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
	vpxorq	%zmm24,%zmm6,%zmm6
	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
	vpxorq	%zmm25,%zmm7,%zmm7
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vextracti64x4	$1,%zmm6,%ymm12
	vpxorq	%ymm12,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm12
	vpxorq	%xmm12,%xmm6,%xmm6
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,512(%r10,%rax,1)
	vmovdqu8	%zmm3,576(%r10,%rax,1)
	vmovdqu8	%zmm4,640(%r10,%rax,1)
	vmovdqu8	%zmm5,704(%r10,%rax,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1024(%rsp)
	vmovdqa64	%zmm3,1088(%rsp)
	vmovdqa64	%zmm4,1152(%rsp)
	vmovdqa64	%zmm5,1216(%rsp)
	vmovdqa64	%zmm6,%zmm14

	addq	$768,%rax
	subq	$768,%r8
	cmpq	$768,%r8
	jae	.L_encrypt_big_nblocks_yoCwxdbotnlDAxA

.L_no_more_big_nblocks_yoCwxdbotnlDAxA:

	cmpq	$512,%r8
	jae	.L_encrypt_32_blocks_yoCwxdbotnlDAxA

	cmpq	$256,%r8
	jae	.L_encrypt_16_blocks_yoCwxdbotnlDAxA
.L_encrypt_0_blocks_ghash_32_yoCwxdbotnlDAxA:
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$256,%ebx
	subl	%r10d,%ebx
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	addl	$256,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_FDaoqEikaxCauGy

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_FDaoqEikaxCauGy
	jb	.L_last_num_blocks_is_7_1_FDaoqEikaxCauGy


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_FDaoqEikaxCauGy
	jb	.L_last_num_blocks_is_11_9_FDaoqEikaxCauGy


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_FDaoqEikaxCauGy
	ja	.L_last_num_blocks_is_16_FDaoqEikaxCauGy
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_FDaoqEikaxCauGy
	jmp	.L_last_num_blocks_is_13_FDaoqEikaxCauGy

.L_last_num_blocks_is_11_9_FDaoqEikaxCauGy:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_FDaoqEikaxCauGy
	ja	.L_last_num_blocks_is_11_FDaoqEikaxCauGy
	jmp	.L_last_num_blocks_is_9_FDaoqEikaxCauGy

.L_last_num_blocks_is_7_1_FDaoqEikaxCauGy:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_FDaoqEikaxCauGy
	jb	.L_last_num_blocks_is_3_1_FDaoqEikaxCauGy

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_FDaoqEikaxCauGy
	je	.L_last_num_blocks_is_6_FDaoqEikaxCauGy
	jmp	.L_last_num_blocks_is_5_FDaoqEikaxCauGy

.L_last_num_blocks_is_3_1_FDaoqEikaxCauGy:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_FDaoqEikaxCauGy
	je	.L_last_num_blocks_is_2_FDaoqEikaxCauGy
.L_last_num_blocks_is_1_FDaoqEikaxCauGy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_EBEdbsAxhmrpBoG
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_EBEdbsAxhmrpBoG

.L_16_blocks_overflow_EBEdbsAxhmrpBoG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_EBEdbsAxhmrpBoG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_tgydfmintwsqqlw





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_tgydfmintwsqqlw
.L_small_initial_partial_block_tgydfmintwsqqlw:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_tgydfmintwsqqlw
.L_small_initial_compute_done_tgydfmintwsqqlw:
.L_after_reduction_tgydfmintwsqqlw:
	jmp	.L_last_blocks_done_FDaoqEikaxCauGy
.L_last_num_blocks_is_2_FDaoqEikaxCauGy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_FpfqdnahrBdFeAd
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_FpfqdnahrBdFeAd

.L_16_blocks_overflow_FpfqdnahrBdFeAd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_FpfqdnahrBdFeAd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_Epvwqetvnbmzehl





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_Epvwqetvnbmzehl
.L_small_initial_partial_block_Epvwqetvnbmzehl:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_Epvwqetvnbmzehl:

	orq	%r8,%r8
	je	.L_after_reduction_Epvwqetvnbmzehl
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_Epvwqetvnbmzehl:
	jmp	.L_last_blocks_done_FDaoqEikaxCauGy
.L_last_num_blocks_is_3_FDaoqEikaxCauGy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_eguuetujDsansew
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_eguuetujDsansew

.L_16_blocks_overflow_eguuetujDsansew:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_eguuetujDsansew:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EozFosqCamltfmf





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EozFosqCamltfmf
.L_small_initial_partial_block_EozFosqCamltfmf:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_EozFosqCamltfmf:

	orq	%r8,%r8
	je	.L_after_reduction_EozFosqCamltfmf
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_EozFosqCamltfmf:
	jmp	.L_last_blocks_done_FDaoqEikaxCauGy
.L_last_num_blocks_is_4_FDaoqEikaxCauGy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_GaifAcohwakFgbn
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_GaifAcohwakFgbn

.L_16_blocks_overflow_GaifAcohwakFgbn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_GaifAcohwakFgbn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jBFxnsvqoiqzjwj





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jBFxnsvqoiqzjwj
.L_small_initial_partial_block_jBFxnsvqoiqzjwj:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jBFxnsvqoiqzjwj:

	orq	%r8,%r8
	je	.L_after_reduction_jBFxnsvqoiqzjwj
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jBFxnsvqoiqzjwj:
	jmp	.L_last_blocks_done_FDaoqEikaxCauGy
.L_last_num_blocks_is_5_FDaoqEikaxCauGy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_ylwsCevjgiqmetk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_ylwsCevjgiqmetk

.L_16_blocks_overflow_ylwsCevjgiqmetk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_ylwsCevjgiqmetk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_tuwzpexdecmjbEC





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_tuwzpexdecmjbEC
.L_small_initial_partial_block_tuwzpexdecmjbEC:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_tuwzpexdecmjbEC:

	orq	%r8,%r8
	je	.L_after_reduction_tuwzpexdecmjbEC
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_tuwzpexdecmjbEC:
	jmp	.L_last_blocks_done_FDaoqEikaxCauGy
.L_last_num_blocks_is_6_FDaoqEikaxCauGy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_hgBrjDakCqGjyzm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_hgBrjDakCqGjyzm

.L_16_blocks_overflow_hgBrjDakCqGjyzm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_hgBrjDakCqGjyzm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zrckgjfCwuCEGxm





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zrckgjfCwuCEGxm
.L_small_initial_partial_block_zrckgjfCwuCEGxm:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zrckgjfCwuCEGxm:

	orq	%r8,%r8
	je	.L_after_reduction_zrckgjfCwuCEGxm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_zrckgjfCwuCEGxm:
	jmp	.L_last_blocks_done_FDaoqEikaxCauGy
.L_last_num_blocks_is_7_FDaoqEikaxCauGy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_wsueAilnClgacjg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_wsueAilnClgacjg

.L_16_blocks_overflow_wsueAilnClgacjg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_wsueAilnClgacjg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_yfFovftuldboFFi





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_yfFovftuldboFFi
.L_small_initial_partial_block_yfFovftuldboFFi:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yfFovftuldboFFi:

	orq	%r8,%r8
	je	.L_after_reduction_yfFovftuldboFFi
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_yfFovftuldboFFi:
	jmp	.L_last_blocks_done_FDaoqEikaxCauGy
.L_last_num_blocks_is_8_FDaoqEikaxCauGy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_fcicfjADucnyyhb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_fcicfjADucnyyhb

.L_16_blocks_overflow_fcicfjADucnyyhb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_fcicfjADucnyyhb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_waderFwiBjdlGrr





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_waderFwiBjdlGrr
.L_small_initial_partial_block_waderFwiBjdlGrr:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_waderFwiBjdlGrr:

	orq	%r8,%r8
	je	.L_after_reduction_waderFwiBjdlGrr
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_waderFwiBjdlGrr:
	jmp	.L_last_blocks_done_FDaoqEikaxCauGy
.L_last_num_blocks_is_9_FDaoqEikaxCauGy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_tuGfzbgexhfduxx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_tuGfzbgexhfduxx

.L_16_blocks_overflow_tuGfzbgexhfduxx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_tuGfzbgexhfduxx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_FukDxnkfFmBajEh





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_FukDxnkfFmBajEh
.L_small_initial_partial_block_FukDxnkfFmBajEh:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_FukDxnkfFmBajEh:

	orq	%r8,%r8
	je	.L_after_reduction_FukDxnkfFmBajEh
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_FukDxnkfFmBajEh:
	jmp	.L_last_blocks_done_FDaoqEikaxCauGy
.L_last_num_blocks_is_10_FDaoqEikaxCauGy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_EngwAGxkfrpvkem
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_EngwAGxkfrpvkem

.L_16_blocks_overflow_EngwAGxkfrpvkem:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_EngwAGxkfrpvkem:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cuBFhiEkbqiBtxe





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cuBFhiEkbqiBtxe
.L_small_initial_partial_block_cuBFhiEkbqiBtxe:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cuBFhiEkbqiBtxe:

	orq	%r8,%r8
	je	.L_after_reduction_cuBFhiEkbqiBtxe
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cuBFhiEkbqiBtxe:
	jmp	.L_last_blocks_done_FDaoqEikaxCauGy
.L_last_num_blocks_is_11_FDaoqEikaxCauGy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_CrxetErijwyFedm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_CrxetErijwyFedm

.L_16_blocks_overflow_CrxetErijwyFedm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_CrxetErijwyFedm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_uqiiofyfmCifDxp





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_uqiiofyfmCifDxp
.L_small_initial_partial_block_uqiiofyfmCifDxp:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_uqiiofyfmCifDxp:

	orq	%r8,%r8
	je	.L_after_reduction_uqiiofyfmCifDxp
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_uqiiofyfmCifDxp:
	jmp	.L_last_blocks_done_FDaoqEikaxCauGy
.L_last_num_blocks_is_12_FDaoqEikaxCauGy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_euGwkrquaGucziu
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_euGwkrquaGucziu

.L_16_blocks_overflow_euGwkrquaGucziu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_euGwkrquaGucziu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gaayjcAixcwrhqn





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gaayjcAixcwrhqn
.L_small_initial_partial_block_gaayjcAixcwrhqn:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gaayjcAixcwrhqn:

	orq	%r8,%r8
	je	.L_after_reduction_gaayjcAixcwrhqn
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_gaayjcAixcwrhqn:
	jmp	.L_last_blocks_done_FDaoqEikaxCauGy
.L_last_num_blocks_is_13_FDaoqEikaxCauGy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_oGBuetylkmmECmc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_oGBuetylkmmECmc

.L_16_blocks_overflow_oGBuetylkmmECmc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_oGBuetylkmmECmc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DikniEoGxnGqrco





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DikniEoGxnGqrco
.L_small_initial_partial_block_DikniEoGxnGqrco:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DikniEoGxnGqrco:

	orq	%r8,%r8
	je	.L_after_reduction_DikniEoGxnGqrco
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DikniEoGxnGqrco:
	jmp	.L_last_blocks_done_FDaoqEikaxCauGy
.L_last_num_blocks_is_14_FDaoqEikaxCauGy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_vsazbragufvxGjn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_vsazbragufvxGjn

.L_16_blocks_overflow_vsazbragufvxGjn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_vsazbragufvxGjn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_yqiEcgAFfehpGtm





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_yqiEcgAFfehpGtm
.L_small_initial_partial_block_yqiEcgAFfehpGtm:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yqiEcgAFfehpGtm:

	orq	%r8,%r8
	je	.L_after_reduction_yqiEcgAFfehpGtm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_yqiEcgAFfehpGtm:
	jmp	.L_last_blocks_done_FDaoqEikaxCauGy
.L_last_num_blocks_is_15_FDaoqEikaxCauGy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_yaqnxEofCidvjzB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_yaqnxEofCidvjzB

.L_16_blocks_overflow_yaqnxEofCidvjzB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_yaqnxEofCidvjzB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GqgkEGhaCEawaBu





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GqgkEGhaCEawaBu
.L_small_initial_partial_block_GqgkEGhaCEawaBu:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GqgkEGhaCEawaBu:

	orq	%r8,%r8
	je	.L_after_reduction_GqgkEGhaCEawaBu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_GqgkEGhaCEawaBu:
	jmp	.L_last_blocks_done_FDaoqEikaxCauGy
.L_last_num_blocks_is_16_FDaoqEikaxCauGy:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_tCiwvnDwDsDDohE
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_tCiwvnDwDsDDohE

.L_16_blocks_overflow_tCiwvnDwDsDDohE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_tCiwvnDwDsDDohE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_tABjcjCzpExjdvF:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_tABjcjCzpExjdvF:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_tABjcjCzpExjdvF:
	jmp	.L_last_blocks_done_FDaoqEikaxCauGy
.L_last_num_blocks_is_0_FDaoqEikaxCauGy:
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_FDaoqEikaxCauGy:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_yoCwxdbotnlDAxA
.L_encrypt_32_blocks_yoCwxdbotnlDAxA:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_AkCBjzuczhnmojp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_AkCBjzuczhnmojp
.L_16_blocks_overflow_AkCBjzuczhnmojp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_AkCBjzuczhnmojp:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_BhxlxovCjepqyCF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_BhxlxovCjepqyCF
.L_16_blocks_overflow_BhxlxovCjepqyCF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_BhxlxovCjepqyCF:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%rax,1),%zmm17
	vmovdqu8	320(%rcx,%rax,1),%zmm19
	vmovdqu8	384(%rcx,%rax,1),%zmm20
	vmovdqu8	448(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%rax,1)
	vmovdqu8	%zmm3,320(%r10,%rax,1)
	vmovdqu8	%zmm4,384(%r10,%rax,1)
	vmovdqu8	%zmm5,448(%r10,%rax,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

	subq	$512,%r8
	addq	$512,%rax
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_CuGfiuyrejhiBjA

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_CuGfiuyrejhiBjA
	jb	.L_last_num_blocks_is_7_1_CuGfiuyrejhiBjA


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_CuGfiuyrejhiBjA
	jb	.L_last_num_blocks_is_11_9_CuGfiuyrejhiBjA


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_CuGfiuyrejhiBjA
	ja	.L_last_num_blocks_is_16_CuGfiuyrejhiBjA
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_CuGfiuyrejhiBjA
	jmp	.L_last_num_blocks_is_13_CuGfiuyrejhiBjA

.L_last_num_blocks_is_11_9_CuGfiuyrejhiBjA:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_CuGfiuyrejhiBjA
	ja	.L_last_num_blocks_is_11_CuGfiuyrejhiBjA
	jmp	.L_last_num_blocks_is_9_CuGfiuyrejhiBjA

.L_last_num_blocks_is_7_1_CuGfiuyrejhiBjA:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_CuGfiuyrejhiBjA
	jb	.L_last_num_blocks_is_3_1_CuGfiuyrejhiBjA

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_CuGfiuyrejhiBjA
	je	.L_last_num_blocks_is_6_CuGfiuyrejhiBjA
	jmp	.L_last_num_blocks_is_5_CuGfiuyrejhiBjA

.L_last_num_blocks_is_3_1_CuGfiuyrejhiBjA:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_CuGfiuyrejhiBjA
	je	.L_last_num_blocks_is_2_CuGfiuyrejhiBjA
.L_last_num_blocks_is_1_CuGfiuyrejhiBjA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_FBCBpDkytcyezxt
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_FBCBpDkytcyezxt

.L_16_blocks_overflow_FBCBpDkytcyezxt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_FBCBpDkytcyezxt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gkADjjGpuACcliv





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gkADjjGpuACcliv
.L_small_initial_partial_block_gkADjjGpuACcliv:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_gkADjjGpuACcliv
.L_small_initial_compute_done_gkADjjGpuACcliv:
.L_after_reduction_gkADjjGpuACcliv:
	jmp	.L_last_blocks_done_CuGfiuyrejhiBjA
.L_last_num_blocks_is_2_CuGfiuyrejhiBjA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_wjfbEqkBcwrxFcF
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_wjfbEqkBcwrxFcF

.L_16_blocks_overflow_wjfbEqkBcwrxFcF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_wjfbEqkBcwrxFcF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qjpoCqevobrBFuf





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qjpoCqevobrBFuf
.L_small_initial_partial_block_qjpoCqevobrBFuf:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qjpoCqevobrBFuf:

	orq	%r8,%r8
	je	.L_after_reduction_qjpoCqevobrBFuf
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_qjpoCqevobrBFuf:
	jmp	.L_last_blocks_done_CuGfiuyrejhiBjA
.L_last_num_blocks_is_3_CuGfiuyrejhiBjA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_vvslaifhdGrxqBs
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_vvslaifhdGrxqBs

.L_16_blocks_overflow_vvslaifhdGrxqBs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_vvslaifhdGrxqBs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_wuakEGawbsFaAkb





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_wuakEGawbsFaAkb
.L_small_initial_partial_block_wuakEGawbsFaAkb:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_wuakEGawbsFaAkb:

	orq	%r8,%r8
	je	.L_after_reduction_wuakEGawbsFaAkb
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_wuakEGawbsFaAkb:
	jmp	.L_last_blocks_done_CuGfiuyrejhiBjA
.L_last_num_blocks_is_4_CuGfiuyrejhiBjA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_dhEEsesbqyjactu
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_dhEEsesbqyjactu

.L_16_blocks_overflow_dhEEsesbqyjactu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_dhEEsesbqyjactu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GznzywGrlCpEnxk





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GznzywGrlCpEnxk
.L_small_initial_partial_block_GznzywGrlCpEnxk:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GznzywGrlCpEnxk:

	orq	%r8,%r8
	je	.L_after_reduction_GznzywGrlCpEnxk
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_GznzywGrlCpEnxk:
	jmp	.L_last_blocks_done_CuGfiuyrejhiBjA
.L_last_num_blocks_is_5_CuGfiuyrejhiBjA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_BiaffkrjuAvtFcD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_BiaffkrjuAvtFcD

.L_16_blocks_overflow_BiaffkrjuAvtFcD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_BiaffkrjuAvtFcD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_trsdgFnBoDDnFfq





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_trsdgFnBoDDnFfq
.L_small_initial_partial_block_trsdgFnBoDDnFfq:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_trsdgFnBoDDnFfq:

	orq	%r8,%r8
	je	.L_after_reduction_trsdgFnBoDDnFfq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_trsdgFnBoDDnFfq:
	jmp	.L_last_blocks_done_CuGfiuyrejhiBjA
.L_last_num_blocks_is_6_CuGfiuyrejhiBjA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_AEnrscGmAGBDCBc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_AEnrscGmAGBDCBc

.L_16_blocks_overflow_AEnrscGmAGBDCBc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_AEnrscGmAGBDCBc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jpsGztctizAxaiF





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jpsGztctizAxaiF
.L_small_initial_partial_block_jpsGztctizAxaiF:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jpsGztctizAxaiF:

	orq	%r8,%r8
	je	.L_after_reduction_jpsGztctizAxaiF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jpsGztctizAxaiF:
	jmp	.L_last_blocks_done_CuGfiuyrejhiBjA
.L_last_num_blocks_is_7_CuGfiuyrejhiBjA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_CjotkaGiuEjibzv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_CjotkaGiuEjibzv

.L_16_blocks_overflow_CjotkaGiuEjibzv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_CjotkaGiuEjibzv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nnAiCmBsmotGpet





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nnAiCmBsmotGpet
.L_small_initial_partial_block_nnAiCmBsmotGpet:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nnAiCmBsmotGpet:

	orq	%r8,%r8
	je	.L_after_reduction_nnAiCmBsmotGpet
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_nnAiCmBsmotGpet:
	jmp	.L_last_blocks_done_CuGfiuyrejhiBjA
.L_last_num_blocks_is_8_CuGfiuyrejhiBjA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_CnFfEFlvqloEhuc
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_CnFfEFlvqloEhuc

.L_16_blocks_overflow_CnFfEFlvqloEhuc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_CnFfEFlvqloEhuc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_FDCrqxijauwbsle





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_FDCrqxijauwbsle
.L_small_initial_partial_block_FDCrqxijauwbsle:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_FDCrqxijauwbsle:

	orq	%r8,%r8
	je	.L_after_reduction_FDCrqxijauwbsle
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_FDCrqxijauwbsle:
	jmp	.L_last_blocks_done_CuGfiuyrejhiBjA
.L_last_num_blocks_is_9_CuGfiuyrejhiBjA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_znzeDFGGcGCCDGu
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_znzeDFGGcGCCDGu

.L_16_blocks_overflow_znzeDFGGcGCCDGu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_znzeDFGGcGCCDGu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CcohGueDxdoftgg





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CcohGueDxdoftgg
.L_small_initial_partial_block_CcohGueDxdoftgg:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CcohGueDxdoftgg:

	orq	%r8,%r8
	je	.L_after_reduction_CcohGueDxdoftgg
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_CcohGueDxdoftgg:
	jmp	.L_last_blocks_done_CuGfiuyrejhiBjA
.L_last_num_blocks_is_10_CuGfiuyrejhiBjA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_aEmnykhzEwmcwql
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_aEmnykhzEwmcwql

.L_16_blocks_overflow_aEmnykhzEwmcwql:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_aEmnykhzEwmcwql:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gtjakAvrkbEqaiF





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gtjakAvrkbEqaiF
.L_small_initial_partial_block_gtjakAvrkbEqaiF:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gtjakAvrkbEqaiF:

	orq	%r8,%r8
	je	.L_after_reduction_gtjakAvrkbEqaiF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_gtjakAvrkbEqaiF:
	jmp	.L_last_blocks_done_CuGfiuyrejhiBjA
.L_last_num_blocks_is_11_CuGfiuyrejhiBjA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_vqqEfjnqFDyatwg
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_vqqEfjnqFDyatwg

.L_16_blocks_overflow_vqqEfjnqFDyatwg:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_vqqEfjnqFDyatwg:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_pBwcGsEerjFDbGh





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_pBwcGsEerjFDbGh
.L_small_initial_partial_block_pBwcGsEerjFDbGh:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_pBwcGsEerjFDbGh:

	orq	%r8,%r8
	je	.L_after_reduction_pBwcGsEerjFDbGh
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_pBwcGsEerjFDbGh:
	jmp	.L_last_blocks_done_CuGfiuyrejhiBjA
.L_last_num_blocks_is_12_CuGfiuyrejhiBjA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_DexefoDvluydxbl
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_DexefoDvluydxbl

.L_16_blocks_overflow_DexefoDvluydxbl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_DexefoDvluydxbl:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rFtvmxtrbokbqaF





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rFtvmxtrbokbqaF
.L_small_initial_partial_block_rFtvmxtrbokbqaF:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rFtvmxtrbokbqaF:

	orq	%r8,%r8
	je	.L_after_reduction_rFtvmxtrbokbqaF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_rFtvmxtrbokbqaF:
	jmp	.L_last_blocks_done_CuGfiuyrejhiBjA
.L_last_num_blocks_is_13_CuGfiuyrejhiBjA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_wzCuDFoljDExprv
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_wzCuDFoljDExprv

.L_16_blocks_overflow_wzCuDFoljDExprv:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_wzCuDFoljDExprv:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GeBcznucitucmwo





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GeBcznucitucmwo
.L_small_initial_partial_block_GeBcznucitucmwo:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GeBcznucitucmwo:

	orq	%r8,%r8
	je	.L_after_reduction_GeBcznucitucmwo
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_GeBcznucitucmwo:
	jmp	.L_last_blocks_done_CuGfiuyrejhiBjA
.L_last_num_blocks_is_14_CuGfiuyrejhiBjA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_wyskuvDckomqmxC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_wyskuvDckomqmxC

.L_16_blocks_overflow_wyskuvDckomqmxC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_wyskuvDckomqmxC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_wuwBCGCrzBzinri





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_wuwBCGCrzBzinri
.L_small_initial_partial_block_wuwBCGCrzBzinri:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_wuwBCGCrzBzinri:

	orq	%r8,%r8
	je	.L_after_reduction_wuwBCGCrzBzinri
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_wuwBCGCrzBzinri:
	jmp	.L_last_blocks_done_CuGfiuyrejhiBjA
.L_last_num_blocks_is_15_CuGfiuyrejhiBjA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_oBcykDdmlamGhBx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_oBcykDdmlamGhBx

.L_16_blocks_overflow_oBcykDdmlamGhBx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_oBcykDdmlamGhBx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_wuCqbyEccECscpr





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_wuCqbyEccECscpr
.L_small_initial_partial_block_wuCqbyEccECscpr:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_wuCqbyEccECscpr:

	orq	%r8,%r8
	je	.L_after_reduction_wuCqbyEccECscpr
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_wuCqbyEccECscpr:
	jmp	.L_last_blocks_done_CuGfiuyrejhiBjA
.L_last_num_blocks_is_16_CuGfiuyrejhiBjA:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_kpkfyGzrnzwdkar
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_kpkfyGzrnzwdkar

.L_16_blocks_overflow_kpkfyGzrnzwdkar:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_kpkfyGzrnzwdkar:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_uGkdbzwtgEBEfjb:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_uGkdbzwtgEBEfjb:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_uGkdbzwtgEBEfjb:
	jmp	.L_last_blocks_done_CuGfiuyrejhiBjA
.L_last_num_blocks_is_0_CuGfiuyrejhiBjA:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_CuGfiuyrejhiBjA:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_yoCwxdbotnlDAxA
.L_encrypt_16_blocks_yoCwxdbotnlDAxA:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_nxreBErAsrkjjjA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_nxreBErAsrkjjjA
.L_16_blocks_overflow_nxreBErAsrkjjjA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_nxreBErAsrkjjjA:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	256(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	320(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	384(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	448(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	subq	$256,%r8
	addq	$256,%rax
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_CvCdFbGmvFadEzD

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_CvCdFbGmvFadEzD
	jb	.L_last_num_blocks_is_7_1_CvCdFbGmvFadEzD


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_CvCdFbGmvFadEzD
	jb	.L_last_num_blocks_is_11_9_CvCdFbGmvFadEzD


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_CvCdFbGmvFadEzD
	ja	.L_last_num_blocks_is_16_CvCdFbGmvFadEzD
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_CvCdFbGmvFadEzD
	jmp	.L_last_num_blocks_is_13_CvCdFbGmvFadEzD

.L_last_num_blocks_is_11_9_CvCdFbGmvFadEzD:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_CvCdFbGmvFadEzD
	ja	.L_last_num_blocks_is_11_CvCdFbGmvFadEzD
	jmp	.L_last_num_blocks_is_9_CvCdFbGmvFadEzD

.L_last_num_blocks_is_7_1_CvCdFbGmvFadEzD:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_CvCdFbGmvFadEzD
	jb	.L_last_num_blocks_is_3_1_CvCdFbGmvFadEzD

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_CvCdFbGmvFadEzD
	je	.L_last_num_blocks_is_6_CvCdFbGmvFadEzD
	jmp	.L_last_num_blocks_is_5_CvCdFbGmvFadEzD

.L_last_num_blocks_is_3_1_CvCdFbGmvFadEzD:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_CvCdFbGmvFadEzD
	je	.L_last_num_blocks_is_2_CvCdFbGmvFadEzD
.L_last_num_blocks_is_1_CvCdFbGmvFadEzD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_ypvgncizzdvGGcp
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_ypvgncizzdvGGcp

.L_16_blocks_overflow_ypvgncizzdvGGcp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_ypvgncizzdvGGcp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%xmm31,%xmm0,%xmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_AdqjphswFvrGlyp





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_AdqjphswFvrGlyp
.L_small_initial_partial_block_AdqjphswFvrGlyp:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)











	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_AdqjphswFvrGlyp
.L_small_initial_compute_done_AdqjphswFvrGlyp:
.L_after_reduction_AdqjphswFvrGlyp:
	jmp	.L_last_blocks_done_CvCdFbGmvFadEzD
.L_last_num_blocks_is_2_CvCdFbGmvFadEzD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_dwnkCCmoboAaGzf
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_dwnkCCmoboAaGzf

.L_16_blocks_overflow_dwnkCCmoboAaGzf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_dwnkCCmoboAaGzf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%ymm31,%ymm0,%ymm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_fstAgGexrumojjG





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_fstAgGexrumojjG
.L_small_initial_partial_block_fstAgGexrumojjG:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_fstAgGexrumojjG:

	orq	%r8,%r8
	je	.L_after_reduction_fstAgGexrumojjG
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_fstAgGexrumojjG:
	jmp	.L_last_blocks_done_CvCdFbGmvFadEzD
.L_last_num_blocks_is_3_CvCdFbGmvFadEzD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_yDfgenBtgGhDErG
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_yDfgenBtgGhDErG

.L_16_blocks_overflow_yDfgenBtgGhDErG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_yDfgenBtgGhDErG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cxehpnEahrkskdu





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cxehpnEahrkskdu
.L_small_initial_partial_block_cxehpnEahrkskdu:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cxehpnEahrkskdu:

	orq	%r8,%r8
	je	.L_after_reduction_cxehpnEahrkskdu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cxehpnEahrkskdu:
	jmp	.L_last_blocks_done_CvCdFbGmvFadEzD
.L_last_num_blocks_is_4_CvCdFbGmvFadEzD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_GwqxzBvewFvFrtu
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_GwqxzBvewFvFrtu

.L_16_blocks_overflow_GwqxzBvewFvFrtu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_GwqxzBvewFvFrtu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_klewssBEDGwkAoG





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_klewssBEDGwkAoG
.L_small_initial_partial_block_klewssBEDGwkAoG:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_klewssBEDGwkAoG:

	orq	%r8,%r8
	je	.L_after_reduction_klewssBEDGwkAoG
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_klewssBEDGwkAoG:
	jmp	.L_last_blocks_done_CvCdFbGmvFadEzD
.L_last_num_blocks_is_5_CvCdFbGmvFadEzD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_DhkvvviFxtykfsw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_DhkvvviFxtykfsw

.L_16_blocks_overflow_DhkvvviFxtykfsw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_DhkvvviFxtykfsw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ajFntsesrpbdEyx





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ajFntsesrpbdEyx
.L_small_initial_partial_block_ajFntsesrpbdEyx:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ajFntsesrpbdEyx:

	orq	%r8,%r8
	je	.L_after_reduction_ajFntsesrpbdEyx
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ajFntsesrpbdEyx:
	jmp	.L_last_blocks_done_CvCdFbGmvFadEzD
.L_last_num_blocks_is_6_CvCdFbGmvFadEzD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_uBhpdaFfgBFCsdd
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_uBhpdaFfgBFCsdd

.L_16_blocks_overflow_uBhpdaFfgBFCsdd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_uBhpdaFfgBFCsdd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_BrCGqEjkvpFFAeb





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_BrCGqEjkvpFFAeb
.L_small_initial_partial_block_BrCGqEjkvpFFAeb:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_BrCGqEjkvpFFAeb:

	orq	%r8,%r8
	je	.L_after_reduction_BrCGqEjkvpFFAeb
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_BrCGqEjkvpFFAeb:
	jmp	.L_last_blocks_done_CvCdFbGmvFadEzD
.L_last_num_blocks_is_7_CvCdFbGmvFadEzD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_xvjAgBCtoovBrDs
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_xvjAgBCtoovBrDs

.L_16_blocks_overflow_xvjAgBCtoovBrDs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_xvjAgBCtoovBrDs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_kFneaCzufnnfmkm





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_kFneaCzufnnfmkm
.L_small_initial_partial_block_kFneaCzufnnfmkm:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_kFneaCzufnnfmkm:

	orq	%r8,%r8
	je	.L_after_reduction_kFneaCzufnnfmkm
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_kFneaCzufnnfmkm:
	jmp	.L_last_blocks_done_CvCdFbGmvFadEzD
.L_last_num_blocks_is_8_CvCdFbGmvFadEzD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_AuEcdhFABArczfo
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_AuEcdhFABArczfo

.L_16_blocks_overflow_AuEcdhFABArczfo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_AuEcdhFABArczfo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dyuhrzEBlzfruCr





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dyuhrzEBlzfruCr
.L_small_initial_partial_block_dyuhrzEBlzfruCr:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dyuhrzEBlzfruCr:

	orq	%r8,%r8
	je	.L_after_reduction_dyuhrzEBlzfruCr
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dyuhrzEBlzfruCr:
	jmp	.L_last_blocks_done_CvCdFbGmvFadEzD
.L_last_num_blocks_is_9_CvCdFbGmvFadEzD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_kvEgFacwsAqBtvx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_kvEgFacwsAqBtvx

.L_16_blocks_overflow_kvEgFacwsAqBtvx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_kvEgFacwsAqBtvx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jqAeoeaCwpEDDuk





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jqAeoeaCwpEDDuk
.L_small_initial_partial_block_jqAeoeaCwpEDDuk:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jqAeoeaCwpEDDuk:

	orq	%r8,%r8
	je	.L_after_reduction_jqAeoeaCwpEDDuk
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jqAeoeaCwpEDDuk:
	jmp	.L_last_blocks_done_CvCdFbGmvFadEzD
.L_last_num_blocks_is_10_CvCdFbGmvFadEzD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_pmBrhcFBlwqkfxm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_pmBrhcFBlwqkfxm

.L_16_blocks_overflow_pmBrhcFBlwqkfxm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_pmBrhcFBlwqkfxm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_txmiohodmbAmEmi





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_txmiohodmbAmEmi
.L_small_initial_partial_block_txmiohodmbAmEmi:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_txmiohodmbAmEmi:

	orq	%r8,%r8
	je	.L_after_reduction_txmiohodmbAmEmi
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_txmiohodmbAmEmi:
	jmp	.L_last_blocks_done_CvCdFbGmvFadEzD
.L_last_num_blocks_is_11_CvCdFbGmvFadEzD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_leqhnEwCBrxjxGe
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_leqhnEwCBrxjxGe

.L_16_blocks_overflow_leqhnEwCBrxjxGe:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_leqhnEwCBrxjxGe:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_azuyDkogpfDApbx





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_azuyDkogpfDApbx
.L_small_initial_partial_block_azuyDkogpfDApbx:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_azuyDkogpfDApbx:

	orq	%r8,%r8
	je	.L_after_reduction_azuyDkogpfDApbx
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_azuyDkogpfDApbx:
	jmp	.L_last_blocks_done_CvCdFbGmvFadEzD
.L_last_num_blocks_is_12_CvCdFbGmvFadEzD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_GhEssFtFvrabvpj
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_GhEssFtFvrabvpj

.L_16_blocks_overflow_GhEssFtFvrabvpj:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_GhEssFtFvrabvpj:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_fshyjcayhbtzbnp





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_fshyjcayhbtzbnp
.L_small_initial_partial_block_fshyjcayhbtzbnp:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_fshyjcayhbtzbnp:

	orq	%r8,%r8
	je	.L_after_reduction_fshyjcayhbtzbnp
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_fshyjcayhbtzbnp:
	jmp	.L_last_blocks_done_CvCdFbGmvFadEzD
.L_last_num_blocks_is_13_CvCdFbGmvFadEzD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_rdvdnhlvkfjCuph
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_rdvdnhlvkfjCuph

.L_16_blocks_overflow_rdvdnhlvkfjCuph:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_rdvdnhlvkfjCuph:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_pAgfzixAdAjxrCw





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_pAgfzixAdAjxrCw
.L_small_initial_partial_block_pAgfzixAdAjxrCw:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_pAgfzixAdAjxrCw:

	orq	%r8,%r8
	je	.L_after_reduction_pAgfzixAdAjxrCw
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_pAgfzixAdAjxrCw:
	jmp	.L_last_blocks_done_CvCdFbGmvFadEzD
.L_last_num_blocks_is_14_CvCdFbGmvFadEzD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_txifahlqkinweyo
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_txifahlqkinweyo

.L_16_blocks_overflow_txifahlqkinweyo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_txifahlqkinweyo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_BsAdlAxtACfBFDj





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_BsAdlAxtACfBFDj
.L_small_initial_partial_block_BsAdlAxtACfBFDj:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_BsAdlAxtACfBFDj:

	orq	%r8,%r8
	je	.L_after_reduction_BsAdlAxtACfBFDj
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_BsAdlAxtACfBFDj:
	jmp	.L_last_blocks_done_CvCdFbGmvFadEzD
.L_last_num_blocks_is_15_CvCdFbGmvFadEzD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_lthonubcahBuzpx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_lthonubcahBuzpx

.L_16_blocks_overflow_lthonubcahBuzpx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_lthonubcahBuzpx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ybGaszmqiwBhgoE





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ybGaszmqiwBhgoE
.L_small_initial_partial_block_ybGaszmqiwBhgoE:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ybGaszmqiwBhgoE:

	orq	%r8,%r8
	je	.L_after_reduction_ybGaszmqiwBhgoE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ybGaszmqiwBhgoE:
	jmp	.L_last_blocks_done_CvCdFbGmvFadEzD
.L_last_num_blocks_is_16_CvCdFbGmvFadEzD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_AlxBaCinyDvAEyx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_AlxBaCinyDvAEyx

.L_16_blocks_overflow_AlxBaCinyDvAEyx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_AlxBaCinyDvAEyx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_sjmBhgakttusBvz:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_sjmBhgakttusBvz:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_sjmBhgakttusBvz:
	jmp	.L_last_blocks_done_CvCdFbGmvFadEzD
.L_last_num_blocks_is_0_CvCdFbGmvFadEzD:
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_CvCdFbGmvFadEzD:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_yoCwxdbotnlDAxA

.L_message_below_32_blocks_yoCwxdbotnlDAxA:


	subq	$256,%r8
	addq	$256,%rax
	movl	%r8d,%r10d
	leaq	96(%rsi),%r12
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_xsGBFqCxoDBqxgq
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)
.L_skip_hkeys_precomputation_xsGBFqCxoDBqxgq:
	movq	$1,%r14
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_llgECmsnpxFfbeD

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_llgECmsnpxFfbeD
	jb	.L_last_num_blocks_is_7_1_llgECmsnpxFfbeD


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_llgECmsnpxFfbeD
	jb	.L_last_num_blocks_is_11_9_llgECmsnpxFfbeD


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_llgECmsnpxFfbeD
	ja	.L_last_num_blocks_is_16_llgECmsnpxFfbeD
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_llgECmsnpxFfbeD
	jmp	.L_last_num_blocks_is_13_llgECmsnpxFfbeD

.L_last_num_blocks_is_11_9_llgECmsnpxFfbeD:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_llgECmsnpxFfbeD
	ja	.L_last_num_blocks_is_11_llgECmsnpxFfbeD
	jmp	.L_last_num_blocks_is_9_llgECmsnpxFfbeD

.L_last_num_blocks_is_7_1_llgECmsnpxFfbeD:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_llgECmsnpxFfbeD
	jb	.L_last_num_blocks_is_3_1_llgECmsnpxFfbeD

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_llgECmsnpxFfbeD
	je	.L_last_num_blocks_is_6_llgECmsnpxFfbeD
	jmp	.L_last_num_blocks_is_5_llgECmsnpxFfbeD

.L_last_num_blocks_is_3_1_llgECmsnpxFfbeD:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_llgECmsnpxFfbeD
	je	.L_last_num_blocks_is_2_llgECmsnpxFfbeD
.L_last_num_blocks_is_1_llgECmsnpxFfbeD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_ijacuiAhudzkiBB
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_ijacuiAhudzkiBB

.L_16_blocks_overflow_ijacuiAhudzkiBB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_ijacuiAhudzkiBB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_pnlxplrnidEwjDF





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_pnlxplrnidEwjDF
.L_small_initial_partial_block_pnlxplrnidEwjDF:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_pnlxplrnidEwjDF
.L_small_initial_compute_done_pnlxplrnidEwjDF:
.L_after_reduction_pnlxplrnidEwjDF:
	jmp	.L_last_blocks_done_llgECmsnpxFfbeD
.L_last_num_blocks_is_2_llgECmsnpxFfbeD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_GpfrtghhmdleouF
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_GpfrtghhmdleouF

.L_16_blocks_overflow_GpfrtghhmdleouF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_GpfrtghhmdleouF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zEkvfachsfvtovu





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zEkvfachsfvtovu
.L_small_initial_partial_block_zEkvfachsfvtovu:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zEkvfachsfvtovu:

	orq	%r8,%r8
	je	.L_after_reduction_zEkvfachsfvtovu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_zEkvfachsfvtovu:
	jmp	.L_last_blocks_done_llgECmsnpxFfbeD
.L_last_num_blocks_is_3_llgECmsnpxFfbeD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_DhlwxFzavGBkwfm
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_DhlwxFzavGBkwfm

.L_16_blocks_overflow_DhlwxFzavGBkwfm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_DhlwxFzavGBkwfm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_wqhGDusEfnsrkcr





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_wqhGDusEfnsrkcr
.L_small_initial_partial_block_wqhGDusEfnsrkcr:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_wqhGDusEfnsrkcr:

	orq	%r8,%r8
	je	.L_after_reduction_wqhGDusEfnsrkcr
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_wqhGDusEfnsrkcr:
	jmp	.L_last_blocks_done_llgECmsnpxFfbeD
.L_last_num_blocks_is_4_llgECmsnpxFfbeD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_bAaElqpbacsuqhl
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_bAaElqpbacsuqhl

.L_16_blocks_overflow_bAaElqpbacsuqhl:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_bAaElqpbacsuqhl:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vddzcDAqlvjopuw





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vddzcDAqlvjopuw
.L_small_initial_partial_block_vddzcDAqlvjopuw:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_vddzcDAqlvjopuw:

	orq	%r8,%r8
	je	.L_after_reduction_vddzcDAqlvjopuw
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_vddzcDAqlvjopuw:
	jmp	.L_last_blocks_done_llgECmsnpxFfbeD
.L_last_num_blocks_is_5_llgECmsnpxFfbeD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_tnrdhygaizuFgck
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_tnrdhygaizuFgck

.L_16_blocks_overflow_tnrdhygaizuFgck:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_tnrdhygaizuFgck:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_igpFgrFhmoEeGcd





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_igpFgrFhmoEeGcd
.L_small_initial_partial_block_igpFgrFhmoEeGcd:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_igpFgrFhmoEeGcd:

	orq	%r8,%r8
	je	.L_after_reduction_igpFgrFhmoEeGcd
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_igpFgrFhmoEeGcd:
	jmp	.L_last_blocks_done_llgECmsnpxFfbeD
.L_last_num_blocks_is_6_llgECmsnpxFfbeD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_wxnErcpcCmEEfys
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_wxnErcpcCmEEfys

.L_16_blocks_overflow_wxnErcpcCmEEfys:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_wxnErcpcCmEEfys:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_iobtfsnicAEligb





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_iobtfsnicAEligb
.L_small_initial_partial_block_iobtfsnicAEligb:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_iobtfsnicAEligb:

	orq	%r8,%r8
	je	.L_after_reduction_iobtfsnicAEligb
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_iobtfsnicAEligb:
	jmp	.L_last_blocks_done_llgECmsnpxFfbeD
.L_last_num_blocks_is_7_llgECmsnpxFfbeD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_CkEissqosFodxlu
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_CkEissqosFodxlu

.L_16_blocks_overflow_CkEissqosFodxlu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_CkEissqosFodxlu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cblgqlmxevEiwcF





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cblgqlmxevEiwcF
.L_small_initial_partial_block_cblgqlmxevEiwcF:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cblgqlmxevEiwcF:

	orq	%r8,%r8
	je	.L_after_reduction_cblgqlmxevEiwcF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cblgqlmxevEiwcF:
	jmp	.L_last_blocks_done_llgECmsnpxFfbeD
.L_last_num_blocks_is_8_llgECmsnpxFfbeD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_oGgqonmjBiplkFD
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_oGgqonmjBiplkFD

.L_16_blocks_overflow_oGgqonmjBiplkFD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_oGgqonmjBiplkFD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_oonkumheopenFmi





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_oonkumheopenFmi
.L_small_initial_partial_block_oonkumheopenFmi:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_oonkumheopenFmi:

	orq	%r8,%r8
	je	.L_after_reduction_oonkumheopenFmi
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_oonkumheopenFmi:
	jmp	.L_last_blocks_done_llgECmsnpxFfbeD
.L_last_num_blocks_is_9_llgECmsnpxFfbeD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_ixwmaCfspbigzry
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_ixwmaCfspbigzry

.L_16_blocks_overflow_ixwmaCfspbigzry:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_ixwmaCfspbigzry:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cvmCnvrvtfCGDEF





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cvmCnvrvtfCGDEF
.L_small_initial_partial_block_cvmCnvrvtfCGDEF:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cvmCnvrvtfCGDEF:

	orq	%r8,%r8
	je	.L_after_reduction_cvmCnvrvtfCGDEF
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cvmCnvrvtfCGDEF:
	jmp	.L_last_blocks_done_llgECmsnpxFfbeD
.L_last_num_blocks_is_10_llgECmsnpxFfbeD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_vbACrmGhBiApEDn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_vbACrmGhBiApEDn

.L_16_blocks_overflow_vbACrmGhBiApEDn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_vbACrmGhBiApEDn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_wkCloqysCnmmibG





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_wkCloqysCnmmibG
.L_small_initial_partial_block_wkCloqysCnmmibG:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_wkCloqysCnmmibG:

	orq	%r8,%r8
	je	.L_after_reduction_wkCloqysCnmmibG
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_wkCloqysCnmmibG:
	jmp	.L_last_blocks_done_llgECmsnpxFfbeD
.L_last_num_blocks_is_11_llgECmsnpxFfbeD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_ExlgzpBcAdGcBbG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_ExlgzpBcAdGcBbG

.L_16_blocks_overflow_ExlgzpBcAdGcBbG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_ExlgzpBcAdGcBbG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nAsFFghjgdpFcjg





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nAsFFghjgdpFcjg
.L_small_initial_partial_block_nAsFFghjgdpFcjg:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nAsFFghjgdpFcjg:

	orq	%r8,%r8
	je	.L_after_reduction_nAsFFghjgdpFcjg
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_nAsFFghjgdpFcjg:
	jmp	.L_last_blocks_done_llgECmsnpxFfbeD
.L_last_num_blocks_is_12_llgECmsnpxFfbeD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_vcdmCkFbjejAage
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_vcdmCkFbjejAage

.L_16_blocks_overflow_vcdmCkFbjejAage:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_vcdmCkFbjejAage:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rCbCusGEekzahxt





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rCbCusGEekzahxt
.L_small_initial_partial_block_rCbCusGEekzahxt:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rCbCusGEekzahxt:

	orq	%r8,%r8
	je	.L_after_reduction_rCbCusGEekzahxt
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_rCbCusGEekzahxt:
	jmp	.L_last_blocks_done_llgECmsnpxFfbeD
.L_last_num_blocks_is_13_llgECmsnpxFfbeD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_DbphayuEFkwadui
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_DbphayuEFkwadui

.L_16_blocks_overflow_DbphayuEFkwadui:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_DbphayuEFkwadui:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vBadexDrvhogqCh





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vBadexDrvhogqCh
.L_small_initial_partial_block_vBadexDrvhogqCh:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_vBadexDrvhogqCh:

	orq	%r8,%r8
	je	.L_after_reduction_vBadexDrvhogqCh
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_vBadexDrvhogqCh:
	jmp	.L_last_blocks_done_llgECmsnpxFfbeD
.L_last_num_blocks_is_14_llgECmsnpxFfbeD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_GBjizuqrhnEnuAd
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_GBjizuqrhnEnuAd

.L_16_blocks_overflow_GBjizuqrhnEnuAd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_GBjizuqrhnEnuAd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DcBsxCGmnqfbvra





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DcBsxCGmnqfbvra
.L_small_initial_partial_block_DcBsxCGmnqfbvra:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DcBsxCGmnqfbvra:

	orq	%r8,%r8
	je	.L_after_reduction_DcBsxCGmnqfbvra
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DcBsxCGmnqfbvra:
	jmp	.L_last_blocks_done_llgECmsnpxFfbeD
.L_last_num_blocks_is_15_llgECmsnpxFfbeD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_agEoxmyuurjgxGt
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_agEoxmyuurjgxGt

.L_16_blocks_overflow_agEoxmyuurjgxGt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_agEoxmyuurjgxGt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_pkmGoreyFBvkluz





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_pkmGoreyFBvkluz
.L_small_initial_partial_block_pkmGoreyFBvkluz:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_pkmGoreyFBvkluz:

	orq	%r8,%r8
	je	.L_after_reduction_pkmGoreyFBvkluz
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_pkmGoreyFBvkluz:
	jmp	.L_last_blocks_done_llgECmsnpxFfbeD
.L_last_num_blocks_is_16_llgECmsnpxFfbeD:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_sFouusDufliCjDx
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_sFouusDufliCjDx

.L_16_blocks_overflow_sFouusDufliCjDx:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_sFouusDufliCjDx:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_etwqqamspxBfege:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_etwqqamspxBfege:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_etwqqamspxBfege:
	jmp	.L_last_blocks_done_llgECmsnpxFfbeD
.L_last_num_blocks_is_0_llgECmsnpxFfbeD:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_llgECmsnpxFfbeD:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_yoCwxdbotnlDAxA

.L_message_below_equal_16_blocks_yoCwxdbotnlDAxA:


	movl	%r8d,%r12d
	addl	$15,%r12d
	shrl	$4,%r12d
	cmpq	$8,%r12
	je	.L_small_initial_num_blocks_is_8_pqeGopBEEbmoAzg
	jl	.L_small_initial_num_blocks_is_7_1_pqeGopBEEbmoAzg


	cmpq	$12,%r12
	je	.L_small_initial_num_blocks_is_12_pqeGopBEEbmoAzg
	jl	.L_small_initial_num_blocks_is_11_9_pqeGopBEEbmoAzg


	cmpq	$16,%r12
	je	.L_small_initial_num_blocks_is_16_pqeGopBEEbmoAzg
	cmpq	$15,%r12
	je	.L_small_initial_num_blocks_is_15_pqeGopBEEbmoAzg
	cmpq	$14,%r12
	je	.L_small_initial_num_blocks_is_14_pqeGopBEEbmoAzg
	jmp	.L_small_initial_num_blocks_is_13_pqeGopBEEbmoAzg

.L_small_initial_num_blocks_is_11_9_pqeGopBEEbmoAzg:

	cmpq	$11,%r12
	je	.L_small_initial_num_blocks_is_11_pqeGopBEEbmoAzg
	cmpq	$10,%r12
	je	.L_small_initial_num_blocks_is_10_pqeGopBEEbmoAzg
	jmp	.L_small_initial_num_blocks_is_9_pqeGopBEEbmoAzg

.L_small_initial_num_blocks_is_7_1_pqeGopBEEbmoAzg:
	cmpq	$4,%r12
	je	.L_small_initial_num_blocks_is_4_pqeGopBEEbmoAzg
	jl	.L_small_initial_num_blocks_is_3_1_pqeGopBEEbmoAzg

	cmpq	$7,%r12
	je	.L_small_initial_num_blocks_is_7_pqeGopBEEbmoAzg
	cmpq	$6,%r12
	je	.L_small_initial_num_blocks_is_6_pqeGopBEEbmoAzg
	jmp	.L_small_initial_num_blocks_is_5_pqeGopBEEbmoAzg

.L_small_initial_num_blocks_is_3_1_pqeGopBEEbmoAzg:

	cmpq	$3,%r12
	je	.L_small_initial_num_blocks_is_3_pqeGopBEEbmoAzg
	cmpq	$2,%r12
	je	.L_small_initial_num_blocks_is_2_pqeGopBEEbmoAzg





.L_small_initial_num_blocks_is_1_pqeGopBEEbmoAzg:
	vmovdqa64	SHUF_MASK(%rip),%xmm29
	vpaddd	ONE(%rip),%xmm2,%xmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm0,%xmm2
	vpshufb	%xmm29,%xmm0,%xmm0
	vmovdqu8	0(%rcx,%rax,1),%xmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%xmm15,%xmm0,%xmm0
	vpxorq	%xmm6,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm6,%xmm6
	vextracti32x4	$0,%zmm6,%xmm13
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_Dhwkblvvtvjbeii





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_Dhwkblvvtvjbeii
.L_small_initial_partial_block_Dhwkblvvtvjbeii:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)











	vpxorq	%xmm13,%xmm14,%xmm14

	jmp	.L_after_reduction_Dhwkblvvtvjbeii
.L_small_initial_compute_done_Dhwkblvvtvjbeii:
.L_after_reduction_Dhwkblvvtvjbeii:
	jmp	.L_small_initial_blocks_encrypted_pqeGopBEEbmoAzg
.L_small_initial_num_blocks_is_2_pqeGopBEEbmoAzg:
	vmovdqa64	SHUF_MASK(%rip),%ymm29
	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm0,%xmm2
	vpshufb	%ymm29,%ymm0,%ymm0
	vmovdqu8	0(%rcx,%rax,1),%ymm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%ymm15,%ymm0,%ymm0
	vpxorq	%ymm6,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm6,%ymm6
	vextracti32x4	$1,%zmm6,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_whGmDAibssvkvsG





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_whGmDAibssvkvsG
.L_small_initial_partial_block_whGmDAibssvkvsG:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_whGmDAibssvkvsG:

	orq	%r8,%r8
	je	.L_after_reduction_whGmDAibssvkvsG
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_whGmDAibssvkvsG:
	jmp	.L_small_initial_blocks_encrypted_pqeGopBEEbmoAzg
.L_small_initial_num_blocks_is_3_pqeGopBEEbmoAzg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vextracti32x4	$2,%zmm6,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_BjGpxufqulupEmw





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_BjGpxufqulupEmw
.L_small_initial_partial_block_BjGpxufqulupEmw:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_BjGpxufqulupEmw:

	orq	%r8,%r8
	je	.L_after_reduction_BjGpxufqulupEmw
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_BjGpxufqulupEmw:
	jmp	.L_small_initial_blocks_encrypted_pqeGopBEEbmoAzg
.L_small_initial_num_blocks_is_4_pqeGopBEEbmoAzg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vextracti32x4	$3,%zmm6,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nhoBbipzvljekhA





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nhoBbipzvljekhA
.L_small_initial_partial_block_nhoBbipzvljekhA:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nhoBbipzvljekhA:

	orq	%r8,%r8
	je	.L_after_reduction_nhoBbipzvljekhA
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_nhoBbipzvljekhA:
	jmp	.L_small_initial_blocks_encrypted_pqeGopBEEbmoAzg
.L_small_initial_num_blocks_is_5_pqeGopBEEbmoAzg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%xmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%xmm15,%xmm3,%xmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%xmm7,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%xmm29,%xmm7,%xmm7
	vextracti32x4	$0,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CnuqlFqvwAdsEhB





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CnuqlFqvwAdsEhB
.L_small_initial_partial_block_CnuqlFqvwAdsEhB:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CnuqlFqvwAdsEhB:

	orq	%r8,%r8
	je	.L_after_reduction_CnuqlFqvwAdsEhB
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_CnuqlFqvwAdsEhB:
	jmp	.L_small_initial_blocks_encrypted_pqeGopBEEbmoAzg
.L_small_initial_num_blocks_is_6_pqeGopBEEbmoAzg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%ymm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%ymm15,%ymm3,%ymm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%ymm7,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%ymm29,%ymm7,%ymm7
	vextracti32x4	$1,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_GkrkqelfikdnkzC





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_GkrkqelfikdnkzC
.L_small_initial_partial_block_GkrkqelfikdnkzC:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_GkrkqelfikdnkzC:

	orq	%r8,%r8
	je	.L_after_reduction_GkrkqelfikdnkzC
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_GkrkqelfikdnkzC:
	jmp	.L_small_initial_blocks_encrypted_pqeGopBEEbmoAzg
.L_small_initial_num_blocks_is_7_pqeGopBEEbmoAzg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vextracti32x4	$2,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_tjhpwcwthiotcni





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_tjhpwcwthiotcni
.L_small_initial_partial_block_tjhpwcwthiotcni:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_tjhpwcwthiotcni:

	orq	%r8,%r8
	je	.L_after_reduction_tjhpwcwthiotcni
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_tjhpwcwthiotcni:
	jmp	.L_small_initial_blocks_encrypted_pqeGopBEEbmoAzg
.L_small_initial_num_blocks_is_8_pqeGopBEEbmoAzg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vextracti32x4	$3,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qtCziegCuAefenl





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qtCziegCuAefenl
.L_small_initial_partial_block_qtCziegCuAefenl:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qtCziegCuAefenl:

	orq	%r8,%r8
	je	.L_after_reduction_qtCziegCuAefenl
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_qtCziegCuAefenl:
	jmp	.L_small_initial_blocks_encrypted_pqeGopBEEbmoAzg
.L_small_initial_num_blocks_is_9_pqeGopBEEbmoAzg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%xmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%xmm15,%xmm4,%xmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%xmm10,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%xmm29,%xmm10,%xmm10
	vextracti32x4	$0,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vpasrEgylnqGxkn





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vpasrEgylnqGxkn
.L_small_initial_partial_block_vpasrEgylnqGxkn:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_vpasrEgylnqGxkn:

	orq	%r8,%r8
	je	.L_after_reduction_vpasrEgylnqGxkn
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_vpasrEgylnqGxkn:
	jmp	.L_small_initial_blocks_encrypted_pqeGopBEEbmoAzg
.L_small_initial_num_blocks_is_10_pqeGopBEEbmoAzg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%ymm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%ymm15,%ymm4,%ymm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%ymm10,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%ymm29,%ymm10,%ymm10
	vextracti32x4	$1,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vwvckrueDApqqjo





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vwvckrueDApqqjo
.L_small_initial_partial_block_vwvckrueDApqqjo:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_vwvckrueDApqqjo:

	orq	%r8,%r8
	je	.L_after_reduction_vwvckrueDApqqjo
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_vwvckrueDApqqjo:
	jmp	.L_small_initial_blocks_encrypted_pqeGopBEEbmoAzg
.L_small_initial_num_blocks_is_11_pqeGopBEEbmoAzg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vextracti32x4	$2,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nrGxaBDwvefwvyh





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nrGxaBDwvefwvyh
.L_small_initial_partial_block_nrGxaBDwvefwvyh:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nrGxaBDwvefwvyh:

	orq	%r8,%r8
	je	.L_after_reduction_nrGxaBDwvefwvyh
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_nrGxaBDwvefwvyh:
	jmp	.L_small_initial_blocks_encrypted_pqeGopBEEbmoAzg
.L_small_initial_num_blocks_is_12_pqeGopBEEbmoAzg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vextracti32x4	$3,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cqnqEejeAmfgkrs





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	64(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cqnqEejeAmfgkrs
.L_small_initial_partial_block_cqnqEejeAmfgkrs:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cqnqEejeAmfgkrs:

	orq	%r8,%r8
	je	.L_after_reduction_cqnqEejeAmfgkrs
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_cqnqEejeAmfgkrs:
	jmp	.L_small_initial_blocks_encrypted_pqeGopBEEbmoAzg
.L_small_initial_num_blocks_is_13_pqeGopBEEbmoAzg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%xmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%xmm15,%xmm5,%xmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%xmm11,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%xmm29,%xmm11,%xmm11
	vextracti32x4	$0,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vCkwtGadouoEEwv





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	48(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vCkwtGadouoEEwv
.L_small_initial_partial_block_vCkwtGadouoEEwv:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	64(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_vCkwtGadouoEEwv:

	orq	%r8,%r8
	je	.L_after_reduction_vCkwtGadouoEEwv
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_vCkwtGadouoEEwv:
	jmp	.L_small_initial_blocks_encrypted_pqeGopBEEbmoAzg
.L_small_initial_num_blocks_is_14_pqeGopBEEbmoAzg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%ymm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%ymm15,%ymm5,%ymm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%ymm11,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%ymm29,%ymm11,%ymm11
	vextracti32x4	$1,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_mrmBlFfuEifiCfE





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	32(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_mrmBlFfuEifiCfE
.L_small_initial_partial_block_mrmBlFfuEifiCfE:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	48(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_mrmBlFfuEifiCfE:

	orq	%r8,%r8
	je	.L_after_reduction_mrmBlFfuEifiCfE
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_mrmBlFfuEifiCfE:
	jmp	.L_small_initial_blocks_encrypted_pqeGopBEEbmoAzg
.L_small_initial_num_blocks_is_15_pqeGopBEEbmoAzg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vextracti32x4	$2,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ezyjbtktzwndmEr





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	16(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ezyjbtktzwndmEr
.L_small_initial_partial_block_ezyjbtktzwndmEr:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	32(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ezyjbtktzwndmEr:

	orq	%r8,%r8
	je	.L_after_reduction_ezyjbtktzwndmEr
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_ezyjbtktzwndmEr:
	jmp	.L_small_initial_blocks_encrypted_pqeGopBEEbmoAzg
.L_small_initial_num_blocks_is_16_pqeGopBEEbmoAzg:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vextracti32x4	$3,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_cCmabynDqujksyv:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	16(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cCmabynDqujksyv:
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_cCmabynDqujksyv:
.L_small_initial_blocks_encrypted_pqeGopBEEbmoAzg:
.L_ghash_done_yoCwxdbotnlDAxA:
	vmovdqu64	%xmm2,0(%rsi)
.L_enc_dec_done_yoCwxdbotnlDAxA:

	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14
	vmovdqu64	%xmm14,64(%rsi)
.L_enc_dec_abort_yoCwxdbotnlDAxA:
	jmp	.Lexit_gcm_decrypt
.align	32
.Laes_gcm_decrypt_256_avx512:
	orq	%r8,%r8
	je	.L_enc_dec_abort_eFCuDlbftGnAEad
	xorq	%r14,%r14
	vmovdqu64	64(%rsi),%xmm14
	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14

	movl	(%rdx),%eax
	orq	%rax,%rax
	je	.L_partial_block_done_bEAmukgfGbkidBC
	movl	$16,%r10d
	leaq	byte_len_to_mask_table(%rip),%r12
	cmpq	%r10,%r8
	cmovcq	%r8,%r10
	kmovw	(%r12,%r10,2),%k1
	vmovdqu8	(%rcx),%xmm0{%k1}{z}

	vmovdqu64	16(%rsi),%xmm3

	leaq	96(%rsi),%r10
	vmovdqu64	240(%r10),%xmm4



	leaq	SHIFT_MASK(%rip),%r12
	addq	%rax,%r12
	vmovdqu64	(%r12),%xmm5
	vpshufb	%xmm5,%xmm3,%xmm3

	vmovdqa64	%xmm0,%xmm6
	vpxorq	%xmm0,%xmm3,%xmm3


	leaq	(%r8,%rax,1),%r13
	subq	$16,%r13
	jge	.L_no_extra_mask_bEAmukgfGbkidBC
	subq	%r13,%r12
.L_no_extra_mask_bEAmukgfGbkidBC:



	vmovdqu64	16(%r12),%xmm0
	vpand	%xmm0,%xmm3,%xmm3
	vpand	%xmm0,%xmm6,%xmm6
	vpshufb	SHUF_MASK(%rip),%xmm6,%xmm6
	vpshufb	%xmm5,%xmm6,%xmm6
	vpxorq	%xmm6,%xmm14,%xmm14
	cmpq	$0,%r13
	jl	.L_partial_incomplete_bEAmukgfGbkidBC

	vpclmulqdq	$0x11,%xmm4,%xmm14,%xmm7
	vpclmulqdq	$0x00,%xmm4,%xmm14,%xmm10
	vpclmulqdq	$0x01,%xmm4,%xmm14,%xmm11
	vpclmulqdq	$0x10,%xmm4,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm14,%xmm14

	vpsrldq	$8,%xmm14,%xmm11
	vpslldq	$8,%xmm14,%xmm14
	vpxorq	%xmm11,%xmm7,%xmm7
	vpxorq	%xmm10,%xmm14,%xmm14



	vmovdqu64	POLY2(%rip),%xmm11

	vpclmulqdq	$0x01,%xmm14,%xmm11,%xmm10
	vpslldq	$8,%xmm10,%xmm10
	vpxorq	%xmm10,%xmm14,%xmm14



	vpclmulqdq	$0x00,%xmm14,%xmm11,%xmm10
	vpsrldq	$4,%xmm10,%xmm10
	vpclmulqdq	$0x10,%xmm14,%xmm11,%xmm14
	vpslldq	$4,%xmm14,%xmm14

	vpternlogq	$0x96,%xmm10,%xmm7,%xmm14

	movl	$0,(%rdx)

	movq	%rax,%r12
	movq	$16,%rax
	subq	%r12,%rax
	jmp	.L_enc_dec_done_bEAmukgfGbkidBC

.L_partial_incomplete_bEAmukgfGbkidBC:
	addl	%r8d,(%rdx)
	movq	%r8,%rax

.L_enc_dec_done_bEAmukgfGbkidBC:


	leaq	byte_len_to_mask_table(%rip),%r12
	kmovw	(%r12,%rax,2),%k1
	movq	%r9,%r12
	vmovdqu8	%xmm3,(%r12){%k1}
.L_partial_block_done_bEAmukgfGbkidBC:
	vmovdqu64	0(%rsi),%xmm2
	subq	%rax,%r8
	je	.L_enc_dec_done_eFCuDlbftGnAEad
	cmpq	$256,%r8
	jbe	.L_message_below_equal_16_blocks_eFCuDlbftGnAEad

	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vmovdqa64	ddq_addbe_4444(%rip),%zmm27
	vmovdqa64	ddq_addbe_1234(%rip),%zmm28






	vmovd	%xmm2,%r15d
	andl	$255,%r15d

	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpshufb	%zmm29,%zmm2,%zmm2



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_llbapmhckhcAjCs
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_llbapmhckhcAjCs
.L_next_16_overflow_llbapmhckhcAjCs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_llbapmhckhcAjCs:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	0(%rcx,%rax,1),%zmm0
	vmovdqu8	64(%rcx,%rax,1),%zmm3
	vmovdqu8	128(%rcx,%rax,1),%zmm4
	vmovdqu8	192(%rcx,%rax,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	208(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	224(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,0(%r10,%rax,1)
	vmovdqu8	%zmm10,64(%r10,%rax,1)
	vmovdqu8	%zmm11,128(%r10,%rax,1)
	vmovdqu8	%zmm12,192(%r10,%rax,1)

	vpshufb	%zmm29,%zmm0,%zmm7
	vpshufb	%zmm29,%zmm3,%zmm10
	vpshufb	%zmm29,%zmm4,%zmm11
	vpshufb	%zmm29,%zmm5,%zmm12
	vmovdqa64	%zmm7,768(%rsp)
	vmovdqa64	%zmm10,832(%rsp)
	vmovdqa64	%zmm11,896(%rsp)
	vmovdqa64	%zmm12,960(%rsp)
	leaq	96(%rsi),%r12
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_gslwslwnlmznhAt

	vmovdqu64	192(%r12),%zmm0
	vmovdqu64	%zmm0,704(%rsp)

	vmovdqu64	128(%r12),%zmm3
	vmovdqu64	%zmm3,640(%rsp)


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	64(%r12),%zmm4
	vmovdqu64	%zmm4,576(%rsp)

	vmovdqu64	0(%r12),%zmm5
	vmovdqu64	%zmm5,512(%rsp)
.L_skip_hkeys_precomputation_gslwslwnlmznhAt:
	cmpq	$512,%r8
	jb	.L_message_below_32_blocks_eFCuDlbftGnAEad



	cmpb	$240,%r15b
	jae	.L_next_16_overflow_mDyyuCgorshGmDd
	vpaddd	%zmm28,%zmm2,%zmm7
	vpaddd	%zmm27,%zmm7,%zmm10
	vpaddd	%zmm27,%zmm10,%zmm11
	vpaddd	%zmm27,%zmm11,%zmm12
	jmp	.L_next_16_ok_mDyyuCgorshGmDd
.L_next_16_overflow_mDyyuCgorshGmDd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm12
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm7
	vpaddd	%zmm12,%zmm7,%zmm10
	vpaddd	%zmm12,%zmm10,%zmm11
	vpaddd	%zmm12,%zmm11,%zmm12
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vpshufb	%zmm29,%zmm12,%zmm12
.L_next_16_ok_mDyyuCgorshGmDd:
	vshufi64x2	$255,%zmm12,%zmm12,%zmm2
	addb	$16,%r15b

	vmovdqu8	256(%rcx,%rax,1),%zmm0
	vmovdqu8	320(%rcx,%rax,1),%zmm3
	vmovdqu8	384(%rcx,%rax,1),%zmm4
	vmovdqu8	448(%rcx,%rax,1),%zmm5


	vbroadcastf64x2	0(%rdi),%zmm6
	vpxorq	%zmm6,%zmm7,%zmm7
	vpxorq	%zmm6,%zmm10,%zmm10
	vpxorq	%zmm6,%zmm11,%zmm11
	vpxorq	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	16(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	32(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	48(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	64(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	80(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	96(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	112(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	128(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	144(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	160(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	176(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	192(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	208(%rdi),%zmm6
	vaesenc	%zmm6,%zmm7,%zmm7
	vaesenc	%zmm6,%zmm10,%zmm10
	vaesenc	%zmm6,%zmm11,%zmm11
	vaesenc	%zmm6,%zmm12,%zmm12
	vbroadcastf64x2	224(%rdi),%zmm6
	vaesenclast	%zmm6,%zmm7,%zmm7
	vaesenclast	%zmm6,%zmm10,%zmm10
	vaesenclast	%zmm6,%zmm11,%zmm11
	vaesenclast	%zmm6,%zmm12,%zmm12


	vpxorq	%zmm0,%zmm7,%zmm7
	vpxorq	%zmm3,%zmm10,%zmm10
	vpxorq	%zmm4,%zmm11,%zmm11
	vpxorq	%zmm5,%zmm12,%zmm12


	movq	%r9,%r10
	vmovdqu8	%zmm7,256(%r10,%rax,1)
	vmovdqu8	%zmm10,320(%r10,%rax,1)
	vmovdqu8	%zmm11,384(%r10,%rax,1)
	vmovdqu8	%zmm12,448(%r10,%rax,1)

	vpshufb	%zmm29,%zmm0,%zmm7
	vpshufb	%zmm29,%zmm3,%zmm10
	vpshufb	%zmm29,%zmm4,%zmm11
	vpshufb	%zmm29,%zmm5,%zmm12
	vmovdqa64	%zmm7,1024(%rsp)
	vmovdqa64	%zmm10,1088(%rsp)
	vmovdqa64	%zmm11,1152(%rsp)
	vmovdqa64	%zmm12,1216(%rsp)
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_CcpcwlxDzFAFcfx
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,192(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,128(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,64(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,0(%rsp)
.L_skip_hkeys_precomputation_CcpcwlxDzFAFcfx:
	movq	$1,%r14
	addq	$512,%rax
	subq	$512,%r8

	cmpq	$768,%r8
	jb	.L_no_more_big_nblocks_eFCuDlbftGnAEad
.L_encrypt_big_nblocks_eFCuDlbftGnAEad:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_FpfjfnBkAvCbkvz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_FpfjfnBkAvCbkvz
.L_16_blocks_overflow_FpfjfnBkAvCbkvz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_FpfjfnBkAvCbkvz:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_mbhEgBvnexGvDru
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_mbhEgBvnexGvDru
.L_16_blocks_overflow_mbhEgBvnexGvDru:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_mbhEgBvnexGvDru:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%rax,1),%zmm17
	vmovdqu8	320(%rcx,%rax,1),%zmm19
	vmovdqu8	384(%rcx,%rax,1),%zmm20
	vmovdqu8	448(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%rax,1)
	vmovdqu8	%zmm3,320(%r10,%rax,1)
	vmovdqu8	%zmm4,384(%r10,%rax,1)
	vmovdqu8	%zmm5,448(%r10,%rax,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_xiDEiFkvxAzxEry
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_xiDEiFkvxAzxEry
.L_16_blocks_overflow_xiDEiFkvxAzxEry:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_xiDEiFkvxAzxEry:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	512(%rcx,%rax,1),%zmm17
	vmovdqu8	576(%rcx,%rax,1),%zmm19
	vmovdqu8	640(%rcx,%rax,1),%zmm20
	vmovdqu8	704(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30


	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10

	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpternlogq	$0x96,%zmm15,%zmm12,%zmm6
	vpxorq	%zmm24,%zmm6,%zmm6
	vpternlogq	$0x96,%zmm10,%zmm13,%zmm7
	vpxorq	%zmm25,%zmm7,%zmm7
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vextracti64x4	$1,%zmm6,%ymm12
	vpxorq	%ymm12,%ymm6,%ymm6
	vextracti32x4	$1,%ymm6,%xmm12
	vpxorq	%xmm12,%xmm6,%xmm6
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm6
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,512(%r10,%rax,1)
	vmovdqu8	%zmm3,576(%r10,%rax,1)
	vmovdqu8	%zmm4,640(%r10,%rax,1)
	vmovdqu8	%zmm5,704(%r10,%rax,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1024(%rsp)
	vmovdqa64	%zmm3,1088(%rsp)
	vmovdqa64	%zmm4,1152(%rsp)
	vmovdqa64	%zmm5,1216(%rsp)
	vmovdqa64	%zmm6,%zmm14

	addq	$768,%rax
	subq	$768,%r8
	cmpq	$768,%r8
	jae	.L_encrypt_big_nblocks_eFCuDlbftGnAEad

.L_no_more_big_nblocks_eFCuDlbftGnAEad:

	cmpq	$512,%r8
	jae	.L_encrypt_32_blocks_eFCuDlbftGnAEad

	cmpq	$256,%r8
	jae	.L_encrypt_16_blocks_eFCuDlbftGnAEad
.L_encrypt_0_blocks_ghash_32_eFCuDlbftGnAEad:
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$256,%ebx
	subl	%r10d,%ebx
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	addl	$256,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_qhogGnfBbgenEmv

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_qhogGnfBbgenEmv
	jb	.L_last_num_blocks_is_7_1_qhogGnfBbgenEmv


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_qhogGnfBbgenEmv
	jb	.L_last_num_blocks_is_11_9_qhogGnfBbgenEmv


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_qhogGnfBbgenEmv
	ja	.L_last_num_blocks_is_16_qhogGnfBbgenEmv
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_qhogGnfBbgenEmv
	jmp	.L_last_num_blocks_is_13_qhogGnfBbgenEmv

.L_last_num_blocks_is_11_9_qhogGnfBbgenEmv:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_qhogGnfBbgenEmv
	ja	.L_last_num_blocks_is_11_qhogGnfBbgenEmv
	jmp	.L_last_num_blocks_is_9_qhogGnfBbgenEmv

.L_last_num_blocks_is_7_1_qhogGnfBbgenEmv:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_qhogGnfBbgenEmv
	jb	.L_last_num_blocks_is_3_1_qhogGnfBbgenEmv

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_qhogGnfBbgenEmv
	je	.L_last_num_blocks_is_6_qhogGnfBbgenEmv
	jmp	.L_last_num_blocks_is_5_qhogGnfBbgenEmv

.L_last_num_blocks_is_3_1_qhogGnfBbgenEmv:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_qhogGnfBbgenEmv
	je	.L_last_num_blocks_is_2_qhogGnfBbgenEmv
.L_last_num_blocks_is_1_qhogGnfBbgenEmv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_FdljfCrwczBafjd
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_FdljfCrwczBafjd

.L_16_blocks_overflow_FdljfCrwczBafjd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_FdljfCrwczBafjd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vqDiixhjgarakDi





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vqDiixhjgarakDi
.L_small_initial_partial_block_vqDiixhjgarakDi:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_vqDiixhjgarakDi
.L_small_initial_compute_done_vqDiixhjgarakDi:
.L_after_reduction_vqDiixhjgarakDi:
	jmp	.L_last_blocks_done_qhogGnfBbgenEmv
.L_last_num_blocks_is_2_qhogGnfBbgenEmv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_vjaBAanbfzBkfgb
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_vjaBAanbfzBkfgb

.L_16_blocks_overflow_vjaBAanbfzBkfgb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_vjaBAanbfzBkfgb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_BqBuFGCvGsdjctD





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_BqBuFGCvGsdjctD
.L_small_initial_partial_block_BqBuFGCvGsdjctD:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_BqBuFGCvGsdjctD:

	orq	%r8,%r8
	je	.L_after_reduction_BqBuFGCvGsdjctD
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_BqBuFGCvGsdjctD:
	jmp	.L_last_blocks_done_qhogGnfBbgenEmv
.L_last_num_blocks_is_3_qhogGnfBbgenEmv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_FbFfzrxpgtBmstm
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_FbFfzrxpgtBmstm

.L_16_blocks_overflow_FbFfzrxpgtBmstm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_FbFfzrxpgtBmstm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ffFohailxlpDjok





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ffFohailxlpDjok
.L_small_initial_partial_block_ffFohailxlpDjok:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ffFohailxlpDjok:

	orq	%r8,%r8
	je	.L_after_reduction_ffFohailxlpDjok
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ffFohailxlpDjok:
	jmp	.L_last_blocks_done_qhogGnfBbgenEmv
.L_last_num_blocks_is_4_qhogGnfBbgenEmv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_ypfiaqqGAzjgrrm
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_ypfiaqqGAzjgrrm

.L_16_blocks_overflow_ypfiaqqGAzjgrrm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_ypfiaqqGAzjgrrm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_kvskAdogEfrmumu





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_kvskAdogEfrmumu
.L_small_initial_partial_block_kvskAdogEfrmumu:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_kvskAdogEfrmumu:

	orq	%r8,%r8
	je	.L_after_reduction_kvskAdogEfrmumu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_kvskAdogEfrmumu:
	jmp	.L_last_blocks_done_qhogGnfBbgenEmv
.L_last_num_blocks_is_5_qhogGnfBbgenEmv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_DDarEgkvDEEygEG
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_DDarEgkvDEEygEG

.L_16_blocks_overflow_DDarEgkvDEEygEG:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_DDarEgkvDEEygEG:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_cdowjgDrgwbsCeE





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_cdowjgDrgwbsCeE
.L_small_initial_partial_block_cdowjgDrgwbsCeE:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_cdowjgDrgwbsCeE:

	orq	%r8,%r8
	je	.L_after_reduction_cdowjgDrgwbsCeE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_cdowjgDrgwbsCeE:
	jmp	.L_last_blocks_done_qhogGnfBbgenEmv
.L_last_num_blocks_is_6_qhogGnfBbgenEmv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_ngvuCqEAnbhBbjk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_ngvuCqEAnbhBbjk

.L_16_blocks_overflow_ngvuCqEAnbhBbjk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_ngvuCqEAnbhBbjk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vCgdbympEyEnjiu





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vCgdbympEyEnjiu
.L_small_initial_partial_block_vCgdbympEyEnjiu:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_vCgdbympEyEnjiu:

	orq	%r8,%r8
	je	.L_after_reduction_vCgdbympEyEnjiu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_vCgdbympEyEnjiu:
	jmp	.L_last_blocks_done_qhogGnfBbgenEmv
.L_last_num_blocks_is_7_qhogGnfBbgenEmv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_hBoczsyeudnvytA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_hBoczsyeudnvytA

.L_16_blocks_overflow_hBoczsyeudnvytA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_hBoczsyeudnvytA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_saDpxirogBandAy





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_saDpxirogBandAy
.L_small_initial_partial_block_saDpxirogBandAy:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_saDpxirogBandAy:

	orq	%r8,%r8
	je	.L_after_reduction_saDpxirogBandAy
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_saDpxirogBandAy:
	jmp	.L_last_blocks_done_qhogGnfBbgenEmv
.L_last_num_blocks_is_8_qhogGnfBbgenEmv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_duefucieyAcmjhA
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_duefucieyAcmjhA

.L_16_blocks_overflow_duefucieyAcmjhA:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_duefucieyAcmjhA:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zlyxtflbEtDGbox





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zlyxtflbEtDGbox
.L_small_initial_partial_block_zlyxtflbEtDGbox:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zlyxtflbEtDGbox:

	orq	%r8,%r8
	je	.L_after_reduction_zlyxtflbEtDGbox
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_zlyxtflbEtDGbox:
	jmp	.L_last_blocks_done_qhogGnfBbgenEmv
.L_last_num_blocks_is_9_qhogGnfBbgenEmv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_tmrcxgnumFDedvt
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_tmrcxgnumFDedvt

.L_16_blocks_overflow_tmrcxgnumFDedvt:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_tmrcxgnumFDedvt:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ejdmorenkFidgth





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ejdmorenkFidgth
.L_small_initial_partial_block_ejdmorenkFidgth:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ejdmorenkFidgth:

	orq	%r8,%r8
	je	.L_after_reduction_ejdmorenkFidgth
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ejdmorenkFidgth:
	jmp	.L_last_blocks_done_qhogGnfBbgenEmv
.L_last_num_blocks_is_10_qhogGnfBbgenEmv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_fgthgClEDniyllo
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_fgthgClEDniyllo

.L_16_blocks_overflow_fgthgClEDniyllo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_fgthgClEDniyllo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CtydBxFdinhGhtq





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CtydBxFdinhGhtq
.L_small_initial_partial_block_CtydBxFdinhGhtq:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CtydBxFdinhGhtq:

	orq	%r8,%r8
	je	.L_after_reduction_CtydBxFdinhGhtq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_CtydBxFdinhGhtq:
	jmp	.L_last_blocks_done_qhogGnfBbgenEmv
.L_last_num_blocks_is_11_qhogGnfBbgenEmv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_jFqAgoyartdnhFh
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_jFqAgoyartdnhFh

.L_16_blocks_overflow_jFqAgoyartdnhFh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_jFqAgoyartdnhFh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ElcgntnadfirghE





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ElcgntnadfirghE
.L_small_initial_partial_block_ElcgntnadfirghE:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ElcgntnadfirghE:

	orq	%r8,%r8
	je	.L_after_reduction_ElcgntnadfirghE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ElcgntnadfirghE:
	jmp	.L_last_blocks_done_qhogGnfBbgenEmv
.L_last_num_blocks_is_12_qhogGnfBbgenEmv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_hlwBvgsgqBeCmng
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_hlwBvgsgqBeCmng

.L_16_blocks_overflow_hlwBvgsgqBeCmng:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_hlwBvgsgqBeCmng:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vDgoAamDwazxghf





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vDgoAamDwazxghf
.L_small_initial_partial_block_vDgoAamDwazxghf:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_vDgoAamDwazxghf:

	orq	%r8,%r8
	je	.L_after_reduction_vDgoAamDwazxghf
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_vDgoAamDwazxghf:
	jmp	.L_last_blocks_done_qhogGnfBbgenEmv
.L_last_num_blocks_is_13_qhogGnfBbgenEmv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_CBkdkEBtEhnfrGm
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_CBkdkEBtEhnfrGm

.L_16_blocks_overflow_CBkdkEBtEhnfrGm:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_CBkdkEBtEhnfrGm:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bEDretalCAEEhvh





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bEDretalCAEEhvh
.L_small_initial_partial_block_bEDretalCAEEhvh:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bEDretalCAEEhvh:

	orq	%r8,%r8
	je	.L_after_reduction_bEDretalCAEEhvh
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_bEDretalCAEEhvh:
	jmp	.L_last_blocks_done_qhogGnfBbgenEmv
.L_last_num_blocks_is_14_qhogGnfBbgenEmv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_xsGzadolshjgEty
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_xsGzadolshjgEty

.L_16_blocks_overflow_xsGzadolshjgEty:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_xsGzadolshjgEty:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_wbodixchrkhcFgE





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_wbodixchrkhcFgE
.L_small_initial_partial_block_wbodixchrkhcFgE:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_wbodixchrkhcFgE:

	orq	%r8,%r8
	je	.L_after_reduction_wbodixchrkhcFgE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_wbodixchrkhcFgE:
	jmp	.L_last_blocks_done_qhogGnfBbgenEmv
.L_last_num_blocks_is_15_qhogGnfBbgenEmv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_saedclBEizAciDd
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_saedclBEizAciDd

.L_16_blocks_overflow_saedclBEizAciDd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_saedclBEizAciDd:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rGCueqhisEGCjmg





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rGCueqhisEGCjmg
.L_small_initial_partial_block_rGCueqhisEGCjmg:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rGCueqhisEGCjmg:

	orq	%r8,%r8
	je	.L_after_reduction_rGCueqhisEGCjmg
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_rGCueqhisEGCjmg:
	jmp	.L_last_blocks_done_qhogGnfBbgenEmv
.L_last_num_blocks_is_16_qhogGnfBbgenEmv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_zgchtacghfqtCep
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_zgchtacghfqtCep

.L_16_blocks_overflow_zgchtacghfqtCep:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_zgchtacghfqtCep:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	1088(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	1152(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	1216(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm14,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_lvecllCeunkCFFB:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_lvecllCeunkCFFB:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_lvecllCeunkCFFB:
	jmp	.L_last_blocks_done_qhogGnfBbgenEmv
.L_last_num_blocks_is_0_qhogGnfBbgenEmv:
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_qhogGnfBbgenEmv:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_eFCuDlbftGnAEad
.L_encrypt_32_blocks_eFCuDlbftGnAEad:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_yDoxleumitbmwBb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_yDoxleumitbmwBb
.L_16_blocks_overflow_yDoxleumitbmwBb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_yDoxleumitbmwBb:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_aFifrcmikFtdffd
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_aFifrcmikFtdffd
.L_16_blocks_overflow_aFifrcmikFtdffd:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_aFifrcmikFtdffd:
	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1024(%rsp),%zmm8
	vmovdqu64	256(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	320(%rsp),%zmm18
	vmovdqa64	1088(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	384(%rsp),%zmm1
	vmovdqa64	1152(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	448(%rsp),%zmm18
	vmovdqa64	1216(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	256(%rcx,%rax,1),%zmm17
	vmovdqu8	320(%rcx,%rax,1),%zmm19
	vmovdqu8	384(%rcx,%rax,1),%zmm20
	vmovdqu8	448(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm15,%zmm10,%zmm26
	vpternlogq	$0x96,%zmm12,%zmm6,%zmm24
	vpternlogq	$0x96,%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,256(%r10,%rax,1)
	vmovdqu8	%zmm3,320(%r10,%rax,1)
	vmovdqu8	%zmm4,384(%r10,%rax,1)
	vmovdqu8	%zmm5,448(%r10,%rax,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,768(%rsp)
	vmovdqa64	%zmm3,832(%rsp)
	vmovdqa64	%zmm4,896(%rsp)
	vmovdqa64	%zmm5,960(%rsp)
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

	subq	$512,%r8
	addq	$512,%rax
	movl	%r8d,%r10d
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_rgrmygdpoyvhfsv

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_rgrmygdpoyvhfsv
	jb	.L_last_num_blocks_is_7_1_rgrmygdpoyvhfsv


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_rgrmygdpoyvhfsv
	jb	.L_last_num_blocks_is_11_9_rgrmygdpoyvhfsv


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_rgrmygdpoyvhfsv
	ja	.L_last_num_blocks_is_16_rgrmygdpoyvhfsv
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_rgrmygdpoyvhfsv
	jmp	.L_last_num_blocks_is_13_rgrmygdpoyvhfsv

.L_last_num_blocks_is_11_9_rgrmygdpoyvhfsv:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_rgrmygdpoyvhfsv
	ja	.L_last_num_blocks_is_11_rgrmygdpoyvhfsv
	jmp	.L_last_num_blocks_is_9_rgrmygdpoyvhfsv

.L_last_num_blocks_is_7_1_rgrmygdpoyvhfsv:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_rgrmygdpoyvhfsv
	jb	.L_last_num_blocks_is_3_1_rgrmygdpoyvhfsv

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_rgrmygdpoyvhfsv
	je	.L_last_num_blocks_is_6_rgrmygdpoyvhfsv
	jmp	.L_last_num_blocks_is_5_rgrmygdpoyvhfsv

.L_last_num_blocks_is_3_1_rgrmygdpoyvhfsv:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_rgrmygdpoyvhfsv
	je	.L_last_num_blocks_is_2_rgrmygdpoyvhfsv
.L_last_num_blocks_is_1_rgrmygdpoyvhfsv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_zeCeDmoFyvbFCoc
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_zeCeDmoFyvbFCoc

.L_16_blocks_overflow_zeCeDmoFyvbFCoc:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_zeCeDmoFyvbFCoc:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zfFDdgFpdGzFjcv





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zfFDdgFpdGzFjcv
.L_small_initial_partial_block_zfFDdgFpdGzFjcv:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_zfFDdgFpdGzFjcv
.L_small_initial_compute_done_zfFDdgFpdGzFjcv:
.L_after_reduction_zfFDdgFpdGzFjcv:
	jmp	.L_last_blocks_done_rgrmygdpoyvhfsv
.L_last_num_blocks_is_2_rgrmygdpoyvhfsv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_pznvyEdsipguiDy
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_pznvyEdsipguiDy

.L_16_blocks_overflow_pznvyEdsipguiDy:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_pznvyEdsipguiDy:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_klcbwuCnvqtumqC





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_klcbwuCnvqtumqC
.L_small_initial_partial_block_klcbwuCnvqtumqC:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_klcbwuCnvqtumqC:

	orq	%r8,%r8
	je	.L_after_reduction_klcbwuCnvqtumqC
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_klcbwuCnvqtumqC:
	jmp	.L_last_blocks_done_rgrmygdpoyvhfsv
.L_last_num_blocks_is_3_rgrmygdpoyvhfsv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_ljGviukgtCygjsu
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_ljGviukgtCygjsu

.L_16_blocks_overflow_ljGviukgtCygjsu:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_ljGviukgtCygjsu:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bfrwqxaGDarBtic





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bfrwqxaGDarBtic
.L_small_initial_partial_block_bfrwqxaGDarBtic:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bfrwqxaGDarBtic:

	orq	%r8,%r8
	je	.L_after_reduction_bfrwqxaGDarBtic
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_bfrwqxaGDarBtic:
	jmp	.L_last_blocks_done_rgrmygdpoyvhfsv
.L_last_num_blocks_is_4_rgrmygdpoyvhfsv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_upECcGpuGpAGBon
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_upECcGpuGpAGBon

.L_16_blocks_overflow_upECcGpuGpAGBon:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_upECcGpuGpAGBon:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dgAjxsrizCkepbh





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dgAjxsrizCkepbh
.L_small_initial_partial_block_dgAjxsrizCkepbh:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dgAjxsrizCkepbh:

	orq	%r8,%r8
	je	.L_after_reduction_dgAjxsrizCkepbh
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dgAjxsrizCkepbh:
	jmp	.L_last_blocks_done_rgrmygdpoyvhfsv
.L_last_num_blocks_is_5_rgrmygdpoyvhfsv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_dwitqqrpcbldruk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_dwitqqrpcbldruk

.L_16_blocks_overflow_dwitqqrpcbldruk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_dwitqqrpcbldruk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qhwclsBlyAErDqE





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qhwclsBlyAErDqE
.L_small_initial_partial_block_qhwclsBlyAErDqE:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qhwclsBlyAErDqE:

	orq	%r8,%r8
	je	.L_after_reduction_qhwclsBlyAErDqE
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_qhwclsBlyAErDqE:
	jmp	.L_last_blocks_done_rgrmygdpoyvhfsv
.L_last_num_blocks_is_6_rgrmygdpoyvhfsv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_qrukwbfBAvotFsn
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_qrukwbfBAvotFsn

.L_16_blocks_overflow_qrukwbfBAvotFsn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_qrukwbfBAvotFsn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jnEtkoefqFjmqFo





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jnEtkoefqFjmqFo
.L_small_initial_partial_block_jnEtkoefqFjmqFo:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jnEtkoefqFjmqFo:

	orq	%r8,%r8
	je	.L_after_reduction_jnEtkoefqFjmqFo
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jnEtkoefqFjmqFo:
	jmp	.L_last_blocks_done_rgrmygdpoyvhfsv
.L_last_num_blocks_is_7_rgrmygdpoyvhfsv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_zEjtasBsgjlwdfB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_zEjtasBsgjlwdfB

.L_16_blocks_overflow_zEjtasBsgjlwdfB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_zEjtasBsgjlwdfB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_afGkGwwlixFapEf





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_afGkGwwlixFapEf
.L_small_initial_partial_block_afGkGwwlixFapEf:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_afGkGwwlixFapEf:

	orq	%r8,%r8
	je	.L_after_reduction_afGkGwwlixFapEf
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_afGkGwwlixFapEf:
	jmp	.L_last_blocks_done_rgrmygdpoyvhfsv
.L_last_num_blocks_is_8_rgrmygdpoyvhfsv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_kxfEhhqbvCBdFiq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_kxfEhhqbvCBdFiq

.L_16_blocks_overflow_kxfEhhqbvCBdFiq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_kxfEhhqbvCBdFiq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zypueDcnDpEvAtz





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zypueDcnDpEvAtz
.L_small_initial_partial_block_zypueDcnDpEvAtz:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zypueDcnDpEvAtz:

	orq	%r8,%r8
	je	.L_after_reduction_zypueDcnDpEvAtz
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_zypueDcnDpEvAtz:
	jmp	.L_last_blocks_done_rgrmygdpoyvhfsv
.L_last_num_blocks_is_9_rgrmygdpoyvhfsv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_GdzyxrisFFCyhuC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_GdzyxrisFFCyhuC

.L_16_blocks_overflow_GdzyxrisFFCyhuC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_GdzyxrisFFCyhuC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gkdnCtnDpnlbwAn





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gkdnCtnDpnlbwAn
.L_small_initial_partial_block_gkdnCtnDpnlbwAn:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gkdnCtnDpnlbwAn:

	orq	%r8,%r8
	je	.L_after_reduction_gkdnCtnDpnlbwAn
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_gkdnCtnDpnlbwAn:
	jmp	.L_last_blocks_done_rgrmygdpoyvhfsv
.L_last_num_blocks_is_10_rgrmygdpoyvhfsv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_uuxyEEEcCujDtoz
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_uuxyEEEcCujDtoz

.L_16_blocks_overflow_uuxyEEEcCujDtoz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_uuxyEEEcCujDtoz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gwsyGBykBqGchBu





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gwsyGBykBqGchBu
.L_small_initial_partial_block_gwsyGBykBqGchBu:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gwsyGBykBqGchBu:

	orq	%r8,%r8
	je	.L_after_reduction_gwsyGBykBqGchBu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_gwsyGBykBqGchBu:
	jmp	.L_last_blocks_done_rgrmygdpoyvhfsv
.L_last_num_blocks_is_11_rgrmygdpoyvhfsv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_rFuhqwFokABulyF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_rFuhqwFokABulyF

.L_16_blocks_overflow_rFuhqwFokABulyF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_rFuhqwFokABulyF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_FinddhkcabBsAne





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_FinddhkcabBsAne
.L_small_initial_partial_block_FinddhkcabBsAne:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_FinddhkcabBsAne:

	orq	%r8,%r8
	je	.L_after_reduction_FinddhkcabBsAne
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_FinddhkcabBsAne:
	jmp	.L_last_blocks_done_rgrmygdpoyvhfsv
.L_last_num_blocks_is_12_rgrmygdpoyvhfsv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_xscjnvElpnanajs
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_xscjnvElpnanajs

.L_16_blocks_overflow_xscjnvElpnanajs:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_xscjnvElpnanajs:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_tfapBrjbjhhcpGr





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_tfapBrjbjhhcpGr
.L_small_initial_partial_block_tfapBrjbjhhcpGr:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_tfapBrjbjhhcpGr:

	orq	%r8,%r8
	je	.L_after_reduction_tfapBrjbjhhcpGr
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_tfapBrjbjhhcpGr:
	jmp	.L_last_blocks_done_rgrmygdpoyvhfsv
.L_last_num_blocks_is_13_rgrmygdpoyvhfsv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_qqzatsCcndxnlvo
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_qqzatsCcndxnlvo

.L_16_blocks_overflow_qqzatsCcndxnlvo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_qqzatsCcndxnlvo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_AEdkvBelCwveFzu





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_AEdkvBelCwveFzu
.L_small_initial_partial_block_AEdkvBelCwveFzu:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_AEdkvBelCwveFzu:

	orq	%r8,%r8
	je	.L_after_reduction_AEdkvBelCwveFzu
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_AEdkvBelCwveFzu:
	jmp	.L_last_blocks_done_rgrmygdpoyvhfsv
.L_last_num_blocks_is_14_rgrmygdpoyvhfsv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_CieEemihxtFzzzr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_CieEemihxtFzzzr

.L_16_blocks_overflow_CieEemihxtFzzzr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_CieEemihxtFzzzr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_tsDBymjhEfBBsgq





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_tsDBymjhEfBBsgq
.L_small_initial_partial_block_tsDBymjhEfBBsgq:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_tsDBymjhEfBBsgq:

	orq	%r8,%r8
	je	.L_after_reduction_tsDBymjhEfBBsgq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_tsDBymjhEfBBsgq:
	jmp	.L_last_blocks_done_rgrmygdpoyvhfsv
.L_last_num_blocks_is_15_rgrmygdpoyvhfsv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_invxggzuihsszEb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_invxggzuihsszEb

.L_16_blocks_overflow_invxggzuihsszEb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_invxggzuihsszEb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ijbxtniiaAuCCGq





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ijbxtniiaAuCCGq
.L_small_initial_partial_block_ijbxtniiaAuCCGq:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ijbxtniiaAuCCGq:

	orq	%r8,%r8
	je	.L_after_reduction_ijbxtniiaAuCCGq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ijbxtniiaAuCCGq:
	jmp	.L_last_blocks_done_rgrmygdpoyvhfsv
.L_last_num_blocks_is_16_rgrmygdpoyvhfsv:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_vhGqitxAniqpyie
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_vhGqitxAniqpyie

.L_16_blocks_overflow_vhGqitxAniqpyie:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_vhGqitxAniqpyie:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_nvlAhwBlknDmgll:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nvlAhwBlknDmgll:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_nvlAhwBlknDmgll:
	jmp	.L_last_blocks_done_rgrmygdpoyvhfsv
.L_last_num_blocks_is_0_rgrmygdpoyvhfsv:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_rgrmygdpoyvhfsv:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_eFCuDlbftGnAEad
.L_encrypt_16_blocks_eFCuDlbftGnAEad:
	cmpb	$240,%r15b
	jae	.L_16_blocks_overflow_hpbraAfaarodrip
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_hpbraAfaarodrip
.L_16_blocks_overflow_hpbraAfaarodrip:
	vpshufb	%zmm29,%zmm2,%zmm2
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_hpbraAfaarodrip:
	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp),%zmm1




	vshufi64x2	$255,%zmm5,%zmm5,%zmm2
	addb	$16,%r15b


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp),%zmm18
	vmovdqa64	832(%rsp),%zmm22







	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30



	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm6
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp),%zmm1
	vmovdqa64	896(%rsp),%zmm8



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp),%zmm18
	vmovdqa64	960(%rsp),%zmm22



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30



	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19


	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31



	vpternlogq	$0x96,%zmm17,%zmm12,%zmm6
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10



	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30



	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21



	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31



	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13


	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm15,%zmm10,%zmm26
	vpxorq	%zmm12,%zmm6,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31

	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30

	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5



	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5



	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1)
	vpshufb	%zmm29,%zmm17,%zmm0
	vpshufb	%zmm29,%zmm19,%zmm3
	vpshufb	%zmm29,%zmm20,%zmm4
	vpshufb	%zmm29,%zmm21,%zmm5
	vmovdqa64	%zmm0,1280(%rsp)
	vmovdqa64	%zmm3,1344(%rsp)
	vmovdqa64	%zmm4,1408(%rsp)
	vmovdqa64	%zmm5,1472(%rsp)
	vmovdqa64	1024(%rsp),%zmm13
	vmovdqu64	256(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1088(%rsp),%zmm13
	vmovdqu64	320(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1152(%rsp),%zmm13
	vmovdqu64	384(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1216(%rsp),%zmm13
	vmovdqu64	448(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	subq	$256,%r8
	addq	$256,%rax
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_rCvgGfdvhrikorf

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_rCvgGfdvhrikorf
	jb	.L_last_num_blocks_is_7_1_rCvgGfdvhrikorf


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_rCvgGfdvhrikorf
	jb	.L_last_num_blocks_is_11_9_rCvgGfdvhrikorf


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_rCvgGfdvhrikorf
	ja	.L_last_num_blocks_is_16_rCvgGfdvhrikorf
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_rCvgGfdvhrikorf
	jmp	.L_last_num_blocks_is_13_rCvgGfdvhrikorf

.L_last_num_blocks_is_11_9_rCvgGfdvhrikorf:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_rCvgGfdvhrikorf
	ja	.L_last_num_blocks_is_11_rCvgGfdvhrikorf
	jmp	.L_last_num_blocks_is_9_rCvgGfdvhrikorf

.L_last_num_blocks_is_7_1_rCvgGfdvhrikorf:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_rCvgGfdvhrikorf
	jb	.L_last_num_blocks_is_3_1_rCvgGfdvhrikorf

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_rCvgGfdvhrikorf
	je	.L_last_num_blocks_is_6_rCvgGfdvhrikorf
	jmp	.L_last_num_blocks_is_5_rCvgGfdvhrikorf

.L_last_num_blocks_is_3_1_rCvgGfdvhrikorf:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_rCvgGfdvhrikorf
	je	.L_last_num_blocks_is_2_rCvgGfdvhrikorf
.L_last_num_blocks_is_1_rCvgGfdvhrikorf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_fDarvCfAhaquyag
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_fDarvCfAhaquyag

.L_16_blocks_overflow_fDarvCfAhaquyag:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_fDarvCfAhaquyag:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%xmm31,%xmm0,%xmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_mCBDioeopbfebvp





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_mCBDioeopbfebvp
.L_small_initial_partial_block_mCBDioeopbfebvp:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)











	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_mCBDioeopbfebvp
.L_small_initial_compute_done_mCBDioeopbfebvp:
.L_after_reduction_mCBDioeopbfebvp:
	jmp	.L_last_blocks_done_rCvgGfdvhrikorf
.L_last_num_blocks_is_2_rCvgGfdvhrikorf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_fFccptnoAqDfslw
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_fFccptnoAqDfslw

.L_16_blocks_overflow_fFccptnoAqDfslw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_fFccptnoAqDfslw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%ymm31,%ymm0,%ymm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_jgyetcjkpmeFkjh





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_jgyetcjkpmeFkjh
.L_small_initial_partial_block_jgyetcjkpmeFkjh:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jgyetcjkpmeFkjh:

	orq	%r8,%r8
	je	.L_after_reduction_jgyetcjkpmeFkjh
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_jgyetcjkpmeFkjh:
	jmp	.L_last_blocks_done_rCvgGfdvhrikorf
.L_last_num_blocks_is_3_rCvgGfdvhrikorf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_mDFEggzodxuGxqD
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_mDFEggzodxuGxqD

.L_16_blocks_overflow_mDFEggzodxuGxqD:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_mDFEggzodxuGxqD:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_bCrubDrrFAekxjv





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_bCrubDrrFAekxjv
.L_small_initial_partial_block_bCrubDrrFAekxjv:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_bCrubDrrFAekxjv:

	orq	%r8,%r8
	je	.L_after_reduction_bCrubDrrFAekxjv
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_bCrubDrrFAekxjv:
	jmp	.L_last_blocks_done_rCvgGfdvhrikorf
.L_last_num_blocks_is_4_rCvgGfdvhrikorf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_qieEmCgBtGizGjo
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_qieEmCgBtGizGjo

.L_16_blocks_overflow_qieEmCgBtGizGjo:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_qieEmCgBtGizGjo:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ehieCkAmsjFkqci





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ehieCkAmsjFkqci
.L_small_initial_partial_block_ehieCkAmsjFkqci:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ehieCkAmsjFkqci:

	orq	%r8,%r8
	je	.L_after_reduction_ehieCkAmsjFkqci
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ehieCkAmsjFkqci:
	jmp	.L_last_blocks_done_rCvgGfdvhrikorf
.L_last_num_blocks_is_5_rCvgGfdvhrikorf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_tpnwfcltcuhFBlh
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_tpnwfcltcuhFBlh

.L_16_blocks_overflow_tpnwfcltcuhFBlh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_tpnwfcltcuhFBlh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_mupktorikDsyfzG





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_mupktorikDsyfzG
.L_small_initial_partial_block_mupktorikDsyfzG:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_mupktorikDsyfzG:

	orq	%r8,%r8
	je	.L_after_reduction_mupktorikDsyfzG
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_mupktorikDsyfzG:
	jmp	.L_last_blocks_done_rCvgGfdvhrikorf
.L_last_num_blocks_is_6_rCvgGfdvhrikorf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_EBuirCaCtobCiwb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_EBuirCaCtobCiwb

.L_16_blocks_overflow_EBuirCaCtobCiwb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_EBuirCaCtobCiwb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_AwelglCzGygfDga





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_AwelglCzGygfDga
.L_small_initial_partial_block_AwelglCzGygfDga:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_AwelglCzGygfDga:

	orq	%r8,%r8
	je	.L_after_reduction_AwelglCzGygfDga
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_AwelglCzGygfDga:
	jmp	.L_last_blocks_done_rCvgGfdvhrikorf
.L_last_num_blocks_is_7_rCvgGfdvhrikorf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_garDrnckFgwaDdh
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_garDrnckFgwaDdh

.L_16_blocks_overflow_garDrnckFgwaDdh:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_garDrnckFgwaDdh:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_hyDqjDruqFnvzGo





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_hyDqjDruqFnvzGo
.L_small_initial_partial_block_hyDqjDruqFnvzGo:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_hyDqjDruqFnvzGo:

	orq	%r8,%r8
	je	.L_after_reduction_hyDqjDruqFnvzGo
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_hyDqjDruqFnvzGo:
	jmp	.L_last_blocks_done_rCvgGfdvhrikorf
.L_last_num_blocks_is_8_rCvgGfdvhrikorf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_fidybnBzGxmjtlp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_fidybnBzGxmjtlp

.L_16_blocks_overflow_fidybnBzGxmjtlp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_fidybnBzGxmjtlp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DCAagdDgqvrAuia





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DCAagdDgqvrAuia
.L_small_initial_partial_block_DCAagdDgqvrAuia:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DCAagdDgqvrAuia:

	orq	%r8,%r8
	je	.L_after_reduction_DCAagdDgqvrAuia
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DCAagdDgqvrAuia:
	jmp	.L_last_blocks_done_rCvgGfdvhrikorf
.L_last_num_blocks_is_9_rCvgGfdvhrikorf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_uxajlmEEFfExsse
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_uxajlmEEFfExsse

.L_16_blocks_overflow_uxajlmEEFfExsse:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_uxajlmEEFfExsse:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_tzhEmkoAgpstBwn





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_tzhEmkoAgpstBwn
.L_small_initial_partial_block_tzhEmkoAgpstBwn:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_tzhEmkoAgpstBwn:

	orq	%r8,%r8
	je	.L_after_reduction_tzhEmkoAgpstBwn
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_tzhEmkoAgpstBwn:
	jmp	.L_last_blocks_done_rCvgGfdvhrikorf
.L_last_num_blocks_is_10_rCvgGfdvhrikorf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_rksnddzDnlruocr
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_rksnddzDnlruocr

.L_16_blocks_overflow_rksnddzDnlruocr:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_rksnddzDnlruocr:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_wDgGlbvCAxiwBDG





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_wDgGlbvCAxiwBDG
.L_small_initial_partial_block_wDgGlbvCAxiwBDG:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_wDgGlbvCAxiwBDG:

	orq	%r8,%r8
	je	.L_after_reduction_wDgGlbvCAxiwBDG
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_wDgGlbvCAxiwBDG:
	jmp	.L_last_blocks_done_rCvgGfdvhrikorf
.L_last_num_blocks_is_11_rCvgGfdvhrikorf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_BlwCiCoqjjhyowi
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_BlwCiCoqjjhyowi

.L_16_blocks_overflow_BlwCiCoqjjhyowi:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_BlwCiCoqjjhyowi:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_xpaAojgchnqGfBe





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_xpaAojgchnqGfBe
.L_small_initial_partial_block_xpaAojgchnqGfBe:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_xpaAojgchnqGfBe:

	orq	%r8,%r8
	je	.L_after_reduction_xpaAojgchnqGfBe
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_xpaAojgchnqGfBe:
	jmp	.L_last_blocks_done_rCvgGfdvhrikorf
.L_last_num_blocks_is_12_rCvgGfdvhrikorf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_veEafAxiygBxbwq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_veEafAxiygBxbwq

.L_16_blocks_overflow_veEafAxiygBxbwq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_veEafAxiygBxbwq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gtldburmbieqDfy





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gtldburmbieqDfy
.L_small_initial_partial_block_gtldburmbieqDfy:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gtldburmbieqDfy:

	orq	%r8,%r8
	je	.L_after_reduction_gtldburmbieqDfy
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_gtldburmbieqDfy:
	jmp	.L_last_blocks_done_rCvgGfdvhrikorf
.L_last_num_blocks_is_13_rCvgGfdvhrikorf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_mozwjrhbtkGcvtq
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_mozwjrhbtkGcvtq

.L_16_blocks_overflow_mozwjrhbtkGcvtq:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_mozwjrhbtkGcvtq:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zwrtwnExbbcGpdx





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zwrtwnExbbcGpdx
.L_small_initial_partial_block_zwrtwnExbbcGpdx:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zwrtwnExbbcGpdx:

	orq	%r8,%r8
	je	.L_after_reduction_zwrtwnExbbcGpdx
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_zwrtwnExbbcGpdx:
	jmp	.L_last_blocks_done_rCvgGfdvhrikorf
.L_last_num_blocks_is_14_rCvgGfdvhrikorf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_cBpbcGtwxDCgnfp
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_cBpbcGtwxDCgnfp

.L_16_blocks_overflow_cBpbcGtwxDCgnfp:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_cBpbcGtwxDCgnfp:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DovnbDhyrzGkDtq





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DovnbDhyrzGkDtq
.L_small_initial_partial_block_DovnbDhyrzGkDtq:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DovnbDhyrzGkDtq:

	orq	%r8,%r8
	je	.L_after_reduction_DovnbDhyrzGkDtq
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DovnbDhyrzGkDtq:
	jmp	.L_last_blocks_done_rCvgGfdvhrikorf
.L_last_num_blocks_is_15_rCvgGfdvhrikorf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_msEhikgDaolfFvf
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_msEhikgDaolfFvf

.L_16_blocks_overflow_msEhikgDaolfFvf:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_msEhikgDaolfFvf:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dgpjedFkdGgcere





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dgpjedFkdGgcere
.L_small_initial_partial_block_dgpjedFkdGgcere:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dgpjedFkdGgcere:

	orq	%r8,%r8
	je	.L_after_reduction_dgpjedFkdGgcere
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_dgpjedFkdGgcere:
	jmp	.L_last_blocks_done_rCvgGfdvhrikorf
.L_last_num_blocks_is_16_rCvgGfdvhrikorf:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_oDfDnhCchcGxljw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_oDfDnhCchcGxljw

.L_16_blocks_overflow_oDfDnhCchcGxljw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_oDfDnhCchcGxljw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vmovdqa64	1280(%rsp),%zmm8
	vmovdqu64	512(%rsp),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	576(%rsp),%zmm18
	vmovdqa64	1344(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	640(%rsp),%zmm1
	vmovdqa64	1408(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	704(%rsp),%zmm18
	vmovdqa64	1472(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpternlogq	$0x96,%zmm12,%zmm24,%zmm14
	vpternlogq	$0x96,%zmm13,%zmm25,%zmm7
	vpternlogq	$0x96,%zmm15,%zmm26,%zmm10
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vpsrldq	$8,%zmm10,%zmm15
	vpslldq	$8,%zmm10,%zmm10

	vmovdqa64	POLY2(%rip),%xmm16
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vpxorq	%zmm15,%zmm14,%zmm14
	vpxorq	%zmm10,%zmm7,%zmm7
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vextracti64x4	$1,%zmm14,%ymm12
	vpxorq	%ymm12,%ymm14,%ymm14
	vextracti32x4	$1,%ymm14,%xmm12
	vpxorq	%xmm12,%xmm14,%xmm14
	vextracti64x4	$1,%zmm7,%ymm13
	vpxorq	%ymm13,%ymm7,%ymm7
	vextracti32x4	$1,%ymm7,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm7
	vbroadcastf64x2	176(%rdi),%zmm31
	vpclmulqdq	$0x01,%xmm7,%xmm16,%xmm13
	vpslldq	$8,%xmm13,%xmm13
	vpxorq	%xmm13,%xmm7,%xmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vpclmulqdq	$0x00,%xmm13,%xmm16,%xmm12
	vpsrldq	$4,%xmm12,%xmm12
	vpclmulqdq	$0x10,%xmm13,%xmm16,%xmm15
	vpslldq	$4,%xmm15,%xmm15

	vpternlogq	$0x96,%xmm12,%xmm15,%xmm14
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_hfghjgbBupDbenF:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vpxorq	%zmm14,%zmm17,%zmm17
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm31,%zmm5,%zmm5
	vpxorq	%zmm8,%zmm0,%zmm0
	vpxorq	%zmm22,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_hfghjgbBupDbenF:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_hfghjgbBupDbenF:
	jmp	.L_last_blocks_done_rCvgGfdvhrikorf
.L_last_num_blocks_is_0_rCvgGfdvhrikorf:
	vmovdqa64	1280(%rsp),%zmm13
	vmovdqu64	512(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1344(%rsp),%zmm13
	vmovdqu64	576(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	1408(%rsp),%zmm13
	vmovdqu64	640(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	1472(%rsp),%zmm13
	vmovdqu64	704(%rsp),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_rCvgGfdvhrikorf:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_eFCuDlbftGnAEad

.L_message_below_32_blocks_eFCuDlbftGnAEad:


	subq	$256,%r8
	addq	$256,%rax
	movl	%r8d,%r10d
	leaq	96(%rsi),%r12
	testq	%r14,%r14
	jnz	.L_skip_hkeys_precomputation_lFcitfvrAtqoiCs
	vmovdqu64	640(%rsp),%zmm3


	vshufi64x2	$0x00,%zmm3,%zmm3,%zmm3

	vmovdqu64	576(%rsp),%zmm4
	vmovdqu64	512(%rsp),%zmm5

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,448(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,384(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm4,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm4,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm4,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm4,%zmm4

	vpsrldq	$8,%zmm4,%zmm10
	vpslldq	$8,%zmm4,%zmm4
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm4,%zmm4



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm4,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm4,%zmm4



	vpclmulqdq	$0x00,%zmm4,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm4,%zmm10,%zmm4
	vpslldq	$4,%zmm4,%zmm4

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm4

	vmovdqu64	%zmm4,320(%rsp)

	vpclmulqdq	$0x11,%zmm3,%zmm5,%zmm6
	vpclmulqdq	$0x00,%zmm3,%zmm5,%zmm7
	vpclmulqdq	$0x01,%zmm3,%zmm5,%zmm10
	vpclmulqdq	$0x10,%zmm3,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm5,%zmm5

	vpsrldq	$8,%zmm5,%zmm10
	vpslldq	$8,%zmm5,%zmm5
	vpxorq	%zmm10,%zmm6,%zmm6
	vpxorq	%zmm7,%zmm5,%zmm5



	vmovdqu64	POLY2(%rip),%zmm10

	vpclmulqdq	$0x01,%zmm5,%zmm10,%zmm7
	vpslldq	$8,%zmm7,%zmm7
	vpxorq	%zmm7,%zmm5,%zmm5



	vpclmulqdq	$0x00,%zmm5,%zmm10,%zmm7
	vpsrldq	$4,%zmm7,%zmm7
	vpclmulqdq	$0x10,%zmm5,%zmm10,%zmm5
	vpslldq	$4,%zmm5,%zmm5

	vpternlogq	$0x96,%zmm7,%zmm6,%zmm5

	vmovdqu64	%zmm5,256(%rsp)
.L_skip_hkeys_precomputation_lFcitfvrAtqoiCs:
	movq	$1,%r14
	andl	$~15,%r10d
	movl	$512,%ebx
	subl	%r10d,%ebx
	movl	%r8d,%r10d
	addl	$15,%r10d
	shrl	$4,%r10d
	je	.L_last_num_blocks_is_0_DocrvoFoxgdxxwu

	cmpl	$8,%r10d
	je	.L_last_num_blocks_is_8_DocrvoFoxgdxxwu
	jb	.L_last_num_blocks_is_7_1_DocrvoFoxgdxxwu


	cmpl	$12,%r10d
	je	.L_last_num_blocks_is_12_DocrvoFoxgdxxwu
	jb	.L_last_num_blocks_is_11_9_DocrvoFoxgdxxwu


	cmpl	$15,%r10d
	je	.L_last_num_blocks_is_15_DocrvoFoxgdxxwu
	ja	.L_last_num_blocks_is_16_DocrvoFoxgdxxwu
	cmpl	$14,%r10d
	je	.L_last_num_blocks_is_14_DocrvoFoxgdxxwu
	jmp	.L_last_num_blocks_is_13_DocrvoFoxgdxxwu

.L_last_num_blocks_is_11_9_DocrvoFoxgdxxwu:

	cmpl	$10,%r10d
	je	.L_last_num_blocks_is_10_DocrvoFoxgdxxwu
	ja	.L_last_num_blocks_is_11_DocrvoFoxgdxxwu
	jmp	.L_last_num_blocks_is_9_DocrvoFoxgdxxwu

.L_last_num_blocks_is_7_1_DocrvoFoxgdxxwu:
	cmpl	$4,%r10d
	je	.L_last_num_blocks_is_4_DocrvoFoxgdxxwu
	jb	.L_last_num_blocks_is_3_1_DocrvoFoxgdxxwu

	cmpl	$6,%r10d
	ja	.L_last_num_blocks_is_7_DocrvoFoxgdxxwu
	je	.L_last_num_blocks_is_6_DocrvoFoxgdxxwu
	jmp	.L_last_num_blocks_is_5_DocrvoFoxgdxxwu

.L_last_num_blocks_is_3_1_DocrvoFoxgdxxwu:

	cmpl	$2,%r10d
	ja	.L_last_num_blocks_is_3_DocrvoFoxgdxxwu
	je	.L_last_num_blocks_is_2_DocrvoFoxgdxxwu
.L_last_num_blocks_is_1_DocrvoFoxgdxxwu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$255,%r15d
	jae	.L_16_blocks_overflow_yBgtFGkkAevfFlz
	vpaddd	%xmm28,%xmm2,%xmm0
	jmp	.L_16_blocks_ok_yBgtFGkkAevfFlz

.L_16_blocks_overflow_yBgtFGkkAevfFlz:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%xmm29,%xmm0,%xmm0
.L_16_blocks_ok_yBgtFGkkAevfFlz:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%xmm17{%k1}{z}
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%xmm30,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%xmm31,%xmm0,%xmm0
	vaesenclast	%xmm30,%xmm0,%xmm0
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%xmm29,%xmm17,%xmm17
	vextracti32x4	$0,%zmm17,%xmm7
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_kGejsuGxGByngcr





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_kGejsuGxGByngcr
.L_small_initial_partial_block_kGejsuGxGByngcr:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)


	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm0


	vpclmulqdq	$0x01,%xmm25,%xmm0,%xmm3
	vpslldq	$8,%xmm3,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm3


	vpclmulqdq	$0x00,%xmm3,%xmm0,%xmm4
	vpsrldq	$4,%xmm4,%xmm4
	vpclmulqdq	$0x10,%xmm3,%xmm0,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm4,%xmm14












	vpxorq	%xmm7,%xmm14,%xmm14

	jmp	.L_after_reduction_kGejsuGxGByngcr
.L_small_initial_compute_done_kGejsuGxGByngcr:
.L_after_reduction_kGejsuGxGByngcr:
	jmp	.L_last_blocks_done_DocrvoFoxgdxxwu
.L_last_num_blocks_is_2_DocrvoFoxgdxxwu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$254,%r15d
	jae	.L_16_blocks_overflow_nunbfAFkzprilmn
	vpaddd	%ymm28,%ymm2,%ymm0
	jmp	.L_16_blocks_ok_nunbfAFkzprilmn

.L_16_blocks_overflow_nunbfAFkzprilmn:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%ymm29,%ymm0,%ymm0
.L_16_blocks_ok_nunbfAFkzprilmn:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%ymm17{%k1}{z}
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%ymm30,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%ymm31,%ymm0,%ymm0
	vaesenclast	%ymm30,%ymm0,%ymm0
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%ymm29,%ymm17,%ymm17
	vextracti32x4	$1,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_swDwGhpjffqadtv





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_swDwGhpjffqadtv
.L_small_initial_partial_block_swDwGhpjffqadtv:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm17,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm17,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm17,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm17,%xmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_swDwGhpjffqadtv:

	orq	%r8,%r8
	je	.L_after_reduction_swDwGhpjffqadtv
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_swDwGhpjffqadtv:
	jmp	.L_last_blocks_done_DocrvoFoxgdxxwu
.L_last_num_blocks_is_3_DocrvoFoxgdxxwu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$253,%r15d
	jae	.L_16_blocks_overflow_hnFpdCDpnxgefrE
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_hnFpdCDpnxgefrE

.L_16_blocks_overflow_hnFpdCDpnxgefrE:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_hnFpdCDpnxgefrE:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$2,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_AvpihEwskGGguFs





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_AvpihEwskGGguFs
.L_small_initial_partial_block_AvpihEwskGGguFs:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm17,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm17,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm17,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm17,%ymm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_AvpihEwskGGguFs:

	orq	%r8,%r8
	je	.L_after_reduction_AvpihEwskGGguFs
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_AvpihEwskGGguFs:
	jmp	.L_last_blocks_done_DocrvoFoxgdxxwu
.L_last_num_blocks_is_4_DocrvoFoxgdxxwu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$252,%r15d
	jae	.L_16_blocks_overflow_hFuetzlkcakpyrB
	vpaddd	%zmm28,%zmm2,%zmm0
	jmp	.L_16_blocks_ok_hFuetzlkcakpyrB

.L_16_blocks_overflow_hFuetzlkcakpyrB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpshufb	%zmm29,%zmm0,%zmm0
.L_16_blocks_ok_hFuetzlkcakpyrB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm0,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm17,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm17,%zmm17{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vextracti32x4	$3,%zmm17,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_aotvBDGhugzAkrd





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_aotvBDGhugzAkrd
.L_small_initial_partial_block_aotvBDGhugzAkrd:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpxorq	%zmm26,%zmm4,%zmm4
	vpxorq	%zmm24,%zmm0,%zmm0
	vpxorq	%zmm25,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_aotvBDGhugzAkrd:

	orq	%r8,%r8
	je	.L_after_reduction_aotvBDGhugzAkrd
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_aotvBDGhugzAkrd:
	jmp	.L_last_blocks_done_DocrvoFoxgdxxwu
.L_last_num_blocks_is_5_DocrvoFoxgdxxwu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$251,%r15d
	jae	.L_16_blocks_overflow_plAnBAFoDljBxve
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%xmm27,%xmm0,%xmm3
	jmp	.L_16_blocks_ok_plAnBAFoDljBxve

.L_16_blocks_overflow_plAnBAFoDljBxve:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
.L_16_blocks_ok_plAnBAFoDljBxve:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%xmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%xmm30,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%xmm31,%xmm3,%xmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%xmm30,%xmm3,%xmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%xmm19,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%xmm29,%xmm19,%xmm19
	vextracti32x4	$0,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ifoCnevaffhatrr





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ifoCnevaffhatrr
.L_small_initial_partial_block_ifoCnevaffhatrr:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ifoCnevaffhatrr:

	orq	%r8,%r8
	je	.L_after_reduction_ifoCnevaffhatrr
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ifoCnevaffhatrr:
	jmp	.L_last_blocks_done_DocrvoFoxgdxxwu
.L_last_num_blocks_is_6_DocrvoFoxgdxxwu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$250,%r15d
	jae	.L_16_blocks_overflow_CGtlvoxufDpmyFe
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%ymm27,%ymm0,%ymm3
	jmp	.L_16_blocks_ok_CGtlvoxufDpmyFe

.L_16_blocks_overflow_CGtlvoxufDpmyFe:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
.L_16_blocks_ok_CGtlvoxufDpmyFe:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%ymm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%ymm30,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%ymm31,%ymm3,%ymm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%ymm30,%ymm3,%ymm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%ymm29,%ymm19,%ymm19
	vextracti32x4	$1,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ewiGwwgoycfeass





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ewiGwwgoycfeass
.L_small_initial_partial_block_ewiGwwgoycfeass:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm19,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm19,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm19,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm19,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ewiGwwgoycfeass:

	orq	%r8,%r8
	je	.L_after_reduction_ewiGwwgoycfeass
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_ewiGwwgoycfeass:
	jmp	.L_last_blocks_done_DocrvoFoxgdxxwu
.L_last_num_blocks_is_7_DocrvoFoxgdxxwu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$249,%r15d
	jae	.L_16_blocks_overflow_BCcjgoFqFBGssqF
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_BCcjgoFqFBGssqF

.L_16_blocks_overflow_BCcjgoFqFBGssqF:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_BCcjgoFqFBGssqF:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$2,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_gyftgqExlEpqDif





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_gyftgqExlEpqDif
.L_small_initial_partial_block_gyftgqExlEpqDif:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm19,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm19,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm19,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm19,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gyftgqExlEpqDif:

	orq	%r8,%r8
	je	.L_after_reduction_gyftgqExlEpqDif
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_gyftgqExlEpqDif:
	jmp	.L_last_blocks_done_DocrvoFoxgdxxwu
.L_last_num_blocks_is_8_DocrvoFoxgdxxwu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$64,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$248,%r15d
	jae	.L_16_blocks_overflow_pxawaAvjcerDnpk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	jmp	.L_16_blocks_ok_pxawaAvjcerDnpk

.L_16_blocks_overflow_pxawaAvjcerDnpk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
.L_16_blocks_ok_pxawaAvjcerDnpk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm3,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm19,%zmm19{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vextracti32x4	$3,%zmm19,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_DdEfcuEuixwsdyv





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_DdEfcuEuixwsdyv
.L_small_initial_partial_block_DdEfcuEuixwsdyv:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_DdEfcuEuixwsdyv:

	orq	%r8,%r8
	je	.L_after_reduction_DdEfcuEuixwsdyv
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_DdEfcuEuixwsdyv:
	jmp	.L_last_blocks_done_DocrvoFoxgdxxwu
.L_last_num_blocks_is_9_DocrvoFoxgdxxwu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$247,%r15d
	jae	.L_16_blocks_overflow_yGdEkbhzhmDqkur
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%xmm27,%xmm3,%xmm4
	jmp	.L_16_blocks_ok_yGdEkbhzhmDqkur

.L_16_blocks_overflow_yGdEkbhzhmDqkur:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
.L_16_blocks_ok_yGdEkbhzhmDqkur:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%xmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%xmm30,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%xmm31,%xmm4,%xmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%xmm30,%xmm4,%xmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%xmm20,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%xmm29,%xmm20,%xmm20
	vextracti32x4	$0,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_AyCkgCeopChCEEg





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_AyCkgCeopChCEEg
.L_small_initial_partial_block_AyCkgCeopChCEEg:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_AyCkgCeopChCEEg:

	orq	%r8,%r8
	je	.L_after_reduction_AyCkgCeopChCEEg
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_AyCkgCeopChCEEg:
	jmp	.L_last_blocks_done_DocrvoFoxgdxxwu
.L_last_num_blocks_is_10_DocrvoFoxgdxxwu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$246,%r15d
	jae	.L_16_blocks_overflow_fhswlttGwBGolmB
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%ymm27,%ymm3,%ymm4
	jmp	.L_16_blocks_ok_fhswlttGwBGolmB

.L_16_blocks_overflow_fhswlttGwBGolmB:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
.L_16_blocks_ok_fhswlttGwBGolmB:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%ymm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%ymm30,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%ymm31,%ymm4,%ymm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%ymm30,%ymm4,%ymm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%ymm20,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%ymm29,%ymm20,%ymm20
	vextracti32x4	$1,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_tnfcjvntCDdbppB





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_tnfcjvntCDdbppB
.L_small_initial_partial_block_tnfcjvntCDdbppB:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm20,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm20,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm20,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm20,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_tnfcjvntCDdbppB:

	orq	%r8,%r8
	je	.L_after_reduction_tnfcjvntCDdbppB
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_tnfcjvntCDdbppB:
	jmp	.L_last_blocks_done_DocrvoFoxgdxxwu
.L_last_num_blocks_is_11_DocrvoFoxgdxxwu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$245,%r15d
	jae	.L_16_blocks_overflow_nDdsCjkumbGFgip
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_nDdsCjkumbGFgip

.L_16_blocks_overflow_nDdsCjkumbGFgip:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_nDdsCjkumbGFgip:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$2,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_eewsnbraqfsGazo





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_eewsnbraqfsGazo
.L_small_initial_partial_block_eewsnbraqfsGazo:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm20,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm20,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm20,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm20,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_eewsnbraqfsGazo:

	orq	%r8,%r8
	je	.L_after_reduction_eewsnbraqfsGazo
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_eewsnbraqfsGazo:
	jmp	.L_last_blocks_done_DocrvoFoxgdxxwu
.L_last_num_blocks_is_12_DocrvoFoxgdxxwu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$128,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$244,%r15d
	jae	.L_16_blocks_overflow_gBcezvkcfuBukgk
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	jmp	.L_16_blocks_ok_gBcezvkcfuBukgk

.L_16_blocks_overflow_gBcezvkcfuBukgk:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
.L_16_blocks_ok_gBcezvkcfuBukgk:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm4,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm20,%zmm20{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vextracti32x4	$3,%zmm20,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_vbtszldwACsocdB





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_vbtszldwACsocdB
.L_small_initial_partial_block_vbtszldwACsocdB:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vpxorq	%zmm8,%zmm0,%zmm8
	vpxorq	%zmm22,%zmm3,%zmm22
	vpxorq	%zmm30,%zmm4,%zmm30
	vpxorq	%zmm31,%zmm5,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_vbtszldwACsocdB:

	orq	%r8,%r8
	je	.L_after_reduction_vbtszldwACsocdB
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_vbtszldwACsocdB:
	jmp	.L_last_blocks_done_DocrvoFoxgdxxwu
.L_last_num_blocks_is_13_DocrvoFoxgdxxwu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$243,%r15d
	jae	.L_16_blocks_overflow_AfwqsrmbkwanpuC
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%xmm27,%xmm4,%xmm5
	jmp	.L_16_blocks_ok_AfwqsrmbkwanpuC

.L_16_blocks_overflow_AfwqsrmbkwanpuC:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
.L_16_blocks_ok_AfwqsrmbkwanpuC:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$0,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%xmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%xmm30,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%xmm31,%xmm5,%xmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%xmm30,%xmm5,%xmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%xmm21,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%xmm29,%xmm21,%xmm21
	vextracti32x4	$0,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_FqDeAAmDEGEuGoC





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_FqDeAAmDEGEuGoC
.L_small_initial_partial_block_FqDeAAmDEGEuGoC:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	64(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	128(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	192(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31

	vpxorq	%zmm26,%zmm30,%zmm30
	vpxorq	%zmm24,%zmm8,%zmm8
	vpxorq	%zmm25,%zmm22,%zmm22

	vpxorq	%zmm31,%zmm30,%zmm30
	vpsrldq	$8,%zmm30,%zmm4
	vpslldq	$8,%zmm30,%zmm5
	vpxorq	%zmm4,%zmm8,%zmm0
	vpxorq	%zmm5,%zmm22,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_FqDeAAmDEGEuGoC:

	orq	%r8,%r8
	je	.L_after_reduction_FqDeAAmDEGEuGoC
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_FqDeAAmDEGEuGoC:
	jmp	.L_last_blocks_done_DocrvoFoxgdxxwu
.L_last_num_blocks_is_14_DocrvoFoxgdxxwu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$242,%r15d
	jae	.L_16_blocks_overflow_wGegzqrqCzjEFte
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%ymm27,%ymm4,%ymm5
	jmp	.L_16_blocks_ok_wGegzqrqCzjEFte

.L_16_blocks_overflow_wGegzqrqCzjEFte:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
.L_16_blocks_ok_wGegzqrqCzjEFte:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$1,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%ymm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%ymm30,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%ymm31,%ymm5,%ymm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%ymm30,%ymm5,%ymm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%ymm21,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%ymm29,%ymm21,%ymm21
	vextracti32x4	$1,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_zaAvrhreDdkCwad





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_zaAvrhreDdkCwad
.L_small_initial_partial_block_zaAvrhreDdkCwad:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	48(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	112(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	176(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	240(%r10),%xmm1
	vpclmulqdq	$0x01,%xmm1,%xmm21,%xmm4
	vpclmulqdq	$0x10,%xmm1,%xmm21,%xmm5
	vpclmulqdq	$0x11,%xmm1,%xmm21,%xmm0
	vpclmulqdq	$0x00,%xmm1,%xmm21,%xmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_zaAvrhreDdkCwad:

	orq	%r8,%r8
	je	.L_after_reduction_zaAvrhreDdkCwad
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_zaAvrhreDdkCwad:
	jmp	.L_last_blocks_done_DocrvoFoxgdxxwu
.L_last_num_blocks_is_15_DocrvoFoxgdxxwu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$241,%r15d
	jae	.L_16_blocks_overflow_wgfDysybiFkqtrb
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_wgfDysybiFkqtrb

.L_16_blocks_overflow_wgfDysybiFkqtrb:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_wgfDysybiFkqtrb:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$2,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$2,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_uzicADmCAonFiip





	subq	$16,%r8
	movl	$0,(%rdx)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_uzicADmCAonFiip
.L_small_initial_partial_block_uzicADmCAonFiip:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	32(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	96(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	160(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	224(%r10),%ymm1
	vpclmulqdq	$0x01,%ymm1,%ymm21,%ymm4
	vpclmulqdq	$0x10,%ymm1,%ymm21,%ymm5
	vpclmulqdq	$0x11,%ymm1,%ymm21,%ymm0
	vpclmulqdq	$0x00,%ymm1,%ymm21,%ymm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_uzicADmCAonFiip:

	orq	%r8,%r8
	je	.L_after_reduction_uzicADmCAonFiip
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_uzicADmCAonFiip:
	jmp	.L_last_blocks_done_DocrvoFoxgdxxwu
.L_last_num_blocks_is_16_DocrvoFoxgdxxwu:
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r11
	subq	$192,%r11
	kmovq	(%r10,%r11,8),%k1
	cmpl	$240,%r15d
	jae	.L_16_blocks_overflow_jiaGrznyvgBhqmw
	vpaddd	%zmm28,%zmm2,%zmm0
	vpaddd	%zmm27,%zmm0,%zmm3
	vpaddd	%zmm27,%zmm3,%zmm4
	vpaddd	%zmm27,%zmm4,%zmm5
	jmp	.L_16_blocks_ok_jiaGrznyvgBhqmw

.L_16_blocks_overflow_jiaGrznyvgBhqmw:
	vpshufb	%zmm29,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vmovdqa64	ddq_add_4444(%rip),%zmm5
	vpaddd	%zmm5,%zmm0,%zmm3
	vpaddd	%zmm5,%zmm3,%zmm4
	vpaddd	%zmm5,%zmm4,%zmm5
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
.L_16_blocks_ok_jiaGrznyvgBhqmw:




	vbroadcastf64x2	0(%rdi),%zmm30
	vpxorq	768(%rsp),%zmm14,%zmm8
	vmovdqu64	0(%rsp,%rbx,1),%zmm1
	vextracti32x4	$3,%zmm5,%xmm2
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2


	vbroadcastf64x2	16(%rdi),%zmm31
	vmovdqu64	64(%rsp,%rbx,1),%zmm18
	vmovdqa64	832(%rsp),%zmm22
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm30,%zmm3,%zmm3
	vpxorq	%zmm30,%zmm4,%zmm4
	vpxorq	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm30


	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm14
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm7
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm10
	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm11
	vmovdqu64	128(%rsp,%rbx,1),%zmm1
	vmovdqa64	896(%rsp),%zmm8
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm18
	vmovdqa64	960(%rsp),%zmm22
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm30


	vpclmulqdq	$0x10,%zmm1,%zmm8,%zmm20
	vpclmulqdq	$0x01,%zmm1,%zmm8,%zmm21
	vpclmulqdq	$0x11,%zmm1,%zmm8,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm8,%zmm19
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm31


	vpternlogq	$0x96,%zmm17,%zmm12,%zmm14
	vpternlogq	$0x96,%zmm19,%zmm13,%zmm7
	vpternlogq	$0x96,%zmm21,%zmm16,%zmm11
	vpternlogq	$0x96,%zmm20,%zmm15,%zmm10
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm30
	vmovdqu8	0(%rcx,%rax,1),%zmm17
	vmovdqu8	64(%rcx,%rax,1),%zmm19
	vmovdqu8	128(%rcx,%rax,1),%zmm20
	vmovdqu8	192(%rcx,%rax,1),%zmm21{%k1}{z}
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm31


	vpclmulqdq	$0x10,%zmm18,%zmm22,%zmm15
	vpclmulqdq	$0x01,%zmm18,%zmm22,%zmm16
	vpclmulqdq	$0x11,%zmm18,%zmm22,%zmm12
	vpclmulqdq	$0x00,%zmm18,%zmm22,%zmm13
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm30
	vpternlogq	$0x96,%zmm16,%zmm11,%zmm10
	vpxorq	%zmm12,%zmm14,%zmm24
	vpxorq	%zmm13,%zmm7,%zmm25
	vpxorq	%zmm15,%zmm10,%zmm26
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm31
	vaesenc	%zmm30,%zmm0,%zmm0
	vaesenc	%zmm30,%zmm3,%zmm3
	vaesenc	%zmm30,%zmm4,%zmm4
	vaesenc	%zmm30,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm30
	vaesenc	%zmm31,%zmm0,%zmm0
	vaesenc	%zmm31,%zmm3,%zmm3
	vaesenc	%zmm31,%zmm4,%zmm4
	vaesenc	%zmm31,%zmm5,%zmm5
	vaesenclast	%zmm30,%zmm0,%zmm0
	vaesenclast	%zmm30,%zmm3,%zmm3
	vaesenclast	%zmm30,%zmm4,%zmm4
	vaesenclast	%zmm30,%zmm5,%zmm5
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vpxorq	%zmm20,%zmm4,%zmm4
	vpxorq	%zmm21,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm11
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm21,%zmm21{%k1}{z}
	vpshufb	%zmm29,%zmm17,%zmm17
	vpshufb	%zmm29,%zmm19,%zmm19
	vpshufb	%zmm29,%zmm20,%zmm20
	vpshufb	%zmm29,%zmm21,%zmm21
	vextracti32x4	$3,%zmm21,%xmm7
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_gnDihwoBCmctfel:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm11,16(%rsi)
	vmovdqu64	16(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm17,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm17,%zmm3
	vpclmulqdq	$0x01,%zmm1,%zmm17,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm17,%zmm5
	vmovdqu64	80(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm19,%zmm8
	vpclmulqdq	$0x00,%zmm1,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm19,%zmm30
	vpclmulqdq	$0x10,%zmm1,%zmm19,%zmm31
	vmovdqu64	144(%r10),%zmm1
	vpclmulqdq	$0x11,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x00,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm0,%zmm17,%zmm8
	vpternlogq	$0x96,%zmm3,%zmm19,%zmm22
	vpclmulqdq	$0x01,%zmm1,%zmm20,%zmm17
	vpclmulqdq	$0x10,%zmm1,%zmm20,%zmm19
	vpternlogq	$0x96,%zmm4,%zmm17,%zmm30
	vpternlogq	$0x96,%zmm5,%zmm19,%zmm31
	vmovdqu64	208(%r10),%ymm1
	vinserti64x2	$2,240(%r10),%zmm1,%zmm1
	vpclmulqdq	$0x01,%zmm1,%zmm21,%zmm4
	vpclmulqdq	$0x10,%zmm1,%zmm21,%zmm5
	vpclmulqdq	$0x11,%zmm1,%zmm21,%zmm0
	vpclmulqdq	$0x00,%zmm1,%zmm21,%zmm3

	vpxorq	%zmm30,%zmm4,%zmm4
	vpternlogq	$0x96,%zmm31,%zmm26,%zmm5
	vpternlogq	$0x96,%zmm8,%zmm24,%zmm0
	vpternlogq	$0x96,%zmm22,%zmm25,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm30
	vpslldq	$8,%zmm4,%zmm31
	vpxorq	%zmm30,%zmm0,%zmm0
	vpxorq	%zmm31,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm30
	vpxorq	%ymm30,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm30
	vpxorq	%xmm30,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm31
	vpxorq	%ymm31,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm31
	vpxorq	%xmm31,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm1


	vpclmulqdq	$0x01,%xmm3,%xmm1,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm1,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm1,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_gnDihwoBCmctfel:
	vpxorq	%xmm7,%xmm14,%xmm14
.L_after_reduction_gnDihwoBCmctfel:
	jmp	.L_last_blocks_done_DocrvoFoxgdxxwu
.L_last_num_blocks_is_0_DocrvoFoxgdxxwu:
	vmovdqa64	768(%rsp),%zmm13
	vpxorq	%zmm14,%zmm13,%zmm13
	vmovdqu64	0(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	832(%rsp),%zmm13
	vmovdqu64	64(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11
	vpxorq	%zmm10,%zmm4,%zmm26
	vpxorq	%zmm6,%zmm0,%zmm24
	vpxorq	%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26
	vmovdqa64	896(%rsp),%zmm13
	vmovdqu64	128(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm0
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm3
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm4
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm5
	vmovdqa64	960(%rsp),%zmm13
	vmovdqu64	192(%rsp,%rbx,1),%zmm12
	vpclmulqdq	$0x11,%zmm12,%zmm13,%zmm6
	vpclmulqdq	$0x00,%zmm12,%zmm13,%zmm7
	vpclmulqdq	$0x01,%zmm12,%zmm13,%zmm10
	vpclmulqdq	$0x10,%zmm12,%zmm13,%zmm11

	vpternlogq	$0x96,%zmm10,%zmm4,%zmm26
	vpternlogq	$0x96,%zmm6,%zmm0,%zmm24
	vpternlogq	$0x96,%zmm7,%zmm3,%zmm25
	vpternlogq	$0x96,%zmm11,%zmm5,%zmm26

	vpsrldq	$8,%zmm26,%zmm0
	vpslldq	$8,%zmm26,%zmm3
	vpxorq	%zmm0,%zmm24,%zmm24
	vpxorq	%zmm3,%zmm25,%zmm25
	vextracti64x4	$1,%zmm24,%ymm0
	vpxorq	%ymm0,%ymm24,%ymm24
	vextracti32x4	$1,%ymm24,%xmm0
	vpxorq	%xmm0,%xmm24,%xmm24
	vextracti64x4	$1,%zmm25,%ymm3
	vpxorq	%ymm3,%ymm25,%ymm25
	vextracti32x4	$1,%ymm25,%xmm3
	vpxorq	%xmm3,%xmm25,%xmm25
	vmovdqa64	POLY2(%rip),%xmm4


	vpclmulqdq	$0x01,%xmm25,%xmm4,%xmm0
	vpslldq	$8,%xmm0,%xmm0
	vpxorq	%xmm0,%xmm25,%xmm0


	vpclmulqdq	$0x00,%xmm0,%xmm4,%xmm3
	vpsrldq	$4,%xmm3,%xmm3
	vpclmulqdq	$0x10,%xmm0,%xmm4,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm24,%xmm3,%xmm14

.L_last_blocks_done_DocrvoFoxgdxxwu:
	vpshufb	%xmm29,%xmm2,%xmm2
	jmp	.L_ghash_done_eFCuDlbftGnAEad

.L_message_below_equal_16_blocks_eFCuDlbftGnAEad:


	movl	%r8d,%r12d
	addl	$15,%r12d
	shrl	$4,%r12d
	cmpq	$8,%r12
	je	.L_small_initial_num_blocks_is_8_ntcmhlBrGDqtspj
	jl	.L_small_initial_num_blocks_is_7_1_ntcmhlBrGDqtspj


	cmpq	$12,%r12
	je	.L_small_initial_num_blocks_is_12_ntcmhlBrGDqtspj
	jl	.L_small_initial_num_blocks_is_11_9_ntcmhlBrGDqtspj


	cmpq	$16,%r12
	je	.L_small_initial_num_blocks_is_16_ntcmhlBrGDqtspj
	cmpq	$15,%r12
	je	.L_small_initial_num_blocks_is_15_ntcmhlBrGDqtspj
	cmpq	$14,%r12
	je	.L_small_initial_num_blocks_is_14_ntcmhlBrGDqtspj
	jmp	.L_small_initial_num_blocks_is_13_ntcmhlBrGDqtspj

.L_small_initial_num_blocks_is_11_9_ntcmhlBrGDqtspj:

	cmpq	$11,%r12
	je	.L_small_initial_num_blocks_is_11_ntcmhlBrGDqtspj
	cmpq	$10,%r12
	je	.L_small_initial_num_blocks_is_10_ntcmhlBrGDqtspj
	jmp	.L_small_initial_num_blocks_is_9_ntcmhlBrGDqtspj

.L_small_initial_num_blocks_is_7_1_ntcmhlBrGDqtspj:
	cmpq	$4,%r12
	je	.L_small_initial_num_blocks_is_4_ntcmhlBrGDqtspj
	jl	.L_small_initial_num_blocks_is_3_1_ntcmhlBrGDqtspj

	cmpq	$7,%r12
	je	.L_small_initial_num_blocks_is_7_ntcmhlBrGDqtspj
	cmpq	$6,%r12
	je	.L_small_initial_num_blocks_is_6_ntcmhlBrGDqtspj
	jmp	.L_small_initial_num_blocks_is_5_ntcmhlBrGDqtspj

.L_small_initial_num_blocks_is_3_1_ntcmhlBrGDqtspj:

	cmpq	$3,%r12
	je	.L_small_initial_num_blocks_is_3_ntcmhlBrGDqtspj
	cmpq	$2,%r12
	je	.L_small_initial_num_blocks_is_2_ntcmhlBrGDqtspj





.L_small_initial_num_blocks_is_1_ntcmhlBrGDqtspj:
	vmovdqa64	SHUF_MASK(%rip),%xmm29
	vpaddd	ONE(%rip),%xmm2,%xmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm0,%xmm2
	vpshufb	%xmm29,%xmm0,%xmm0
	vmovdqu8	0(%rcx,%rax,1),%xmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%xmm15,%xmm0,%xmm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%xmm15,%xmm0,%xmm0
	vpxorq	%xmm6,%xmm0,%xmm0
	vextracti32x4	$0,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%xmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%xmm29,%xmm6,%xmm6
	vextracti32x4	$0,%zmm6,%xmm13
	leaq	96(%rsi),%r10


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dAypeclCEcGGrDB





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dAypeclCEcGGrDB
.L_small_initial_partial_block_dAypeclCEcGGrDB:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)











	vpxorq	%xmm13,%xmm14,%xmm14

	jmp	.L_after_reduction_dAypeclCEcGGrDB
.L_small_initial_compute_done_dAypeclCEcGGrDB:
.L_after_reduction_dAypeclCEcGGrDB:
	jmp	.L_small_initial_blocks_encrypted_ntcmhlBrGDqtspj
.L_small_initial_num_blocks_is_2_ntcmhlBrGDqtspj:
	vmovdqa64	SHUF_MASK(%rip),%ymm29
	vshufi64x2	$0,%ymm2,%ymm2,%ymm0
	vpaddd	ddq_add_1234(%rip),%ymm0,%ymm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm0,%xmm2
	vpshufb	%ymm29,%ymm0,%ymm0
	vmovdqu8	0(%rcx,%rax,1),%ymm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%ymm15,%ymm0,%ymm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%ymm15,%ymm0,%ymm0
	vpxorq	%ymm6,%ymm0,%ymm0
	vextracti32x4	$1,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%ymm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%ymm29,%ymm6,%ymm6
	vextracti32x4	$1,%zmm6,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (2 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_yEiAdfiasfAbgEt





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_yEiAdfiasfAbgEt
.L_small_initial_partial_block_yEiAdfiasfAbgEt:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm6,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm6,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm6,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm6,%xmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yEiAdfiasfAbgEt:

	orq	%r8,%r8
	je	.L_after_reduction_yEiAdfiasfAbgEt
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_yEiAdfiasfAbgEt:
	jmp	.L_small_initial_blocks_encrypted_ntcmhlBrGDqtspj
.L_small_initial_num_blocks_is_3_ntcmhlBrGDqtspj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$2,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vextracti32x4	$2,%zmm6,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (3 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_llqauGcekwrpeva





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_llqauGcekwrpeva
.L_small_initial_partial_block_llqauGcekwrpeva:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm6,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm6,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm6,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm6,%ymm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_llqauGcekwrpeva:

	orq	%r8,%r8
	je	.L_after_reduction_llqauGcekwrpeva
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_llqauGcekwrpeva:
	jmp	.L_small_initial_blocks_encrypted_ntcmhlBrGDqtspj
.L_small_initial_num_blocks_is_4_ntcmhlBrGDqtspj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm0,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vmovdqu8	0(%rcx,%rax,1),%zmm6{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm6,%zmm0,%zmm0
	vextracti32x4	$3,%zmm0,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1){%k1}
	vmovdqu8	%zmm0,%zmm0{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vextracti32x4	$3,%zmm6,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (4 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_rdpnapFslsciqly





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_rdpnapFslsciqly
.L_small_initial_partial_block_rdpnapFslsciqly:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_rdpnapFslsciqly:

	orq	%r8,%r8
	je	.L_after_reduction_rdpnapFslsciqly
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_rdpnapFslsciqly:
	jmp	.L_small_initial_blocks_encrypted_ntcmhlBrGDqtspj
.L_small_initial_num_blocks_is_5_ntcmhlBrGDqtspj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%xmm29,%xmm3,%xmm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%xmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%xmm15,%xmm3,%xmm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%xmm15,%xmm3,%xmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%xmm7,%xmm3,%xmm3
	vextracti32x4	$0,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%xmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%xmm29,%xmm7,%xmm7
	vextracti32x4	$0,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (5 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_CgmmuAswlqvmxzu





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_CgmmuAswlqvmxzu
.L_small_initial_partial_block_CgmmuAswlqvmxzu:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_CgmmuAswlqvmxzu:

	orq	%r8,%r8
	je	.L_after_reduction_CgmmuAswlqvmxzu
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_CgmmuAswlqvmxzu:
	jmp	.L_small_initial_blocks_encrypted_ntcmhlBrGDqtspj
.L_small_initial_num_blocks_is_6_ntcmhlBrGDqtspj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%ymm29,%ymm3,%ymm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%ymm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%ymm15,%ymm3,%ymm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%ymm15,%ymm3,%ymm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%ymm7,%ymm3,%ymm3
	vextracti32x4	$1,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%ymm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%ymm29,%ymm7,%ymm7
	vextracti32x4	$1,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (6 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_yGejFBCehCAerGD





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_yGejFBCehCAerGD
.L_small_initial_partial_block_yGejFBCehCAerGD:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm7,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm7,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm7,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm7,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_yGejFBCehCAerGD:

	orq	%r8,%r8
	je	.L_after_reduction_yGejFBCehCAerGD
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_yGejFBCehCAerGD:
	jmp	.L_small_initial_blocks_encrypted_ntcmhlBrGDqtspj
.L_small_initial_num_blocks_is_7_ntcmhlBrGDqtspj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$2,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vextracti32x4	$2,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (7 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_qfBejFEvnisGGrw





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_qfBejFEvnisGGrw
.L_small_initial_partial_block_qfBejFEvnisGGrw:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm7,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm7,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm7,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm7,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_qfBejFEvnisGGrw:

	orq	%r8,%r8
	je	.L_after_reduction_qfBejFEvnisGGrw
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_qfBejFEvnisGGrw:
	jmp	.L_small_initial_blocks_encrypted_ntcmhlBrGDqtspj
.L_small_initial_num_blocks_is_8_ntcmhlBrGDqtspj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$64,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm3,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vextracti32x4	$3,%zmm3,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1){%k1}
	vmovdqu8	%zmm3,%zmm3{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vextracti32x4	$3,%zmm7,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (8 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ElihrggqFpGimjf





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ElihrggqFpGimjf
.L_small_initial_partial_block_ElihrggqFpGimjf:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ElihrggqFpGimjf:

	orq	%r8,%r8
	je	.L_after_reduction_ElihrggqFpGimjf
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_ElihrggqFpGimjf:
	jmp	.L_small_initial_blocks_encrypted_ntcmhlBrGDqtspj
.L_small_initial_num_blocks_is_9_ntcmhlBrGDqtspj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%xmm29,%xmm4,%xmm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%xmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%xmm15,%xmm4,%xmm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%xmm15,%xmm4,%xmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%xmm10,%xmm4,%xmm4
	vextracti32x4	$0,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%xmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%xmm29,%xmm10,%xmm10
	vextracti32x4	$0,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (9 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_dtfrdpgwoxdexDc





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_dtfrdpgwoxdexDc
.L_small_initial_partial_block_dtfrdpgwoxdexDc:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_dtfrdpgwoxdexDc:

	orq	%r8,%r8
	je	.L_after_reduction_dtfrdpgwoxdexDc
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_dtfrdpgwoxdexDc:
	jmp	.L_small_initial_blocks_encrypted_ntcmhlBrGDqtspj
.L_small_initial_num_blocks_is_10_ntcmhlBrGDqtspj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%ymm29,%ymm4,%ymm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%ymm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%ymm15,%ymm4,%ymm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%ymm15,%ymm4,%ymm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%ymm10,%ymm4,%ymm4
	vextracti32x4	$1,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%ymm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%ymm29,%ymm10,%ymm10
	vextracti32x4	$1,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (10 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_mtCGysrfsFlAsEs





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_mtCGysrfsFlAsEs
.L_small_initial_partial_block_mtCGysrfsFlAsEs:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm10,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm10,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm10,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm10,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_mtCGysrfsFlAsEs:

	orq	%r8,%r8
	je	.L_after_reduction_mtCGysrfsFlAsEs
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_mtCGysrfsFlAsEs:
	jmp	.L_small_initial_blocks_encrypted_ntcmhlBrGDqtspj
.L_small_initial_num_blocks_is_11_ntcmhlBrGDqtspj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$2,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vextracti32x4	$2,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (11 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nFxksvAxxgyfcvv





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nFxksvAxxgyfcvv
.L_small_initial_partial_block_nFxksvAxxgyfcvv:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm10,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm10,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm10,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm10,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nFxksvAxxgyfcvv:

	orq	%r8,%r8
	je	.L_after_reduction_nFxksvAxxgyfcvv
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_nFxksvAxxgyfcvv:
	jmp	.L_small_initial_blocks_encrypted_ntcmhlBrGDqtspj
.L_small_initial_num_blocks_is_12_ntcmhlBrGDqtspj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$128,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm4,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vextracti32x4	$3,%zmm4,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1){%k1}
	vmovdqu8	%zmm4,%zmm4{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vextracti32x4	$3,%zmm10,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (12 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_ueefzedfybBEldv





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	64(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_ueefzedfybBEldv
.L_small_initial_partial_block_ueefzedfybBEldv:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vpxorq	%zmm15,%zmm0,%zmm15
	vpxorq	%zmm16,%zmm3,%zmm16
	vpxorq	%zmm17,%zmm4,%zmm17
	vpxorq	%zmm19,%zmm5,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_ueefzedfybBEldv:

	orq	%r8,%r8
	je	.L_after_reduction_ueefzedfybBEldv
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_ueefzedfybBEldv:
	jmp	.L_small_initial_blocks_encrypted_ntcmhlBrGDqtspj
.L_small_initial_num_blocks_is_13_ntcmhlBrGDqtspj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$0,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%xmm29,%xmm5,%xmm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%xmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%xmm15,%xmm5,%xmm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%xmm15,%xmm5,%xmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%xmm11,%xmm5,%xmm5
	vextracti32x4	$0,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%xmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%xmm29,%xmm11,%xmm11
	vextracti32x4	$0,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (13 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_nrEvCabdilFdCtw





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	48(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_nrEvCabdilFdCtw
.L_small_initial_partial_block_nrEvCabdilFdCtw:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	64(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	128(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	192(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19

	vpxorq	%zmm19,%zmm17,%zmm17
	vpsrldq	$8,%zmm17,%zmm4
	vpslldq	$8,%zmm17,%zmm5
	vpxorq	%zmm4,%zmm15,%zmm0
	vpxorq	%zmm5,%zmm16,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_nrEvCabdilFdCtw:

	orq	%r8,%r8
	je	.L_after_reduction_nrEvCabdilFdCtw
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_nrEvCabdilFdCtw:
	jmp	.L_small_initial_blocks_encrypted_ntcmhlBrGDqtspj
.L_small_initial_num_blocks_is_14_ntcmhlBrGDqtspj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$1,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%ymm29,%ymm5,%ymm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%ymm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%ymm15,%ymm5,%ymm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%ymm15,%ymm5,%ymm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%ymm11,%ymm5,%ymm5
	vextracti32x4	$1,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%ymm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%ymm29,%ymm11,%ymm11
	vextracti32x4	$1,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (14 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_EehxtdEnzcxrrsx





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	32(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_EehxtdEnzcxrrsx
.L_small_initial_partial_block_EehxtdEnzcxrrsx:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	48(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	112(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	176(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	240(%r10),%xmm20
	vpclmulqdq	$0x01,%xmm20,%xmm11,%xmm4
	vpclmulqdq	$0x10,%xmm20,%xmm11,%xmm5
	vpclmulqdq	$0x11,%xmm20,%xmm11,%xmm0
	vpclmulqdq	$0x00,%xmm20,%xmm11,%xmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_EehxtdEnzcxrrsx:

	orq	%r8,%r8
	je	.L_after_reduction_EehxtdEnzcxrrsx
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_EehxtdEnzcxrrsx:
	jmp	.L_small_initial_blocks_encrypted_ntcmhlBrGDqtspj
.L_small_initial_num_blocks_is_15_ntcmhlBrGDqtspj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$2,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$2,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vextracti32x4	$2,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (15 - 1),%r8


	cmpq	$16,%r8
	jl	.L_small_initial_partial_block_hxAuljvCByhvnfk





	subq	$16,%r8
	movl	$0,(%rdx)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	16(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

	jmp	.L_small_initial_compute_done_hxAuljvCByhvnfk
.L_small_initial_partial_block_hxAuljvCByhvnfk:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	32(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	96(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	160(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	224(%r10),%ymm20
	vpclmulqdq	$0x01,%ymm20,%ymm11,%ymm4
	vpclmulqdq	$0x10,%ymm20,%ymm11,%ymm5
	vpclmulqdq	$0x11,%ymm20,%ymm11,%ymm0
	vpclmulqdq	$0x00,%ymm20,%ymm11,%ymm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_hxAuljvCByhvnfk:

	orq	%r8,%r8
	je	.L_after_reduction_hxAuljvCByhvnfk
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_hxAuljvCByhvnfk:
	jmp	.L_small_initial_blocks_encrypted_ntcmhlBrGDqtspj
.L_small_initial_num_blocks_is_16_ntcmhlBrGDqtspj:
	vmovdqa64	SHUF_MASK(%rip),%zmm29
	vshufi64x2	$0,%zmm2,%zmm2,%zmm2
	vpaddd	ddq_add_1234(%rip),%zmm2,%zmm0
	vpaddd	ddq_add_5678(%rip),%zmm2,%zmm3
	vpaddd	ddq_add_8888(%rip),%zmm0,%zmm4
	vpaddd	ddq_add_8888(%rip),%zmm3,%zmm5
	leaq	byte64_len_to_mask_table(%rip),%r10
	movq	%r8,%r15
	subq	$192,%r15
	kmovq	(%r10,%r15,8),%k1
	vextracti32x4	$3,%zmm5,%xmm2
	vpshufb	%zmm29,%zmm0,%zmm0
	vpshufb	%zmm29,%zmm3,%zmm3
	vpshufb	%zmm29,%zmm4,%zmm4
	vpshufb	%zmm29,%zmm5,%zmm5
	vmovdqu8	0(%rcx,%rax,1),%zmm6
	vmovdqu8	64(%rcx,%rax,1),%zmm7
	vmovdqu8	128(%rcx,%rax,1),%zmm10
	vmovdqu8	192(%rcx,%rax,1),%zmm11{%k1}{z}
	vbroadcastf64x2	0(%rdi),%zmm15
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm15,%zmm3,%zmm3
	vpxorq	%zmm15,%zmm4,%zmm4
	vpxorq	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	16(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	32(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	48(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	64(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	80(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	96(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	112(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	128(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	144(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	160(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	176(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	192(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	208(%rdi),%zmm15
	vaesenc	%zmm15,%zmm0,%zmm0
	vaesenc	%zmm15,%zmm3,%zmm3
	vaesenc	%zmm15,%zmm4,%zmm4
	vaesenc	%zmm15,%zmm5,%zmm5
	vbroadcastf64x2	224(%rdi),%zmm15
	vaesenclast	%zmm15,%zmm0,%zmm0
	vaesenclast	%zmm15,%zmm3,%zmm3
	vaesenclast	%zmm15,%zmm4,%zmm4
	vaesenclast	%zmm15,%zmm5,%zmm5
	vpxorq	%zmm6,%zmm0,%zmm0
	vpxorq	%zmm7,%zmm3,%zmm3
	vpxorq	%zmm10,%zmm4,%zmm4
	vpxorq	%zmm11,%zmm5,%zmm5
	vextracti32x4	$3,%zmm5,%xmm12
	movq	%r9,%r10
	vmovdqu8	%zmm0,0(%r10,%rax,1)
	vmovdqu8	%zmm3,64(%r10,%rax,1)
	vmovdqu8	%zmm4,128(%r10,%rax,1)
	vmovdqu8	%zmm5,192(%r10,%rax,1){%k1}
	vmovdqu8	%zmm5,%zmm5{%k1}{z}
	vpshufb	%zmm29,%zmm6,%zmm6
	vpshufb	%zmm29,%zmm7,%zmm7
	vpshufb	%zmm29,%zmm10,%zmm10
	vpshufb	%zmm29,%zmm11,%zmm11
	vextracti32x4	$3,%zmm11,%xmm13
	leaq	96(%rsi),%r10
	subq	$16 * (16 - 1),%r8
.L_small_initial_partial_block_jAixbjCxhdDkAso:








	movl	%r8d,(%rdx)
	vmovdqu64	%xmm12,16(%rsi)
	vpxorq	%zmm14,%zmm6,%zmm6
	vmovdqu64	16(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm6,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm6,%zmm3
	vpclmulqdq	$0x01,%zmm20,%zmm6,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm6,%zmm5
	vmovdqu64	80(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm7,%zmm15
	vpclmulqdq	$0x00,%zmm20,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm7,%zmm17
	vpclmulqdq	$0x10,%zmm20,%zmm7,%zmm19
	vmovdqu64	144(%r10),%zmm20
	vpclmulqdq	$0x11,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x00,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm0,%zmm6,%zmm15
	vpternlogq	$0x96,%zmm3,%zmm7,%zmm16
	vpclmulqdq	$0x01,%zmm20,%zmm10,%zmm6
	vpclmulqdq	$0x10,%zmm20,%zmm10,%zmm7
	vpternlogq	$0x96,%zmm4,%zmm6,%zmm17
	vpternlogq	$0x96,%zmm5,%zmm7,%zmm19
	vmovdqu64	208(%r10),%ymm20
	vinserti64x2	$2,240(%r10),%zmm20,%zmm20
	vpclmulqdq	$0x01,%zmm20,%zmm11,%zmm4
	vpclmulqdq	$0x10,%zmm20,%zmm11,%zmm5
	vpclmulqdq	$0x11,%zmm20,%zmm11,%zmm0
	vpclmulqdq	$0x00,%zmm20,%zmm11,%zmm3

	vpxorq	%zmm17,%zmm4,%zmm4
	vpxorq	%zmm19,%zmm5,%zmm5
	vpxorq	%zmm15,%zmm0,%zmm0
	vpxorq	%zmm16,%zmm3,%zmm3

	vpxorq	%zmm5,%zmm4,%zmm4
	vpsrldq	$8,%zmm4,%zmm17
	vpslldq	$8,%zmm4,%zmm19
	vpxorq	%zmm17,%zmm0,%zmm0
	vpxorq	%zmm19,%zmm3,%zmm3
	vextracti64x4	$1,%zmm0,%ymm17
	vpxorq	%ymm17,%ymm0,%ymm0
	vextracti32x4	$1,%ymm0,%xmm17
	vpxorq	%xmm17,%xmm0,%xmm0
	vextracti64x4	$1,%zmm3,%ymm19
	vpxorq	%ymm19,%ymm3,%ymm3
	vextracti32x4	$1,%ymm3,%xmm19
	vpxorq	%xmm19,%xmm3,%xmm3
	vmovdqa64	POLY2(%rip),%xmm20


	vpclmulqdq	$0x01,%xmm3,%xmm20,%xmm4
	vpslldq	$8,%xmm4,%xmm4
	vpxorq	%xmm4,%xmm3,%xmm4


	vpclmulqdq	$0x00,%xmm4,%xmm20,%xmm5
	vpsrldq	$4,%xmm5,%xmm5
	vpclmulqdq	$0x10,%xmm4,%xmm20,%xmm14
	vpslldq	$4,%xmm14,%xmm14
	vpternlogq	$0x96,%xmm0,%xmm5,%xmm14

.L_small_initial_compute_done_jAixbjCxhdDkAso:
	vpxorq	%xmm13,%xmm14,%xmm14
.L_after_reduction_jAixbjCxhdDkAso:
.L_small_initial_blocks_encrypted_ntcmhlBrGDqtspj:
.L_ghash_done_eFCuDlbftGnAEad:
	vmovdqu64	%xmm2,0(%rsi)
.L_enc_dec_done_eFCuDlbftGnAEad:

	vpshufb	SHUF_MASK(%rip),%xmm14,%xmm14
	vmovdqu64	%xmm14,64(%rsi)
.L_enc_dec_abort_eFCuDlbftGnAEad:
	jmp	.Lexit_gcm_decrypt
.Lexit_gcm_decrypt:
	cmpq	$256,%r8
	jbe	.Lskip_hkeys_cleanup_EnCudbEBzEzyqEz
	vpxor	%xmm0,%xmm0,%xmm0
	vmovdqa64	%zmm0,0(%rsp)
	vmovdqa64	%zmm0,64(%rsp)
	vmovdqa64	%zmm0,128(%rsp)
	vmovdqa64	%zmm0,192(%rsp)
	vmovdqa64	%zmm0,256(%rsp)
	vmovdqa64	%zmm0,320(%rsp)
	vmovdqa64	%zmm0,384(%rsp)
	vmovdqa64	%zmm0,448(%rsp)
	vmovdqa64	%zmm0,512(%rsp)
	vmovdqa64	%zmm0,576(%rsp)
	vmovdqa64	%zmm0,640(%rsp)
	vmovdqa64	%zmm0,704(%rsp)
.Lskip_hkeys_cleanup_EnCudbEBzEzyqEz:
	vzeroupper
	leaq	(%rbp),%rsp
.cfi_def_cfa_register	%rsp
	popq	%r15
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r15
	popq	%r14
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r14
	popq	%r13
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r13
	popq	%r12
.cfi_adjust_cfa_offset	-8
.cfi_restore	%r12
	popq	%rbp
.cfi_adjust_cfa_offset	-8
.cfi_restore	%rbp
	popq	%rbx
.cfi_adjust_cfa_offset	-8
.cfi_restore	%rbx
	.byte	0xf3,0xc3
.Ldecrypt_seh_end:
.cfi_endproc	
.size	aes_gcm_decrypt_avx512, .-aes_gcm_decrypt_avx512
.data	
.align	16
POLY:.quad	0x0000000000000001, 0xC200000000000000

.align	64
POLY2:
.quad	0x00000001C2000000, 0xC200000000000000
.quad	0x00000001C2000000, 0xC200000000000000
.quad	0x00000001C2000000, 0xC200000000000000
.quad	0x00000001C2000000, 0xC200000000000000

.align	16
TWOONE:.quad	0x0000000000000001, 0x0000000100000000



.align	64
SHUF_MASK:
.quad	0x08090A0B0C0D0E0F, 0x0001020304050607
.quad	0x08090A0B0C0D0E0F, 0x0001020304050607
.quad	0x08090A0B0C0D0E0F, 0x0001020304050607
.quad	0x08090A0B0C0D0E0F, 0x0001020304050607

.align	16
SHIFT_MASK:
.quad	0x0706050403020100, 0x0f0e0d0c0b0a0908

ALL_F:
.quad	0xffffffffffffffff, 0xffffffffffffffff

ZERO:
.quad	0x0000000000000000, 0x0000000000000000

.align	16
ONE:
.quad	0x0000000000000001, 0x0000000000000000

.align	16
ONEf:
.quad	0x0000000000000000, 0x0100000000000000

.align	64
ddq_add_1234:
.quad	0x0000000000000001, 0x0000000000000000
.quad	0x0000000000000002, 0x0000000000000000
.quad	0x0000000000000003, 0x0000000000000000
.quad	0x0000000000000004, 0x0000000000000000

.align	64
ddq_add_5678:
.quad	0x0000000000000005, 0x0000000000000000
.quad	0x0000000000000006, 0x0000000000000000
.quad	0x0000000000000007, 0x0000000000000000
.quad	0x0000000000000008, 0x0000000000000000

.align	64
ddq_add_4444:
.quad	0x0000000000000004, 0x0000000000000000
.quad	0x0000000000000004, 0x0000000000000000
.quad	0x0000000000000004, 0x0000000000000000
.quad	0x0000000000000004, 0x0000000000000000

.align	64
ddq_add_8888:
.quad	0x0000000000000008, 0x0000000000000000
.quad	0x0000000000000008, 0x0000000000000000
.quad	0x0000000000000008, 0x0000000000000000
.quad	0x0000000000000008, 0x0000000000000000

.align	64
ddq_addbe_1234:
.quad	0x0000000000000000, 0x0100000000000000
.quad	0x0000000000000000, 0x0200000000000000
.quad	0x0000000000000000, 0x0300000000000000
.quad	0x0000000000000000, 0x0400000000000000

.align	64
ddq_addbe_4444:
.quad	0x0000000000000000, 0x0400000000000000
.quad	0x0000000000000000, 0x0400000000000000
.quad	0x0000000000000000, 0x0400000000000000
.quad	0x0000000000000000, 0x0400000000000000

.align	64
byte_len_to_mask_table:
.value	0x0000, 0x0001, 0x0003, 0x0007
.value	0x000f, 0x001f, 0x003f, 0x007f
.value	0x00ff, 0x01ff, 0x03ff, 0x07ff
.value	0x0fff, 0x1fff, 0x3fff, 0x7fff
.value	0xffff

.align	64
byte64_len_to_mask_table:
.quad	0x0000000000000000, 0x0000000000000001
.quad	0x0000000000000003, 0x0000000000000007
.quad	0x000000000000000f, 0x000000000000001f
.quad	0x000000000000003f, 0x000000000000007f
.quad	0x00000000000000ff, 0x00000000000001ff
.quad	0x00000000000003ff, 0x00000000000007ff
.quad	0x0000000000000fff, 0x0000000000001fff
.quad	0x0000000000003fff, 0x0000000000007fff
.quad	0x000000000000ffff, 0x000000000001ffff
.quad	0x000000000003ffff, 0x000000000007ffff
.quad	0x00000000000fffff, 0x00000000001fffff
.quad	0x00000000003fffff, 0x00000000007fffff
.quad	0x0000000000ffffff, 0x0000000001ffffff
.quad	0x0000000003ffffff, 0x0000000007ffffff
.quad	0x000000000fffffff, 0x000000001fffffff
.quad	0x000000003fffffff, 0x000000007fffffff
.quad	0x00000000ffffffff, 0x00000001ffffffff
.quad	0x00000003ffffffff, 0x00000007ffffffff
.quad	0x0000000fffffffff, 0x0000001fffffffff
.quad	0x0000003fffffffff, 0x0000007fffffffff
.quad	0x000000ffffffffff, 0x000001ffffffffff
.quad	0x000003ffffffffff, 0x000007ffffffffff
.quad	0x00000fffffffffff, 0x00001fffffffffff
.quad	0x00003fffffffffff, 0x00007fffffffffff
.quad	0x0000ffffffffffff, 0x0001ffffffffffff
.quad	0x0003ffffffffffff, 0x0007ffffffffffff
.quad	0x000fffffffffffff, 0x001fffffffffffff
.quad	0x003fffffffffffff, 0x007fffffffffffff
.quad	0x00ffffffffffffff, 0x01ffffffffffffff
.quad	0x03ffffffffffffff, 0x07ffffffffffffff
.quad	0x0fffffffffffffff, 0x1fffffffffffffff
.quad	0x3fffffffffffffff, 0x7fffffffffffffff
.quad	0xffffffffffffffff
#endif
.section	.note.GNU-stack,"",@progbits
